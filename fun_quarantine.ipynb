{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fun_quarantine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrc9RyvqBLO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/jokes.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0nXLMuWdsB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny6VQU-4_EwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "41715c8a-55e8-4087-ef4f-7259707bc4f3"
      },
      "source": [
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Did you hear about the Native American man tha...</td>\n",
              "      <td>He nearly drown in his own tea pee.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>What's the best anti diarrheal prescription?</td>\n",
              "      <td>Mycheexarphlexin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>What do you call a person who is outside a doo...</td>\n",
              "      <td>Matt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Which Star Trek character is a member of the m...</td>\n",
              "      <td>Jean-Luc Pickacard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>What's the difference between a bullet and a h...</td>\n",
              "      <td>A bullet doesn't miss Harambe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38264</th>\n",
              "      <td>38265</td>\n",
              "      <td>Q: Why did the pacifist /b/tard try to calm ev...</td>\n",
              "      <td>He did it for the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38265</th>\n",
              "      <td>38266</td>\n",
              "      <td>Q: Why can't Obama poke fun at himself?</td>\n",
              "      <td>A: Because that would be racist.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38266</th>\n",
              "      <td>38267</td>\n",
              "      <td>Why is gambling not allowed in Africa?</td>\n",
              "      <td>Because there are too many cheetahs.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38267</th>\n",
              "      <td>38268</td>\n",
              "      <td>What do you call three witches in a hot tub?</td>\n",
              "      <td>A self-cleaning coven.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38268</th>\n",
              "      <td>38269</td>\n",
              "      <td>What do scientists use to measure a chicken's ...</td>\n",
              "      <td>An egg timer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>38269 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...                                Answer\n",
              "0          1  ...   He nearly drown in his own tea pee.\n",
              "1          2  ...                      Mycheexarphlexin\n",
              "2          3  ...                                  Matt\n",
              "3          4  ...                    Jean-Luc Pickacard\n",
              "4          5  ...         A bullet doesn't miss Harambe\n",
              "...      ...  ...                                   ...\n",
              "38264  38265  ...                    He did it for the \n",
              "38265  38266  ...     A: Because that would be racist. \n",
              "38266  38267  ...  Because there are too many cheetahs.\n",
              "38267  38268  ...                A self-cleaning coven.\n",
              "38268  38269  ...                          An egg timer\n",
              "\n",
              "[38269 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EUJSbfLaduc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs =[(data['Question'][i],data['Answer'][i])for i in range(38269)]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jps4MRVlFraa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e1e7db2-038b-4cb6-e7ea-f5b2269e7357"
      },
      "source": [
        "i = []\n",
        "for pair in pairs:\n",
        "  a=len(pair[0])\n",
        "  b=len(pair[1])\n",
        "  i.append(max(a,b))\n",
        "i"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[70,\n",
              " 44,\n",
              " 73,\n",
              " 58,\n",
              " 51,\n",
              " 34,\n",
              " 86,\n",
              " 31,\n",
              " 47,\n",
              " 143,\n",
              " 52,\n",
              " 32,\n",
              " 43,\n",
              " 54,\n",
              " 85,\n",
              " 51,\n",
              " 28,\n",
              " 35,\n",
              " 47,\n",
              " 30,\n",
              " 46,\n",
              " 51,\n",
              " 67,\n",
              " 47,\n",
              " 39,\n",
              " 35,\n",
              " 39,\n",
              " 50,\n",
              " 41,\n",
              " 50,\n",
              " 47,\n",
              " 45,\n",
              " 46,\n",
              " 131,\n",
              " 46,\n",
              " 58,\n",
              " 51,\n",
              " 55,\n",
              " 51,\n",
              " 112,\n",
              " 39,\n",
              " 54,\n",
              " 143,\n",
              " 33,\n",
              " 46,\n",
              " 52,\n",
              " 35,\n",
              " 48,\n",
              " 49,\n",
              " 57,\n",
              " 57,\n",
              " 24,\n",
              " 41,\n",
              " 43,\n",
              " 62,\n",
              " 75,\n",
              " 46,\n",
              " 57,\n",
              " 67,\n",
              " 46,\n",
              " 95,\n",
              " 41,\n",
              " 49,\n",
              " 48,\n",
              " 54,\n",
              " 42,\n",
              " 74,\n",
              " 50,\n",
              " 69,\n",
              " 39,\n",
              " 51,\n",
              " 51,\n",
              " 62,\n",
              " 53,\n",
              " 65,\n",
              " 29,\n",
              " 45,\n",
              " 49,\n",
              " 51,\n",
              " 46,\n",
              " 53,\n",
              " 32,\n",
              " 65,\n",
              " 58,\n",
              " 50,\n",
              " 48,\n",
              " 37,\n",
              " 36,\n",
              " 48,\n",
              " 41,\n",
              " 45,\n",
              " 36,\n",
              " 94,\n",
              " 47,\n",
              " 70,\n",
              " 45,\n",
              " 112,\n",
              " 70,\n",
              " 37,\n",
              " 45,\n",
              " 46,\n",
              " 74,\n",
              " 39,\n",
              " 83,\n",
              " 45,\n",
              " 42,\n",
              " 48,\n",
              " 59,\n",
              " 37,\n",
              " 56,\n",
              " 46,\n",
              " 93,\n",
              " 45,\n",
              " 56,\n",
              " 53,\n",
              " 40,\n",
              " 63,\n",
              " 41,\n",
              " 34,\n",
              " 53,\n",
              " 14,\n",
              " 30,\n",
              " 47,\n",
              " 68,\n",
              " 65,\n",
              " 36,\n",
              " 76,\n",
              " 76,\n",
              " 38,\n",
              " 58,\n",
              " 63,\n",
              " 74,\n",
              " 153,\n",
              " 40,\n",
              " 35,\n",
              " 30,\n",
              " 56,\n",
              " 290,\n",
              " 53,\n",
              " 37,\n",
              " 36,\n",
              " 46,\n",
              " 34,\n",
              " 65,\n",
              " 35,\n",
              " 86,\n",
              " 35,\n",
              " 98,\n",
              " 61,\n",
              " 54,\n",
              " 42,\n",
              " 57,\n",
              " 30,\n",
              " 47,\n",
              " 72,\n",
              " 53,\n",
              " 62,\n",
              " 70,\n",
              " 44,\n",
              " 40,\n",
              " 29,\n",
              " 41,\n",
              " 44,\n",
              " 31,\n",
              " 39,\n",
              " 73,\n",
              " 81,\n",
              " 83,\n",
              " 55,\n",
              " 50,\n",
              " 59,\n",
              " 36,\n",
              " 67,\n",
              " 88,\n",
              " 51,\n",
              " 67,\n",
              " 90,\n",
              " 57,\n",
              " 54,\n",
              " 67,\n",
              " 94,\n",
              " 57,\n",
              " 56,\n",
              " 38,\n",
              " 49,\n",
              " 47,\n",
              " 61,\n",
              " 56,\n",
              " 30,\n",
              " 33,\n",
              " 31,\n",
              " 69,\n",
              " 24,\n",
              " 45,\n",
              " 25,\n",
              " 54,\n",
              " 45,\n",
              " 81,\n",
              " 52,\n",
              " 35,\n",
              " 61,\n",
              " 60,\n",
              " 36,\n",
              " 43,\n",
              " 43,\n",
              " 38,\n",
              " 34,\n",
              " 85,\n",
              " 36,\n",
              " 70,\n",
              " 70,\n",
              " 59,\n",
              " 46,\n",
              " 49,\n",
              " 48,\n",
              " 54,\n",
              " 59,\n",
              " 37,\n",
              " 47,\n",
              " 36,\n",
              " 45,\n",
              " 42,\n",
              " 30,\n",
              " 58,\n",
              " 119,\n",
              " 37,\n",
              " 49,\n",
              " 56,\n",
              " 55,\n",
              " 73,\n",
              " 43,\n",
              " 105,\n",
              " 49,\n",
              " 37,\n",
              " 39,\n",
              " 57,\n",
              " 71,\n",
              " 38,\n",
              " 40,\n",
              " 49,\n",
              " 37,\n",
              " 101,\n",
              " 35,\n",
              " 246,\n",
              " 57,\n",
              " 37,\n",
              " 41,\n",
              " 63,\n",
              " 64,\n",
              " 35,\n",
              " 37,\n",
              " 26,\n",
              " 34,\n",
              " 54,\n",
              " 29,\n",
              " 29,\n",
              " 25,\n",
              " 331,\n",
              " 28,\n",
              " 55,\n",
              " 61,\n",
              " 38,\n",
              " 60,\n",
              " 55,\n",
              " 62,\n",
              " 40,\n",
              " 55,\n",
              " 56,\n",
              " 61,\n",
              " 54,\n",
              " 47,\n",
              " 35,\n",
              " 117,\n",
              " 47,\n",
              " 34,\n",
              " 38,\n",
              " 43,\n",
              " 55,\n",
              " 37,\n",
              " 44,\n",
              " 36,\n",
              " 44,\n",
              " 65,\n",
              " 58,\n",
              " 47,\n",
              " 34,\n",
              " 56,\n",
              " 61,\n",
              " 41,\n",
              " 65,\n",
              " 58,\n",
              " 51,\n",
              " 73,\n",
              " 59,\n",
              " 27,\n",
              " 39,\n",
              " 48,\n",
              " 44,\n",
              " 47,\n",
              " 53,\n",
              " 61,\n",
              " 41,\n",
              " 29,\n",
              " 37,\n",
              " 40,\n",
              " 35,\n",
              " 43,\n",
              " 35,\n",
              " 178,\n",
              " 64,\n",
              " 33,\n",
              " 58,\n",
              " 38,\n",
              " 53,\n",
              " 36,\n",
              " 52,\n",
              " 55,\n",
              " 44,\n",
              " 54,\n",
              " 70,\n",
              " 46,\n",
              " 35,\n",
              " 40,\n",
              " 61,\n",
              " 65,\n",
              " 43,\n",
              " 39,\n",
              " 37,\n",
              " 43,\n",
              " 78,\n",
              " 43,\n",
              " 42,\n",
              " 66,\n",
              " 96,\n",
              " 40,\n",
              " 52,\n",
              " 51,\n",
              " 28,\n",
              " 49,\n",
              " 60,\n",
              " 28,\n",
              " 50,\n",
              " 72,\n",
              " 71,\n",
              " 62,\n",
              " 37,\n",
              " 84,\n",
              " 47,\n",
              " 46,\n",
              " 28,\n",
              " 44,\n",
              " 54,\n",
              " 34,\n",
              " 46,\n",
              " 84,\n",
              " 32,\n",
              " 51,\n",
              " 49,\n",
              " 36,\n",
              " 66,\n",
              " 39,\n",
              " 84,\n",
              " 35,\n",
              " 58,\n",
              " 45,\n",
              " 79,\n",
              " 143,\n",
              " 36,\n",
              " 79,\n",
              " 44,\n",
              " 41,\n",
              " 56,\n",
              " 31,\n",
              " 38,\n",
              " 44,\n",
              " 53,\n",
              " 52,\n",
              " 35,\n",
              " 37,\n",
              " 59,\n",
              " 49,\n",
              " 69,\n",
              " 93,\n",
              " 37,\n",
              " 58,\n",
              " 79,\n",
              " 47,\n",
              " 48,\n",
              " 40,\n",
              " 59,\n",
              " 62,\n",
              " 72,\n",
              " 61,\n",
              " 44,\n",
              " 68,\n",
              " 23,\n",
              " 61,\n",
              " 62,\n",
              " 59,\n",
              " 25,\n",
              " 72,\n",
              " 37,\n",
              " 40,\n",
              " 33,\n",
              " 34,\n",
              " 38,\n",
              " 35,\n",
              " 45,\n",
              " 54,\n",
              " 37,\n",
              " 44,\n",
              " 42,\n",
              " 53,\n",
              " 53,\n",
              " 37,\n",
              " 69,\n",
              " 38,\n",
              " 53,\n",
              " 53,\n",
              " 47,\n",
              " 48,\n",
              " 66,\n",
              " 25,\n",
              " 71,\n",
              " 61,\n",
              " 56,\n",
              " 54,\n",
              " 48,\n",
              " 36,\n",
              " 41,\n",
              " 56,\n",
              " 60,\n",
              " 55,\n",
              " 45,\n",
              " 86,\n",
              " 69,\n",
              " 41,\n",
              " 55,\n",
              " 57,\n",
              " 45,\n",
              " 58,\n",
              " 33,\n",
              " 41,\n",
              " 80,\n",
              " 56,\n",
              " 31,\n",
              " 61,\n",
              " 44,\n",
              " 53,\n",
              " 46,\n",
              " 48,\n",
              " 38,\n",
              " 57,\n",
              " 53,\n",
              " 82,\n",
              " 67,\n",
              " 35,\n",
              " 51,\n",
              " 35,\n",
              " 182,\n",
              " 37,\n",
              " 44,\n",
              " 57,\n",
              " 68,\n",
              " 49,\n",
              " 50,\n",
              " 30,\n",
              " 54,\n",
              " 44,\n",
              " 41,\n",
              " 43,\n",
              " 54,\n",
              " 55,\n",
              " 53,\n",
              " 38,\n",
              " 59,\n",
              " 57,\n",
              " 66,\n",
              " 38,\n",
              " 37,\n",
              " 79,\n",
              " 85,\n",
              " 59,\n",
              " 49,\n",
              " 41,\n",
              " 52,\n",
              " 49,\n",
              " 48,\n",
              " 32,\n",
              " 93,\n",
              " 29,\n",
              " 65,\n",
              " 71,\n",
              " 42,\n",
              " 34,\n",
              " 73,\n",
              " 49,\n",
              " 28,\n",
              " 81,\n",
              " 40,\n",
              " 74,\n",
              " 75,\n",
              " 44,\n",
              " 49,\n",
              " 56,\n",
              " 41,\n",
              " 47,\n",
              " 47,\n",
              " 56,\n",
              " 54,\n",
              " 55,\n",
              " 249,\n",
              " 44,\n",
              " 58,\n",
              " 54,\n",
              " 55,\n",
              " 54,\n",
              " 38,\n",
              " 63,\n",
              " 38,\n",
              " 39,\n",
              " 24,\n",
              " 55,\n",
              " 58,\n",
              " 65,\n",
              " 39,\n",
              " 54,\n",
              " 68,\n",
              " 41,\n",
              " 33,\n",
              " 67,\n",
              " 58,\n",
              " 70,\n",
              " 76,\n",
              " 499,\n",
              " 38,\n",
              " 41,\n",
              " 41,\n",
              " 57,\n",
              " 133,\n",
              " 24,\n",
              " 40,\n",
              " 86,\n",
              " 51,\n",
              " 49,\n",
              " 183,\n",
              " 21,\n",
              " 63,\n",
              " 68,\n",
              " 50,\n",
              " 59,\n",
              " 67,\n",
              " 40,\n",
              " 63,\n",
              " 54,\n",
              " 30,\n",
              " 27,\n",
              " 59,\n",
              " 47,\n",
              " 42,\n",
              " 58,\n",
              " 30,\n",
              " 37,\n",
              " 45,\n",
              " 32,\n",
              " 35,\n",
              " 40,\n",
              " 78,\n",
              " 48,\n",
              " 91,\n",
              " 70,\n",
              " 51,\n",
              " 47,\n",
              " 37,\n",
              " 86,\n",
              " 75,\n",
              " 47,\n",
              " 62,\n",
              " 77,\n",
              " 40,\n",
              " 25,\n",
              " 61,\n",
              " 166,\n",
              " 35,\n",
              " 68,\n",
              " 59,\n",
              " 66,\n",
              " 88,\n",
              " 39,\n",
              " 52,\n",
              " 41,\n",
              " 44,\n",
              " 35,\n",
              " 28,\n",
              " 38,\n",
              " 50,\n",
              " 32,\n",
              " 41,\n",
              " 53,\n",
              " 35,\n",
              " 32,\n",
              " 48,\n",
              " 38,\n",
              " 47,\n",
              " 33,\n",
              " 60,\n",
              " 32,\n",
              " 51,\n",
              " 60,\n",
              " 47,\n",
              " 34,\n",
              " 46,\n",
              " 46,\n",
              " 43,\n",
              " 41,\n",
              " 56,\n",
              " 59,\n",
              " 38,\n",
              " 58,\n",
              " 52,\n",
              " 40,\n",
              " 55,\n",
              " 43,\n",
              " 49,\n",
              " 57,\n",
              " 127,\n",
              " 34,\n",
              " 63,\n",
              " 31,\n",
              " 44,\n",
              " 34,\n",
              " 55,\n",
              " 81,\n",
              " 35,\n",
              " 33,\n",
              " 43,\n",
              " 44,\n",
              " 43,\n",
              " 30,\n",
              " 41,\n",
              " 45,\n",
              " 36,\n",
              " 65,\n",
              " 69,\n",
              " 87,\n",
              " 36,\n",
              " 46,\n",
              " 41,\n",
              " 40,\n",
              " 42,\n",
              " 37,\n",
              " 40,\n",
              " 40,\n",
              " 39,\n",
              " 57,\n",
              " 42,\n",
              " 37,\n",
              " 39,\n",
              " 60,\n",
              " 33,\n",
              " 34,\n",
              " 39,\n",
              " 63,\n",
              " 51,\n",
              " 36,\n",
              " 75,\n",
              " 43,\n",
              " 50,\n",
              " 41,\n",
              " 53,\n",
              " 32,\n",
              " 46,\n",
              " 33,\n",
              " 52,\n",
              " 42,\n",
              " 43,\n",
              " 60,\n",
              " 69,\n",
              " 59,\n",
              " 49,\n",
              " 35,\n",
              " 58,\n",
              " 61,\n",
              " 70,\n",
              " 48,\n",
              " 51,\n",
              " 31,\n",
              " 57,\n",
              " 49,\n",
              " 38,\n",
              " 74,\n",
              " 44,\n",
              " 38,\n",
              " 37,\n",
              " 52,\n",
              " 72,\n",
              " 46,\n",
              " 47,\n",
              " 78,\n",
              " 47,\n",
              " 70,\n",
              " 38,\n",
              " 49,\n",
              " 69,\n",
              " 33,\n",
              " 37,\n",
              " 40,\n",
              " 41,\n",
              " 39,\n",
              " 41,\n",
              " 26,\n",
              " 49,\n",
              " 44,\n",
              " 43,\n",
              " 47,\n",
              " 34,\n",
              " 63,\n",
              " 61,\n",
              " 52,\n",
              " 80,\n",
              " 56,\n",
              " 120,\n",
              " 35,\n",
              " 38,\n",
              " 56,\n",
              " 38,\n",
              " 29,\n",
              " 69,\n",
              " 48,\n",
              " 41,\n",
              " 45,\n",
              " 46,\n",
              " 59,\n",
              " 47,\n",
              " 99,\n",
              " 50,\n",
              " 57,\n",
              " 43,\n",
              " 58,\n",
              " 64,\n",
              " 50,\n",
              " 48,\n",
              " 35,\n",
              " 53,\n",
              " 31,\n",
              " 40,\n",
              " 60,\n",
              " 41,\n",
              " 47,\n",
              " 43,\n",
              " 92,\n",
              " 36,\n",
              " 70,\n",
              " 58,\n",
              " 49,\n",
              " 54,\n",
              " 29,\n",
              " 40,\n",
              " 34,\n",
              " 55,\n",
              " 43,\n",
              " 46,\n",
              " 23,\n",
              " 52,\n",
              " 39,\n",
              " 70,\n",
              " 61,\n",
              " 38,\n",
              " 53,\n",
              " 59,\n",
              " 54,\n",
              " 65,\n",
              " 28,\n",
              " 42,\n",
              " 70,\n",
              " 21,\n",
              " 35,\n",
              " 32,\n",
              " 119,\n",
              " 36,\n",
              " 51,\n",
              " 44,\n",
              " 83,\n",
              " 42,\n",
              " 49,\n",
              " 39,\n",
              " 32,\n",
              " 55,\n",
              " 62,\n",
              " 19,\n",
              " 43,\n",
              " 44,\n",
              " 28,\n",
              " 47,\n",
              " 37,\n",
              " 52,\n",
              " 57,\n",
              " 64,\n",
              " 41,\n",
              " 35,\n",
              " 39,\n",
              " 36,\n",
              " 22,\n",
              " 74,\n",
              " 62,\n",
              " 50,\n",
              " 35,\n",
              " 39,\n",
              " 68,\n",
              " 70,\n",
              " 49,\n",
              " 71,\n",
              " 52,\n",
              " 54,\n",
              " 55,\n",
              " 65,\n",
              " 36,\n",
              " 55,\n",
              " 62,\n",
              " 54,\n",
              " 70,\n",
              " 55,\n",
              " 47,\n",
              " 43,\n",
              " 64,\n",
              " 44,\n",
              " 50,\n",
              " 58,\n",
              " 59,\n",
              " 39,\n",
              " 95,\n",
              " 52,\n",
              " 52,\n",
              " 64,\n",
              " 40,\n",
              " 102,\n",
              " 71,\n",
              " 34,\n",
              " 87,\n",
              " 35,\n",
              " 42,\n",
              " 50,\n",
              " 111,\n",
              " 44,\n",
              " 70,\n",
              " 79,\n",
              " 39,\n",
              " 59,\n",
              " 53,\n",
              " 59,\n",
              " 27,\n",
              " 80,\n",
              " 48,\n",
              " 86,\n",
              " 82,\n",
              " 49,\n",
              " 86,\n",
              " 15,\n",
              " 51,\n",
              " 66,\n",
              " 56,\n",
              " 74,\n",
              " 41,\n",
              " 59,\n",
              " 56,\n",
              " 34,\n",
              " 72,\n",
              " 73,\n",
              " 42,\n",
              " 42,\n",
              " 67,\n",
              " 48,\n",
              " 46,\n",
              " 63,\n",
              " 56,\n",
              " 37,\n",
              " 52,\n",
              " 26,\n",
              " 28,\n",
              " 39,\n",
              " 52,\n",
              " 61,\n",
              " 28,\n",
              " 57,\n",
              " 32,\n",
              " 41,\n",
              " 64,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 43,\n",
              " 71,\n",
              " 51,\n",
              " 62,\n",
              " 29,\n",
              " 62,\n",
              " 41,\n",
              " 44,\n",
              " 63,\n",
              " 79,\n",
              " 42,\n",
              " 34,\n",
              " 42,\n",
              " 44,\n",
              " 46,\n",
              " 49,\n",
              " 72,\n",
              " 39,\n",
              " 57,\n",
              " 40,\n",
              " 69,\n",
              " 46,\n",
              " 47,\n",
              " 39,\n",
              " 37,\n",
              " 73,\n",
              " 39,\n",
              " 66,\n",
              " 57,\n",
              " 46,\n",
              " 29,\n",
              " 46,\n",
              " 41,\n",
              " 61,\n",
              " 35,\n",
              " 41,\n",
              " 44,\n",
              " 61,\n",
              " 145,\n",
              " 56,\n",
              " 31,\n",
              " 47,\n",
              " 57,\n",
              " 45,\n",
              " 26,\n",
              " 47,\n",
              " 48,\n",
              " 42,\n",
              " 38,\n",
              " 39,\n",
              " 47,\n",
              " 40,\n",
              " 37,\n",
              " 55,\n",
              " 58,\n",
              " 457,\n",
              " 56,\n",
              " 65,\n",
              " 68,\n",
              " 34,\n",
              " 99,\n",
              " 34,\n",
              " 64,\n",
              " 46,\n",
              " 60,\n",
              " 60,\n",
              " 55,\n",
              " 61,\n",
              " 41,\n",
              " 44,\n",
              " 38,\n",
              " 56,\n",
              " 40,\n",
              " 53,\n",
              " 48,\n",
              " 30,\n",
              " 43,\n",
              " 60,\n",
              " 38,\n",
              " 34,\n",
              " 59,\n",
              " 70,\n",
              " 61,\n",
              " 55,\n",
              " 32,\n",
              " 48,\n",
              " 46,\n",
              " 23,\n",
              " 53,\n",
              " 31,\n",
              " 23,\n",
              " 46,\n",
              " 36,\n",
              " 74,\n",
              " 42,\n",
              " 35,\n",
              " 68,\n",
              " 37,\n",
              " 88,\n",
              " 28,\n",
              " 63,\n",
              " 64,\n",
              " 50,\n",
              " 51,\n",
              " 64,\n",
              " 49,\n",
              " 27,\n",
              " 29,\n",
              " 43,\n",
              " 49,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZJ9B8IPaq7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_tok = 0\n",
        "sos_tok = 1\n",
        "eos_tok = 2\n",
        "class VOC:\n",
        "  def __init__(self,name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count={}\n",
        "    self.index2word = {pad_tok:\"PAD\",sos_tok:\"SOS\",eos_tok:\"EOS\"}\n",
        "    self.num_word = 3\n",
        "\n",
        "  def addline(self,s):\n",
        "    for word in s.split(' '):\n",
        "      self.addword(word)\n",
        "  def addword(self,word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word]= self.num_word\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.num_word] = word \n",
        "      self.num_word+=1\n",
        "    else:\n",
        "      self.word2count[word]+=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g6zg_gbBgzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def fiterpair(pair):\n",
        "  return [p for p in pairs if len(p[0].split(' '))<max_length and len(p[1].split(' '))<max_length]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CspPQojOC3dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 20\n",
        "clean_pairs=fiterpair(pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHDLeuWAGk0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1745da73-f9e8-4861-cee1-41b76764eef2"
      },
      "source": [
        "len(clean_pairs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERF2VgoVKenk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = VOC(\"dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A7bTLlfKwYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in pairs:\n",
        "  voc.addline(p[0])\n",
        "  voc.addline(p[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR36nT06K9IL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c8a4ec9-e330-4b23-c931-ba96981a7833"
      },
      "source": [
        "voc.num_word"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62373"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ls4WzQuMMdc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15540eea-b466-415b-e0ed-8306dbe0f202"
      },
      "source": [
        "voc.word2count"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Did': 1939,\n",
              " 'you': 15037,\n",
              " 'hear': 2038,\n",
              " 'about': 2938,\n",
              " 'the': 26931,\n",
              " 'Native': 38,\n",
              " 'American': 154,\n",
              " 'man': 996,\n",
              " 'that': 2501,\n",
              " 'drank': 35,\n",
              " '200': 28,\n",
              " 'cups': 9,\n",
              " 'of': 6864,\n",
              " 'tea?': 43,\n",
              " 'He': 1909,\n",
              " 'nearly': 5,\n",
              " 'drown': 21,\n",
              " 'in': 7537,\n",
              " 'his': 2771,\n",
              " 'own': 115,\n",
              " 'tea': 60,\n",
              " 'pee.': 7,\n",
              " \"What's\": 4234,\n",
              " 'best': 589,\n",
              " 'anti': 8,\n",
              " 'diarrheal': 1,\n",
              " 'prescription?': 1,\n",
              " 'Mycheexarphlexin': 1,\n",
              " 'What': 14737,\n",
              " 'do': 11471,\n",
              " 'call': 5088,\n",
              " 'a': 24528,\n",
              " 'person': 245,\n",
              " 'who': 1657,\n",
              " 'is': 5905,\n",
              " 'outside': 49,\n",
              " 'door': 53,\n",
              " 'and': 8828,\n",
              " 'has': 1066,\n",
              " 'no': 905,\n",
              " 'arms': 103,\n",
              " 'nor': 9,\n",
              " 'legs?': 108,\n",
              " 'Matt': 8,\n",
              " 'Which': 248,\n",
              " 'Star': 49,\n",
              " 'Trek': 10,\n",
              " 'character': 25,\n",
              " 'member': 30,\n",
              " 'magic': 24,\n",
              " 'circle?': 12,\n",
              " 'Jean-Luc': 2,\n",
              " 'Pickacard': 1,\n",
              " 'difference': 2381,\n",
              " 'between': 2644,\n",
              " 'bullet': 13,\n",
              " 'human?': 6,\n",
              " 'A': 3737,\n",
              " \"doesn't\": 555,\n",
              " 'miss': 33,\n",
              " 'Harambe': 12,\n",
              " 'Why': 7407,\n",
              " 'was': 3523,\n",
              " 'Ethiopian': 42,\n",
              " 'baby': 262,\n",
              " 'crying?': 19,\n",
              " 'having': 289,\n",
              " 'mid-life': 4,\n",
              " 'crisis': 8,\n",
              " 'corn': 39,\n",
              " 'husker': 5,\n",
              " 'with': 3336,\n",
              " 'epilepsy': 7,\n",
              " 'hooker': 73,\n",
              " 'dysentery?': 4,\n",
              " 'One': 501,\n",
              " 'shucks': 24,\n",
              " 'fits...': 5,\n",
              " 'Who': 443,\n",
              " \"2016's\": 1,\n",
              " 'biggest': 67,\n",
              " 'sellout?': 1,\n",
              " 'Kevin': 9,\n",
              " 'Durant': 1,\n",
              " 'or': 471,\n",
              " 'Bernie': 48,\n",
              " 'Sanders?': 5,\n",
              " 'little': 444,\n",
              " \"Annie's\": 1,\n",
              " 'shoe': 29,\n",
              " 'floating': 15,\n",
              " 'sea?': 16,\n",
              " 'Because': 3791,\n",
              " 'shark': 9,\n",
              " 'burped.': 1,\n",
              " '': 7786,\n",
              " 'married': 56,\n",
              " 'bachelor?': 1,\n",
              " 'bachelor': 3,\n",
              " 'will': 462,\n",
              " 'go': 970,\n",
              " 'to': 10307,\n",
              " 'fridge,': 2,\n",
              " 'sees': 36,\n",
              " 'nothing': 64,\n",
              " 'he': 2538,\n",
              " 'wants,': 3,\n",
              " 'bed': 48,\n",
              " 'bed,': 9,\n",
              " 'fridge!': 1,\n",
              " 'are': 2516,\n",
              " 'there': 536,\n",
              " 'so': 1070,\n",
              " 'many': 1325,\n",
              " 'blood': 56,\n",
              " 'cells': 10,\n",
              " 'female': 122,\n",
              " 'prisons?': 3,\n",
              " 'sentences': 7,\n",
              " 'usually': 37,\n",
              " 'end': 144,\n",
              " 'periods.': 21,\n",
              " 'russian': 9,\n",
              " 'tree?': 91,\n",
              " 'Dimitree': 1,\n",
              " 'How': 4562,\n",
              " 'it': 4405,\n",
              " 'when': 3288,\n",
              " 'an': 2571,\n",
              " 'egg': 66,\n",
              " 'on': 3145,\n",
              " 'point?': 4,\n",
              " 'Egg': 4,\n",
              " 'zactly!': 1,\n",
              " \"Where'd\": 3,\n",
              " 'dog': 322,\n",
              " 'lost': 134,\n",
              " 'tail': 10,\n",
              " 'get': 2756,\n",
              " 'new': 451,\n",
              " 'one?': 43,\n",
              " 'retail': 9,\n",
              " 'store.': 17,\n",
              " 'epileptic': 59,\n",
              " 'oyster': 26,\n",
              " 'shucker': 8,\n",
              " 'diarrhea?': 40,\n",
              " 'fits': 21,\n",
              " 'went': 244,\n",
              " 'ball?': 26,\n",
              " 'pulled': 35,\n",
              " 'muscle': 6,\n",
              " 'frogs': 19,\n",
              " 'happy?': 21,\n",
              " 'They': 1594,\n",
              " 'eat': 419,\n",
              " 'whatever': 12,\n",
              " 'bugs': 12,\n",
              " 'them.': 184,\n",
              " 'turn': 164,\n",
              " 'Indian': 86,\n",
              " 'woman': 473,\n",
              " 'on?': 73,\n",
              " 'Press': 5,\n",
              " 'red': 181,\n",
              " 'button.': 5,\n",
              " 'Shall': 4,\n",
              " 'I': 3084,\n",
              " 'tell': 701,\n",
              " 'joke': 651,\n",
              " 'kidnappers?': 1,\n",
              " \"I'd\": 56,\n",
              " 'better': 198,\n",
              " 'not.': 20,\n",
              " 'You': 1324,\n",
              " 'might': 74,\n",
              " 'carried': 8,\n",
              " 'away.': 41,\n",
              " 'Do': 502,\n",
              " 'like': 1306,\n",
              " 'fish': 164,\n",
              " 'sticks?': 4,\n",
              " 'Well': 94,\n",
              " 'then,': 6,\n",
              " \"you're\": 352,\n",
              " 'gay': 453,\n",
              " 'fish.': 16,\n",
              " 'did': 5541,\n",
              " '2': 242,\n",
              " 'rednecks': 25,\n",
              " 'say': 2542,\n",
              " 'after': 697,\n",
              " 'breaking': 28,\n",
              " 'up?': 107,\n",
              " \"Let's\": 28,\n",
              " 'just': 854,\n",
              " 'be': 1127,\n",
              " 'cousins.': 2,\n",
              " 'pizza': 133,\n",
              " 'Jew?': 49,\n",
              " 'The': 2255,\n",
              " 'Pizza': 19,\n",
              " 'scream': 45,\n",
              " 'put': 367,\n",
              " 'oven.': 37,\n",
              " 'does': 3688,\n",
              " 'Santa': 158,\n",
              " 'have': 3125,\n",
              " 'three': 201,\n",
              " 'gardens?': 4,\n",
              " 'Q:': 195,\n",
              " 'A:': 230,\n",
              " 'So': 330,\n",
              " 'can': 1511,\n",
              " '\"hoe,': 1,\n",
              " 'hoe,': 3,\n",
              " 'hoe.\"': 1,\n",
              " 'hipster': 81,\n",
              " 'burn': 66,\n",
              " 'tongue?': 11,\n",
              " 'coffee': 53,\n",
              " 'before': 400,\n",
              " 'cool.': 41,\n",
              " 'Mozart': 13,\n",
              " 'kill': 214,\n",
              " 'chickens?': 6,\n",
              " 'were': 560,\n",
              " 'yelling': 13,\n",
              " '\"Bach': 1,\n",
              " 'Bach': 16,\n",
              " 'Bach\"': 2,\n",
              " \"pirate's\": 55,\n",
              " 'worst': 237,\n",
              " 'nightmare?': 10,\n",
              " 'sunken': 7,\n",
              " 'chest': 15,\n",
              " 'booty.': 9,\n",
              " \"can't\": 1022,\n",
              " 'e-mail': 3,\n",
              " 'photo': 14,\n",
              " 'Jedi?': 7,\n",
              " 'attachments': 3,\n",
              " 'forbidden.': 1,\n",
              " 'happen': 30,\n",
              " 'if': 1104,\n",
              " 'inside': 105,\n",
              " 'black': 954,\n",
              " 'hole?': 22,\n",
              " \"don't\": 1304,\n",
              " 'know': 1567,\n",
              " 'either.': 17,\n",
              " 'It': 873,\n",
              " 'must': 40,\n",
              " 'out-of-this-world.': 1,\n",
              " 'came': 218,\n",
              " 'first,': 17,\n",
              " 'Chicken': 42,\n",
              " 'Egg?': 1,\n",
              " 'Rooster': 2,\n",
              " 'common': 123,\n",
              " 'good': 553,\n",
              " 'boyfriend': 24,\n",
              " 'lion?': 12,\n",
              " \"They're\": 291,\n",
              " 'both': 730,\n",
              " 'ready': 26,\n",
              " 'semen': 21,\n",
              " 'basement?': 6,\n",
              " 'know.': 55,\n",
              " \"That's\": 83,\n",
              " 'way': 292,\n",
              " \"it's\": 736,\n",
              " 'spelled.': 1,\n",
              " 'your': 2268,\n",
              " 'wife': 237,\n",
              " 'starts': 71,\n",
              " 'smoking?': 8,\n",
              " 'Slow': 15,\n",
              " 'down': 429,\n",
              " 'apply': 6,\n",
              " 'lube': 9,\n",
              " 'Want': 120,\n",
              " 'UDP?': 1,\n",
              " 'Never': 41,\n",
              " 'mind.': 13,\n",
              " \"won't\": 162,\n",
              " 'it,': 144,\n",
              " 'care': 49,\n",
              " '[OC]': 28,\n",
              " 'Person': 20,\n",
              " '1:': 15,\n",
              " 'parents': 76,\n",
              " 'do?': 82,\n",
              " \"2:They're\": 1,\n",
              " 'doctors,': 2,\n",
              " 'what': 1125,\n",
              " 'beat': 90,\n",
              " 'me': 621,\n",
              " 'pool': 47,\n",
              " 'sticks.': 1,\n",
              " '2:': 11,\n",
              " 'Oh,': 34,\n",
              " 'they': 2397,\n",
              " 'really': 385,\n",
              " 'at': 1697,\n",
              " 'billiards.': 1,\n",
              " 'species': 10,\n",
              " 'rappers?': 3,\n",
              " 'Dragons,': 1,\n",
              " 'because': 522,\n",
              " \"they're\": 374,\n",
              " 'always': 680,\n",
              " \"spittin'\": 1,\n",
              " 'fire.': 21,\n",
              " 'elves': 6,\n",
              " 'make': 907,\n",
              " 'toys?': 4,\n",
              " 'Leave': 13,\n",
              " 'my': 1228,\n",
              " 'presents': 11,\n",
              " 'statistically': 2,\n",
              " 'times': 60,\n",
              " 'worse': 132,\n",
              " 'than': 548,\n",
              " 'war?': 19,\n",
              " 'Three': 33,\n",
              " 'wars': 5,\n",
              " 'row': 20,\n",
              " 'hares': 2,\n",
              " 'hopping': 3,\n",
              " 'backward?': 4,\n",
              " 'receding': 9,\n",
              " 'line.': 29,\n",
              " \"I've\": 130,\n",
              " 'got': 824,\n",
              " 'dead': 305,\n",
              " 'budgie': 1,\n",
              " 'for': 2485,\n",
              " 'sale,': 1,\n",
              " 'anyone': 136,\n",
              " 'interested?': 1,\n",
              " \"isn't\": 106,\n",
              " 'going': 312,\n",
              " 'cheep.': 1,\n",
              " 'politically': 6,\n",
              " 'correct': 10,\n",
              " 'name': 292,\n",
              " '\"African': 1,\n",
              " 'Americans': 70,\n",
              " 'Down': 12,\n",
              " 'Syndrome\"': 1,\n",
              " 'group?': 12,\n",
              " 'Black': 95,\n",
              " 'Lives': 7,\n",
              " 'Matter': 5,\n",
              " 'Edit1:': 1,\n",
              " 'No': 173,\n",
              " 'Im': 21,\n",
              " 'not': 857,\n",
              " 'targeting': 2,\n",
              " 'people,': 14,\n",
              " 'people': 1012,\n",
              " 'actual': 7,\n",
              " 'syndrome,': 1,\n",
              " 'group': 181,\n",
              " 'scarecrow': 31,\n",
              " 'promotion?': 4,\n",
              " 'outstanding': 34,\n",
              " 'field': 34,\n",
              " 'blondes': 40,\n",
              " 'take': 1412,\n",
              " 'screw': 417,\n",
              " 'lightbulb?': 404,\n",
              " '5': 119,\n",
              " '1': 141,\n",
              " 'hold': 93,\n",
              " 'bulb': 62,\n",
              " '4': 137,\n",
              " 'spin': 11,\n",
              " 'ladder.': 14,\n",
              " 'NYC': 4,\n",
              " 'nanosecond?': 1,\n",
              " 'If': 492,\n",
              " 'stuck': 101,\n",
              " 'light,': 1,\n",
              " 'its': 358,\n",
              " 'time': 394,\n",
              " 'takes': 147,\n",
              " 'occupant': 1,\n",
              " 'car': 253,\n",
              " 'behind': 67,\n",
              " 'honk': 1,\n",
              " 'horn': 8,\n",
              " 'light': 490,\n",
              " 'turns': 61,\n",
              " 'green.': 7,\n",
              " 'Where': 628,\n",
              " 'Tumblr': 12,\n",
              " 'users': 21,\n",
              " 'pray?': 2,\n",
              " 'Cis-Teen': 1,\n",
              " 'Chapel': 2,\n",
              " 'left': 223,\n",
              " 'leg': 60,\n",
              " 'right': 211,\n",
              " 'leg?': 52,\n",
              " 'That': 83,\n",
              " 'one': 1545,\n",
              " 'middle': 92,\n",
              " 'thinks': 25,\n",
              " \"he's\": 280,\n",
              " 'hard.': 30,\n",
              " 'below': 16,\n",
              " 'sha-na-na-na-na-na-na-na-na': 1,\n",
              " 'knees?': 9,\n",
              " 'Your': 160,\n",
              " 'Guns': 6,\n",
              " \"N'\": 4,\n",
              " 'Toeses': 1,\n",
              " \"I'll\": 249,\n",
              " 'see': 519,\n",
              " 'myself': 134,\n",
              " 'out.': 161,\n",
              " 'Humans': 4,\n",
              " 'Reproduce?': 1,\n",
              " 'Sexually': 5,\n",
              " 'B:': 14,\n",
              " 'C:': 6,\n",
              " 'Nun': 23,\n",
              " 'save': 59,\n",
              " 'herself': 10,\n",
              " 'from': 1184,\n",
              " 'being': 351,\n",
              " 'poisoned?': 1,\n",
              " 'chucks.': 2,\n",
              " 'Michael': 171,\n",
              " 'J.': 26,\n",
              " 'Fox': 32,\n",
              " 'milkshakes?': 5,\n",
              " 'uses': 37,\n",
              " 'finest': 2,\n",
              " 'ingredients.': 5,\n",
              " 'Donald': 279,\n",
              " 'Trump': 303,\n",
              " 'orange?': 6,\n",
              " 'Orange': 26,\n",
              " 'thick': 11,\n",
              " 'skin': 43,\n",
              " 'actually': 100,\n",
              " 'it.': 515,\n",
              " 'job': 103,\n",
              " 'Model': 2,\n",
              " 'T': 19,\n",
              " 'factory?': 34,\n",
              " 'heard': 525,\n",
              " 'great': 157,\n",
              " 'line': 82,\n",
              " 'work.': 37,\n",
              " 'life': 76,\n",
              " 'as': 543,\n",
              " 'virgin?': 10,\n",
              " 'Its': 71,\n",
              " 'hard': 219,\n",
              " 'Caitlyn': 20,\n",
              " 'Jenner': 22,\n",
              " 'lie': 19,\n",
              " 'her': 1064,\n",
              " 'kids?': 54,\n",
              " \"She's\": 37,\n",
              " 'transparent.': 4,\n",
              " 'loses': 11,\n",
              " 'tail?': 11,\n",
              " 'Goes': 3,\n",
              " 'store': 66,\n",
              " 'find': 413,\n",
              " 'another': 167,\n",
              " 'one.': 97,\n",
              " 'Mexicans': 128,\n",
              " 'drivers': 28,\n",
              " 'Ed': 12,\n",
              " 'sex': 399,\n",
              " 'same': 187,\n",
              " 'day?': 87,\n",
              " 'give': 327,\n",
              " 'donkey': 27,\n",
              " 'break.': 4,\n",
              " 'Is': 187,\n",
              " 'this': 788,\n",
              " 'InkJet': 1,\n",
              " 'any': 362,\n",
              " 'good?': 15,\n",
              " 'Sure,': 7,\n",
              " \"we've\": 5,\n",
              " 'sold': 16,\n",
              " 'royalty': 1,\n",
              " 'Princesses?': 1,\n",
              " 'Mate,': 1,\n",
              " 'prints': 5,\n",
              " 'ALL': 4,\n",
              " 'letters!': 2,\n",
              " 'Router': 1,\n",
              " 'released': 9,\n",
              " 'early': 26,\n",
              " 'prison?': 60,\n",
              " 'had': 834,\n",
              " 'connections.': 1,\n",
              " 'Whats': 452,\n",
              " 'bodybuilder': 9,\n",
              " 'whose': 31,\n",
              " 'fan': 37,\n",
              " 'X-Men?': 1,\n",
              " 'Huge': 2,\n",
              " 'Jackman': 1,\n",
              " 'failed': 19,\n",
              " 'erection?': 20,\n",
              " 'feelings.': 2,\n",
              " 'Have': 478,\n",
              " 'corduroy': 24,\n",
              " 'pillows?': 16,\n",
              " 'making': 150,\n",
              " 'headlines': 9,\n",
              " 'everywhere.': 8,\n",
              " 'Guess': 21,\n",
              " 'how': 559,\n",
              " 'girlfriends': 10,\n",
              " 'now?': 25,\n",
              " 'Well,': 102,\n",
              " 'even': 133,\n",
              " 'count,': 2,\n",
              " 'range': 5,\n",
              " 'many.': 7,\n",
              " '<1.': 1,\n",
              " 'opposite': 61,\n",
              " 'Christopher': 36,\n",
              " 'Reeve?': 3,\n",
              " 'Walken': 11,\n",
              " 'pencil': 17,\n",
              " 'marijuana?': 5,\n",
              " 'look': 198,\n",
              " 'smart,': 2,\n",
              " 'blunt.': 1,\n",
              " 'explain': 35,\n",
              " 'puns': 14,\n",
              " 'kleptomaniacs?': 5,\n",
              " 'things': 76,\n",
              " 'literally.': 6,\n",
              " 'cancer': 43,\n",
              " 'Brazilian?': 1,\n",
              " 'Cancer': 25,\n",
              " 'evolves,': 1,\n",
              " 'Brazilian': 10,\n",
              " \"doesn't.\": 5,\n",
              " 'OP': 21,\n",
              " 'eggs?': 33,\n",
              " 'Eggs': 8,\n",
              " 'laid.': 7,\n",
              " 'mix': 66,\n",
              " 'insurance': 11,\n",
              " 'company': 30,\n",
              " 'NFL': 17,\n",
              " 'quarterback?': 1,\n",
              " 'An': 400,\n",
              " 'Aflacco': 1,\n",
              " 'why': 405,\n",
              " 'Amish': 33,\n",
              " 'girl': 366,\n",
              " 'excommunicated?': 2,\n",
              " 'Too': 43,\n",
              " 'Mennonite': 6,\n",
              " 'someone': 409,\n",
              " 'unemployed': 17,\n",
              " 'gender-expert?': 1,\n",
              " 'Nothing': 36,\n",
              " 'Norwegian': 3,\n",
              " 'Beef': 31,\n",
              " 'Stew?': 1,\n",
              " 'Stew': 2,\n",
              " 'all': 1130,\n",
              " 'pirates': 82,\n",
              " 'buried': 22,\n",
              " 'treasure?': 1,\n",
              " 'Thanks': 37,\n",
              " 'gold!': 2,\n",
              " 'dad': 133,\n",
              " 'Carly': 2,\n",
              " 'Rae': 1,\n",
              " 'Jepsen': 1,\n",
              " 'common?': 895,\n",
              " 'said': 207,\n",
              " 'WWI': 4,\n",
              " 'vet': 6,\n",
              " 'angry': 55,\n",
              " 'German': 97,\n",
              " 'veteran?': 1,\n",
              " \"Can't\": 21,\n",
              " 'we': 317,\n",
              " 'let': 136,\n",
              " \"Argonne's\": 1,\n",
              " \"Argonne's?\": 1,\n",
              " 'cops': 50,\n",
              " 'change': 567,\n",
              " 'bulb?': 335,\n",
              " \"don't.\": 21,\n",
              " 'room': 91,\n",
              " 'black.': 38,\n",
              " 'near-sighted': 4,\n",
              " 'gynecologist': 36,\n",
              " 'puppy': 16,\n",
              " 'wet': 42,\n",
              " 'nose': 40,\n",
              " 'two': 718,\n",
              " 'bananas?': 5,\n",
              " 'Slippers': 1,\n",
              " 'nice': 44,\n",
              " 'girls': 188,\n",
              " 'assholes?': 5,\n",
              " 'pegging.': 2,\n",
              " 'farmer': 83,\n",
              " 'tractor?': 11,\n",
              " \"Where's\": 33,\n",
              " 'Whatâ€™s': 62,\n",
              " 'dicks?': 16,\n",
              " 'Redditors': 17,\n",
              " 'canâ€™t': 9,\n",
              " 'joke.': 115,\n",
              " 'part': 376,\n",
              " 'eating': 125,\n",
              " 'vegetables?': 22,\n",
              " 'Finding': 21,\n",
              " 'place': 134,\n",
              " 'wheelchairs.': 3,\n",
              " 'should': 288,\n",
              " 'talk': 94,\n",
              " 'back?': 38,\n",
              " 'Fart': 3,\n",
              " \"Desiigner's\": 1,\n",
              " 'favorite': 1119,\n",
              " 'car?': 155,\n",
              " 'Kia': 8,\n",
              " 'cheaper,': 1,\n",
              " 'Beer': 16,\n",
              " 'Nuts': 7,\n",
              " 'Deer': 17,\n",
              " 'Nuts?': 3,\n",
              " 'Nuts.': 2,\n",
              " '$1.49,': 1,\n",
              " 'while': 237,\n",
              " 'nuts': 84,\n",
              " 'under': 120,\n",
              " 'Buck!': 2,\n",
              " 'bird': 98,\n",
              " 'peace': 11,\n",
              " 'dove,': 1,\n",
              " \"what's\": 206,\n",
              " 'love?': 33,\n",
              " 'swallow.': 6,\n",
              " 'Hippo': 3,\n",
              " 'Zippo?': 5,\n",
              " 'heavy': 20,\n",
              " 'Zippo': 3,\n",
              " 'lighter.': 12,\n",
              " 'every': 191,\n",
              " 'infomercial': 1,\n",
              " 'Kuwait?': 1,\n",
              " 'But': 79,\n",
              " 'Kuwait,': 1,\n",
              " \"there's\": 158,\n",
              " 'more!': 1,\n",
              " 'shin': 2,\n",
              " '?': 434,\n",
              " 'Tony': 19,\n",
              " 'Fonzie': 4,\n",
              " 'stop': 237,\n",
              " 'sleeping': 33,\n",
              " 'around?': 14,\n",
              " 'AIIIIIDS.': 1,\n",
              " 'embarrasses': 1,\n",
              " 'archaeologist?': 10,\n",
              " 'Give': 61,\n",
              " 'him': 499,\n",
              " 'tampon': 45,\n",
              " 'ask': 179,\n",
              " 'period': 54,\n",
              " 'from.': 17,\n",
              " \"Why'd\": 25,\n",
              " 'vulture': 6,\n",
              " 'check': 42,\n",
              " 'bag?': 24,\n",
              " 'airline': 7,\n",
              " \"didn't\": 504,\n",
              " 'allow': 15,\n",
              " 'carrion': 3,\n",
              " 'luggage.': 1,\n",
              " 'drug': 85,\n",
              " 'addicts': 5,\n",
              " 'hang': 74,\n",
              " 'out': 1437,\n",
              " 'beach?': 55,\n",
              " 'getting': 251,\n",
              " 'sand': 20,\n",
              " 'their': 1086,\n",
              " 'crack.': 7,\n",
              " 'seeing': 30,\n",
              " 'Rouge': 3,\n",
              " 'week?': 13,\n",
              " 'prequel': 4,\n",
              " 'Maroon': 2,\n",
              " '5.': 6,\n",
              " 'high': 145,\n",
              " 'drunk': 55,\n",
              " 'driver?': 19,\n",
              " 'driver': 55,\n",
              " 'goes': 261,\n",
              " 'through': 203,\n",
              " 'sign,': 4,\n",
              " 'waits': 26,\n",
              " \"Wife:Isn't\": 1,\n",
              " 'hot': 130,\n",
              " 'here': 121,\n",
              " 'hun?': 1,\n",
              " 'Troll': 1,\n",
              " 'husband': 57,\n",
              " ':': 76,\n",
              " 'kinda': 17,\n",
              " 'is,': 13,\n",
              " 'ill': 13,\n",
              " 'adjust': 3,\n",
              " 'AC.': 1,\n",
              " 'Knock': 82,\n",
              " 'knock.': 32,\n",
              " \"Who's\": 133,\n",
              " 'there?': 87,\n",
              " 'Dave.': 3,\n",
              " 'Dave': 11,\n",
              " 'who?': 29,\n",
              " 'promptly': 2,\n",
              " 'burst': 7,\n",
              " 'into': 650,\n",
              " 'tears': 10,\n",
              " 'everyone': 136,\n",
              " 'world': 105,\n",
              " 'knew': 23,\n",
              " 'Mohammad': 4,\n",
              " 'Holy': 23,\n",
              " 'Land?': 2,\n",
              " 'Makkah-roni': 1,\n",
              " 'cheese!': 2,\n",
              " 'Boy': 34,\n",
              " 'Girl-': 1,\n",
              " 'Hi': 17,\n",
              " 'Sweetheart,': 1,\n",
              " 'day': 180,\n",
              " 'going?': 6,\n",
              " 'G-': 2,\n",
              " 'Pretty': 11,\n",
              " 'well,': 18,\n",
              " 'want': 493,\n",
              " 'walk': 94,\n",
              " 'B-': 2,\n",
              " 'friendzone': 3,\n",
              " 'tried': 113,\n",
              " 'escape.': 1,\n",
              " 'last': 278,\n",
              " 'minds': 4,\n",
              " '9/11': 34,\n",
              " 'jumpers?': 3,\n",
              " 'Their': 91,\n",
              " 'ankles': 4,\n",
              " 'Korean': 33,\n",
              " 'food?': 115,\n",
              " 'Seoul': 6,\n",
              " 'overweight': 15,\n",
              " 'bounty': 2,\n",
              " 'hunter?': 1,\n",
              " 'Boba': 4,\n",
              " 'Fat': 8,\n",
              " 'beaver': 6,\n",
              " 'fell': 112,\n",
              " 'water?': 76,\n",
              " 'Damn': 62,\n",
              " 'Mexico': 34,\n",
              " 'willing': 6,\n",
              " 'pay': 127,\n",
              " 'build': 45,\n",
              " 'wall': 52,\n",
              " 'US': 66,\n",
              " 'Mexico?': 29,\n",
              " 'finally': 34,\n",
              " 'Olympic': 27,\n",
              " 'team.': 8,\n",
              " 'float?': 8,\n",
              " 'Easy!': 1,\n",
              " 'Just': 191,\n",
              " 'add': 20,\n",
              " 'Root': 4,\n",
              " 'beer': 82,\n",
              " 'Ice': 20,\n",
              " 'Cream!': 1,\n",
              " 'Muslim': 74,\n",
              " 'house': 102,\n",
              " 'wives': 5,\n",
              " 'could': 306,\n",
              " 'KFC?': 6,\n",
              " 'harem,': 1,\n",
              " 'fowl.': 5,\n",
              " 'bit': 68,\n",
              " 'travel': 33,\n",
              " 'CPU': 3,\n",
              " 'HDD?': 1,\n",
              " 'took': 94,\n",
              " 'bus.': 11,\n",
              " 'streaker': 2,\n",
              " 'church?': 46,\n",
              " 'caught': 85,\n",
              " 'by': 632,\n",
              " 'organ': 14,\n",
              " 'called': 376,\n",
              " 'police': 111,\n",
              " 'work': 189,\n",
              " 'overtime?': 2,\n",
              " 'Copper': 3,\n",
              " 'Nitrate.': 1,\n",
              " 'Jewish': 238,\n",
              " 'men': 310,\n",
              " 'circumcised': 11,\n",
              " 'Cause': 159,\n",
              " 'women': 330,\n",
              " 'touch': 29,\n",
              " 'anything': 69,\n",
              " \"that's\": 191,\n",
              " '10%': 12,\n",
              " 'off.': 99,\n",
              " 'riot': 7,\n",
              " 'early?': 16,\n",
              " 'To': 386,\n",
              " 'crowd': 10,\n",
              " 'whistle-blower': 2,\n",
              " 'during': 156,\n",
              " 'Russian': 72,\n",
              " 'blizzard?': 5,\n",
              " 'Nothing,': 77,\n",
              " 'Snowden.': 12,\n",
              " 'buttock': 1,\n",
              " 'implants': 3,\n",
              " 'popular': 64,\n",
              " 'Australia?': 10,\n",
              " 'boob': 18,\n",
              " 'jobs': 15,\n",
              " 'cheaper': 8,\n",
              " 'under.': 4,\n",
              " 'would': 784,\n",
              " 'first': 356,\n",
              " 'thing': 451,\n",
              " 'D.': 5,\n",
              " 'write': 37,\n",
              " 'Death': 15,\n",
              " 'Note': 10,\n",
              " '(if': 3,\n",
              " 'exist)??': 1,\n",
              " '\"YUUUUUGEE\"': 1,\n",
              " 'horse': 76,\n",
              " 'other': 747,\n",
              " 'horse?': 31,\n",
              " 'Hay,I': 1,\n",
              " 'thought': 148,\n",
              " 'horses': 23,\n",
              " \"couldn't\": 405,\n",
              " 'speak!': 1,\n",
              " 'suicide': 56,\n",
              " 'bomber': 12,\n",
              " 'teaching': 8,\n",
              " 'class?': 32,\n",
              " 'Pay': 10,\n",
              " 'attention!': 1,\n",
              " \"I'm\": 551,\n",
              " 'only': 679,\n",
              " 'show': 126,\n",
              " 'once.': 23,\n",
              " 'chicken': 239,\n",
              " 'cat': 129,\n",
              " 'mashed': 6,\n",
              " 'potatoes?': 7,\n",
              " 'side.': 68,\n",
              " 'ass': 107,\n",
              " 'cheek': 8,\n",
              " 'other?': 141,\n",
              " 'We': 81,\n",
              " 'gotta': 32,\n",
              " 'shit': 165,\n",
              " 'together.': 21,\n",
              " 'clickbait': 3,\n",
              " 'articles': 2,\n",
              " 'answer': 46,\n",
              " 'may': 53,\n",
              " 'shock': 5,\n",
              " 'you.': 161,\n",
              " 'UK': 7,\n",
              " 'banning': 6,\n",
              " 'hummus?': 1,\n",
              " \"It's\": 589,\n",
              " 'chickpea': 14,\n",
              " 'android?': 1,\n",
              " 'Synthia': 2,\n",
              " 'Lmao': 3,\n",
              " 'Police': 30,\n",
              " 'Officer:': 2,\n",
              " '\"Can': 14,\n",
              " 'identify': 5,\n",
              " 'yourself,': 4,\n",
              " 'Sir\"?': 1,\n",
              " 'Driver': 3,\n",
              " 'pulls': 9,\n",
              " 'mirror': 9,\n",
              " 'says:': 14,\n",
              " '\"Yes,': 8,\n",
              " 'me\".': 1,\n",
              " '11': 27,\n",
              " '&': 99,\n",
              " '2?': 8,\n",
              " 'Cowboys': 10,\n",
              " 'Mexican': 336,\n",
              " 'drive?': 46,\n",
              " 'Quebrolet.': 1,\n",
              " 'brothel': 15,\n",
              " 'riddled': 1,\n",
              " 'rabies?': 1,\n",
              " 'frothel': 1,\n",
              " 'mature': 6,\n",
              " 'immature': 10,\n",
              " 'sense': 31,\n",
              " 'humor?': 11,\n",
              " 'poop.': 5,\n",
              " 'Usain': 22,\n",
              " 'Bolt': 17,\n",
              " 'standing': 57,\n",
              " 'next': 115,\n",
              " 'mom?': 15,\n",
              " 'runner': 8,\n",
              " 'scoring': 1,\n",
              " 'position.': 11,\n",
              " 'happens': 254,\n",
              " 'pirate': 144,\n",
              " '60?': 2,\n",
              " 'joins': 3,\n",
              " 'AARP': 1,\n",
              " 'live': 82,\n",
              " 'matters': 1,\n",
              " 'protester': 2,\n",
              " 'knows': 82,\n",
              " 'too': 408,\n",
              " 'dark': 58,\n",
              " 'ten': 53,\n",
              " 'night?': 78,\n",
              " 'found': 142,\n",
              " 'teepee': 1,\n",
              " 'bear': 110,\n",
              " 'teeth?': 32,\n",
              " 'Bare': 1,\n",
              " 'Grylls': 7,\n",
              " 'Tribe': 1,\n",
              " 'Called': 3,\n",
              " 'Quest': 1,\n",
              " 'margarine': 2,\n",
              " 'butter,': 2,\n",
              " 'baby.': 16,\n",
              " 'cross': 524,\n",
              " 'road?': 232,\n",
              " 'house...': 1,\n",
              " 'Knock,': 8,\n",
              " 'chicken.': 42,\n",
              " 'painter': 5,\n",
              " 'finishing': 5,\n",
              " 'portrait': 1,\n",
              " 'brother': 41,\n",
              " 'Andrew?': 1,\n",
              " 'drew': 4,\n",
              " 'snowman': 33,\n",
              " 'cons': 2,\n",
              " 'people?': 105,\n",
              " 'snowfake': 1,\n",
              " 'appropriate': 12,\n",
              " 'year.': 34,\n",
              " 'Merry': 7,\n",
              " 'Christmas': 100,\n",
              " 'reddit!': 4,\n",
              " 'permission': 3,\n",
              " 'Xmas': 2,\n",
              " 'parties': 8,\n",
              " 'attend': 7,\n",
              " 'snail': 14,\n",
              " 'sail': 5,\n",
              " 'boat?': 32,\n",
              " 'snailor': 2,\n",
              " 'Tom': 32,\n",
              " 'hanks': 1,\n",
              " 'woods?': 17,\n",
              " 'forrest': 2,\n",
              " 'dump.': 2,\n",
              " 'herpes': 5,\n",
              " 'pun?': 6,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih-hNhOEcXGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efeb24a1-08f5-4e0c-e977-9a5b864b7e40"
      },
      "source": [
        "voc.word2index"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Did': 3,\n",
              " 'you': 4,\n",
              " 'hear': 5,\n",
              " 'about': 6,\n",
              " 'the': 7,\n",
              " 'Native': 8,\n",
              " 'American': 9,\n",
              " 'man': 10,\n",
              " 'that': 11,\n",
              " 'drank': 12,\n",
              " '200': 13,\n",
              " 'cups': 14,\n",
              " 'of': 15,\n",
              " 'tea?': 16,\n",
              " 'He': 17,\n",
              " 'nearly': 18,\n",
              " 'drown': 19,\n",
              " 'in': 20,\n",
              " 'his': 21,\n",
              " 'own': 22,\n",
              " 'tea': 23,\n",
              " 'pee.': 24,\n",
              " \"What's\": 25,\n",
              " 'best': 26,\n",
              " 'anti': 27,\n",
              " 'diarrheal': 28,\n",
              " 'prescription?': 29,\n",
              " 'Mycheexarphlexin': 30,\n",
              " 'What': 31,\n",
              " 'do': 32,\n",
              " 'call': 33,\n",
              " 'a': 34,\n",
              " 'person': 35,\n",
              " 'who': 36,\n",
              " 'is': 37,\n",
              " 'outside': 38,\n",
              " 'door': 39,\n",
              " 'and': 40,\n",
              " 'has': 41,\n",
              " 'no': 42,\n",
              " 'arms': 43,\n",
              " 'nor': 44,\n",
              " 'legs?': 45,\n",
              " 'Matt': 46,\n",
              " 'Which': 47,\n",
              " 'Star': 48,\n",
              " 'Trek': 49,\n",
              " 'character': 50,\n",
              " 'member': 51,\n",
              " 'magic': 52,\n",
              " 'circle?': 53,\n",
              " 'Jean-Luc': 54,\n",
              " 'Pickacard': 55,\n",
              " 'difference': 56,\n",
              " 'between': 57,\n",
              " 'bullet': 58,\n",
              " 'human?': 59,\n",
              " 'A': 60,\n",
              " \"doesn't\": 61,\n",
              " 'miss': 62,\n",
              " 'Harambe': 63,\n",
              " 'Why': 64,\n",
              " 'was': 65,\n",
              " 'Ethiopian': 66,\n",
              " 'baby': 67,\n",
              " 'crying?': 68,\n",
              " 'having': 69,\n",
              " 'mid-life': 70,\n",
              " 'crisis': 71,\n",
              " 'corn': 72,\n",
              " 'husker': 73,\n",
              " 'with': 74,\n",
              " 'epilepsy': 75,\n",
              " 'hooker': 76,\n",
              " 'dysentery?': 77,\n",
              " 'One': 78,\n",
              " 'shucks': 79,\n",
              " 'fits...': 80,\n",
              " 'Who': 81,\n",
              " \"2016's\": 82,\n",
              " 'biggest': 83,\n",
              " 'sellout?': 84,\n",
              " 'Kevin': 85,\n",
              " 'Durant': 86,\n",
              " 'or': 87,\n",
              " 'Bernie': 88,\n",
              " 'Sanders?': 89,\n",
              " 'little': 90,\n",
              " \"Annie's\": 91,\n",
              " 'shoe': 92,\n",
              " 'floating': 93,\n",
              " 'sea?': 94,\n",
              " 'Because': 95,\n",
              " 'shark': 96,\n",
              " 'burped.': 97,\n",
              " '': 98,\n",
              " 'married': 99,\n",
              " 'bachelor?': 100,\n",
              " 'bachelor': 101,\n",
              " 'will': 102,\n",
              " 'go': 103,\n",
              " 'to': 104,\n",
              " 'fridge,': 105,\n",
              " 'sees': 106,\n",
              " 'nothing': 107,\n",
              " 'he': 108,\n",
              " 'wants,': 109,\n",
              " 'bed': 110,\n",
              " 'bed,': 111,\n",
              " 'fridge!': 112,\n",
              " 'are': 113,\n",
              " 'there': 114,\n",
              " 'so': 115,\n",
              " 'many': 116,\n",
              " 'blood': 117,\n",
              " 'cells': 118,\n",
              " 'female': 119,\n",
              " 'prisons?': 120,\n",
              " 'sentences': 121,\n",
              " 'usually': 122,\n",
              " 'end': 123,\n",
              " 'periods.': 124,\n",
              " 'russian': 125,\n",
              " 'tree?': 126,\n",
              " 'Dimitree': 127,\n",
              " 'How': 128,\n",
              " 'it': 129,\n",
              " 'when': 130,\n",
              " 'an': 131,\n",
              " 'egg': 132,\n",
              " 'on': 133,\n",
              " 'point?': 134,\n",
              " 'Egg': 135,\n",
              " 'zactly!': 136,\n",
              " \"Where'd\": 137,\n",
              " 'dog': 138,\n",
              " 'lost': 139,\n",
              " 'tail': 140,\n",
              " 'get': 141,\n",
              " 'new': 142,\n",
              " 'one?': 143,\n",
              " 'retail': 144,\n",
              " 'store.': 145,\n",
              " 'epileptic': 146,\n",
              " 'oyster': 147,\n",
              " 'shucker': 148,\n",
              " 'diarrhea?': 149,\n",
              " 'fits': 150,\n",
              " 'went': 151,\n",
              " 'ball?': 152,\n",
              " 'pulled': 153,\n",
              " 'muscle': 154,\n",
              " 'frogs': 155,\n",
              " 'happy?': 156,\n",
              " 'They': 157,\n",
              " 'eat': 158,\n",
              " 'whatever': 159,\n",
              " 'bugs': 160,\n",
              " 'them.': 161,\n",
              " 'turn': 162,\n",
              " 'Indian': 163,\n",
              " 'woman': 164,\n",
              " 'on?': 165,\n",
              " 'Press': 166,\n",
              " 'red': 167,\n",
              " 'button.': 168,\n",
              " 'Shall': 169,\n",
              " 'I': 170,\n",
              " 'tell': 171,\n",
              " 'joke': 172,\n",
              " 'kidnappers?': 173,\n",
              " \"I'd\": 174,\n",
              " 'better': 175,\n",
              " 'not.': 176,\n",
              " 'You': 177,\n",
              " 'might': 178,\n",
              " 'carried': 179,\n",
              " 'away.': 180,\n",
              " 'Do': 181,\n",
              " 'like': 182,\n",
              " 'fish': 183,\n",
              " 'sticks?': 184,\n",
              " 'Well': 185,\n",
              " 'then,': 186,\n",
              " \"you're\": 187,\n",
              " 'gay': 188,\n",
              " 'fish.': 189,\n",
              " 'did': 190,\n",
              " '2': 191,\n",
              " 'rednecks': 192,\n",
              " 'say': 193,\n",
              " 'after': 194,\n",
              " 'breaking': 195,\n",
              " 'up?': 196,\n",
              " \"Let's\": 197,\n",
              " 'just': 198,\n",
              " 'be': 199,\n",
              " 'cousins.': 200,\n",
              " 'pizza': 201,\n",
              " 'Jew?': 202,\n",
              " 'The': 203,\n",
              " 'Pizza': 204,\n",
              " 'scream': 205,\n",
              " 'put': 206,\n",
              " 'oven.': 207,\n",
              " 'does': 208,\n",
              " 'Santa': 209,\n",
              " 'have': 210,\n",
              " 'three': 211,\n",
              " 'gardens?': 212,\n",
              " 'Q:': 213,\n",
              " 'A:': 214,\n",
              " 'So': 215,\n",
              " 'can': 216,\n",
              " '\"hoe,': 217,\n",
              " 'hoe,': 218,\n",
              " 'hoe.\"': 219,\n",
              " 'hipster': 220,\n",
              " 'burn': 221,\n",
              " 'tongue?': 222,\n",
              " 'coffee': 223,\n",
              " 'before': 224,\n",
              " 'cool.': 225,\n",
              " 'Mozart': 226,\n",
              " 'kill': 227,\n",
              " 'chickens?': 228,\n",
              " 'were': 229,\n",
              " 'yelling': 230,\n",
              " '\"Bach': 231,\n",
              " 'Bach': 232,\n",
              " 'Bach\"': 233,\n",
              " \"pirate's\": 234,\n",
              " 'worst': 235,\n",
              " 'nightmare?': 236,\n",
              " 'sunken': 237,\n",
              " 'chest': 238,\n",
              " 'booty.': 239,\n",
              " \"can't\": 240,\n",
              " 'e-mail': 241,\n",
              " 'photo': 242,\n",
              " 'Jedi?': 243,\n",
              " 'attachments': 244,\n",
              " 'forbidden.': 245,\n",
              " 'happen': 246,\n",
              " 'if': 247,\n",
              " 'inside': 248,\n",
              " 'black': 249,\n",
              " 'hole?': 250,\n",
              " \"don't\": 251,\n",
              " 'know': 252,\n",
              " 'either.': 253,\n",
              " 'It': 254,\n",
              " 'must': 255,\n",
              " 'out-of-this-world.': 256,\n",
              " 'came': 257,\n",
              " 'first,': 258,\n",
              " 'Chicken': 259,\n",
              " 'Egg?': 260,\n",
              " 'Rooster': 261,\n",
              " 'common': 262,\n",
              " 'good': 263,\n",
              " 'boyfriend': 264,\n",
              " 'lion?': 265,\n",
              " \"They're\": 266,\n",
              " 'both': 267,\n",
              " 'ready': 268,\n",
              " 'semen': 269,\n",
              " 'basement?': 270,\n",
              " 'know.': 271,\n",
              " \"That's\": 272,\n",
              " 'way': 273,\n",
              " \"it's\": 274,\n",
              " 'spelled.': 275,\n",
              " 'your': 276,\n",
              " 'wife': 277,\n",
              " 'starts': 278,\n",
              " 'smoking?': 279,\n",
              " 'Slow': 280,\n",
              " 'down': 281,\n",
              " 'apply': 282,\n",
              " 'lube': 283,\n",
              " 'Want': 284,\n",
              " 'UDP?': 285,\n",
              " 'Never': 286,\n",
              " 'mind.': 287,\n",
              " \"won't\": 288,\n",
              " 'it,': 289,\n",
              " 'care': 290,\n",
              " '[OC]': 291,\n",
              " 'Person': 292,\n",
              " '1:': 293,\n",
              " 'parents': 294,\n",
              " 'do?': 295,\n",
              " \"2:They're\": 296,\n",
              " 'doctors,': 297,\n",
              " 'what': 298,\n",
              " 'beat': 299,\n",
              " 'me': 300,\n",
              " 'pool': 301,\n",
              " 'sticks.': 302,\n",
              " '2:': 303,\n",
              " 'Oh,': 304,\n",
              " 'they': 305,\n",
              " 'really': 306,\n",
              " 'at': 307,\n",
              " 'billiards.': 308,\n",
              " 'species': 309,\n",
              " 'rappers?': 310,\n",
              " 'Dragons,': 311,\n",
              " 'because': 312,\n",
              " \"they're\": 313,\n",
              " 'always': 314,\n",
              " \"spittin'\": 315,\n",
              " 'fire.': 316,\n",
              " 'elves': 317,\n",
              " 'make': 318,\n",
              " 'toys?': 319,\n",
              " 'Leave': 320,\n",
              " 'my': 321,\n",
              " 'presents': 322,\n",
              " 'statistically': 323,\n",
              " 'times': 324,\n",
              " 'worse': 325,\n",
              " 'than': 326,\n",
              " 'war?': 327,\n",
              " 'Three': 328,\n",
              " 'wars': 329,\n",
              " 'row': 330,\n",
              " 'hares': 331,\n",
              " 'hopping': 332,\n",
              " 'backward?': 333,\n",
              " 'receding': 334,\n",
              " 'line.': 335,\n",
              " \"I've\": 336,\n",
              " 'got': 337,\n",
              " 'dead': 338,\n",
              " 'budgie': 339,\n",
              " 'for': 340,\n",
              " 'sale,': 341,\n",
              " 'anyone': 342,\n",
              " 'interested?': 343,\n",
              " \"isn't\": 344,\n",
              " 'going': 345,\n",
              " 'cheep.': 346,\n",
              " 'politically': 347,\n",
              " 'correct': 348,\n",
              " 'name': 349,\n",
              " '\"African': 350,\n",
              " 'Americans': 351,\n",
              " 'Down': 352,\n",
              " 'Syndrome\"': 353,\n",
              " 'group?': 354,\n",
              " 'Black': 355,\n",
              " 'Lives': 356,\n",
              " 'Matter': 357,\n",
              " 'Edit1:': 358,\n",
              " 'No': 359,\n",
              " 'Im': 360,\n",
              " 'not': 361,\n",
              " 'targeting': 362,\n",
              " 'people,': 363,\n",
              " 'people': 364,\n",
              " 'actual': 365,\n",
              " 'syndrome,': 366,\n",
              " 'group': 367,\n",
              " 'scarecrow': 368,\n",
              " 'promotion?': 369,\n",
              " 'outstanding': 370,\n",
              " 'field': 371,\n",
              " 'blondes': 372,\n",
              " 'take': 373,\n",
              " 'screw': 374,\n",
              " 'lightbulb?': 375,\n",
              " '5': 376,\n",
              " '1': 377,\n",
              " 'hold': 378,\n",
              " 'bulb': 379,\n",
              " '4': 380,\n",
              " 'spin': 381,\n",
              " 'ladder.': 382,\n",
              " 'NYC': 383,\n",
              " 'nanosecond?': 384,\n",
              " 'If': 385,\n",
              " 'stuck': 386,\n",
              " 'light,': 387,\n",
              " 'its': 388,\n",
              " 'time': 389,\n",
              " 'takes': 390,\n",
              " 'occupant': 391,\n",
              " 'car': 392,\n",
              " 'behind': 393,\n",
              " 'honk': 394,\n",
              " 'horn': 395,\n",
              " 'light': 396,\n",
              " 'turns': 397,\n",
              " 'green.': 398,\n",
              " 'Where': 399,\n",
              " 'Tumblr': 400,\n",
              " 'users': 401,\n",
              " 'pray?': 402,\n",
              " 'Cis-Teen': 403,\n",
              " 'Chapel': 404,\n",
              " 'left': 405,\n",
              " 'leg': 406,\n",
              " 'right': 407,\n",
              " 'leg?': 408,\n",
              " 'That': 409,\n",
              " 'one': 410,\n",
              " 'middle': 411,\n",
              " 'thinks': 412,\n",
              " \"he's\": 413,\n",
              " 'hard.': 414,\n",
              " 'below': 415,\n",
              " 'sha-na-na-na-na-na-na-na-na': 416,\n",
              " 'knees?': 417,\n",
              " 'Your': 418,\n",
              " 'Guns': 419,\n",
              " \"N'\": 420,\n",
              " 'Toeses': 421,\n",
              " \"I'll\": 422,\n",
              " 'see': 423,\n",
              " 'myself': 424,\n",
              " 'out.': 425,\n",
              " 'Humans': 426,\n",
              " 'Reproduce?': 427,\n",
              " 'Sexually': 428,\n",
              " 'B:': 429,\n",
              " 'C:': 430,\n",
              " 'Nun': 431,\n",
              " 'save': 432,\n",
              " 'herself': 433,\n",
              " 'from': 434,\n",
              " 'being': 435,\n",
              " 'poisoned?': 436,\n",
              " 'chucks.': 437,\n",
              " 'Michael': 438,\n",
              " 'J.': 439,\n",
              " 'Fox': 440,\n",
              " 'milkshakes?': 441,\n",
              " 'uses': 442,\n",
              " 'finest': 443,\n",
              " 'ingredients.': 444,\n",
              " 'Donald': 445,\n",
              " 'Trump': 446,\n",
              " 'orange?': 447,\n",
              " 'Orange': 448,\n",
              " 'thick': 449,\n",
              " 'skin': 450,\n",
              " 'actually': 451,\n",
              " 'it.': 452,\n",
              " 'job': 453,\n",
              " 'Model': 454,\n",
              " 'T': 455,\n",
              " 'factory?': 456,\n",
              " 'heard': 457,\n",
              " 'great': 458,\n",
              " 'line': 459,\n",
              " 'work.': 460,\n",
              " 'life': 461,\n",
              " 'as': 462,\n",
              " 'virgin?': 463,\n",
              " 'Its': 464,\n",
              " 'hard': 465,\n",
              " 'Caitlyn': 466,\n",
              " 'Jenner': 467,\n",
              " 'lie': 468,\n",
              " 'her': 469,\n",
              " 'kids?': 470,\n",
              " \"She's\": 471,\n",
              " 'transparent.': 472,\n",
              " 'loses': 473,\n",
              " 'tail?': 474,\n",
              " 'Goes': 475,\n",
              " 'store': 476,\n",
              " 'find': 477,\n",
              " 'another': 478,\n",
              " 'one.': 479,\n",
              " 'Mexicans': 480,\n",
              " 'drivers': 481,\n",
              " 'Ed': 482,\n",
              " 'sex': 483,\n",
              " 'same': 484,\n",
              " 'day?': 485,\n",
              " 'give': 486,\n",
              " 'donkey': 487,\n",
              " 'break.': 488,\n",
              " 'Is': 489,\n",
              " 'this': 490,\n",
              " 'InkJet': 491,\n",
              " 'any': 492,\n",
              " 'good?': 493,\n",
              " 'Sure,': 494,\n",
              " \"we've\": 495,\n",
              " 'sold': 496,\n",
              " 'royalty': 497,\n",
              " 'Princesses?': 498,\n",
              " 'Mate,': 499,\n",
              " 'prints': 500,\n",
              " 'ALL': 501,\n",
              " 'letters!': 502,\n",
              " 'Router': 503,\n",
              " 'released': 504,\n",
              " 'early': 505,\n",
              " 'prison?': 506,\n",
              " 'had': 507,\n",
              " 'connections.': 508,\n",
              " 'Whats': 509,\n",
              " 'bodybuilder': 510,\n",
              " 'whose': 511,\n",
              " 'fan': 512,\n",
              " 'X-Men?': 513,\n",
              " 'Huge': 514,\n",
              " 'Jackman': 515,\n",
              " 'failed': 516,\n",
              " 'erection?': 517,\n",
              " 'feelings.': 518,\n",
              " 'Have': 519,\n",
              " 'corduroy': 520,\n",
              " 'pillows?': 521,\n",
              " 'making': 522,\n",
              " 'headlines': 523,\n",
              " 'everywhere.': 524,\n",
              " 'Guess': 525,\n",
              " 'how': 526,\n",
              " 'girlfriends': 527,\n",
              " 'now?': 528,\n",
              " 'Well,': 529,\n",
              " 'even': 530,\n",
              " 'count,': 531,\n",
              " 'range': 532,\n",
              " 'many.': 533,\n",
              " '<1.': 534,\n",
              " 'opposite': 535,\n",
              " 'Christopher': 536,\n",
              " 'Reeve?': 537,\n",
              " 'Walken': 538,\n",
              " 'pencil': 539,\n",
              " 'marijuana?': 540,\n",
              " 'look': 541,\n",
              " 'smart,': 542,\n",
              " 'blunt.': 543,\n",
              " 'explain': 544,\n",
              " 'puns': 545,\n",
              " 'kleptomaniacs?': 546,\n",
              " 'things': 547,\n",
              " 'literally.': 548,\n",
              " 'cancer': 549,\n",
              " 'Brazilian?': 550,\n",
              " 'Cancer': 551,\n",
              " 'evolves,': 552,\n",
              " 'Brazilian': 553,\n",
              " \"doesn't.\": 554,\n",
              " 'OP': 555,\n",
              " 'eggs?': 556,\n",
              " 'Eggs': 557,\n",
              " 'laid.': 558,\n",
              " 'mix': 559,\n",
              " 'insurance': 560,\n",
              " 'company': 561,\n",
              " 'NFL': 562,\n",
              " 'quarterback?': 563,\n",
              " 'An': 564,\n",
              " 'Aflacco': 565,\n",
              " 'why': 566,\n",
              " 'Amish': 567,\n",
              " 'girl': 568,\n",
              " 'excommunicated?': 569,\n",
              " 'Too': 570,\n",
              " 'Mennonite': 571,\n",
              " 'someone': 572,\n",
              " 'unemployed': 573,\n",
              " 'gender-expert?': 574,\n",
              " 'Nothing': 575,\n",
              " 'Norwegian': 576,\n",
              " 'Beef': 577,\n",
              " 'Stew?': 578,\n",
              " 'Stew': 579,\n",
              " 'all': 580,\n",
              " 'pirates': 581,\n",
              " 'buried': 582,\n",
              " 'treasure?': 583,\n",
              " 'Thanks': 584,\n",
              " 'gold!': 585,\n",
              " 'dad': 586,\n",
              " 'Carly': 587,\n",
              " 'Rae': 588,\n",
              " 'Jepsen': 589,\n",
              " 'common?': 590,\n",
              " 'said': 591,\n",
              " 'WWI': 592,\n",
              " 'vet': 593,\n",
              " 'angry': 594,\n",
              " 'German': 595,\n",
              " 'veteran?': 596,\n",
              " \"Can't\": 597,\n",
              " 'we': 598,\n",
              " 'let': 599,\n",
              " \"Argonne's\": 600,\n",
              " \"Argonne's?\": 601,\n",
              " 'cops': 602,\n",
              " 'change': 603,\n",
              " 'bulb?': 604,\n",
              " \"don't.\": 605,\n",
              " 'room': 606,\n",
              " 'black.': 607,\n",
              " 'near-sighted': 608,\n",
              " 'gynecologist': 609,\n",
              " 'puppy': 610,\n",
              " 'wet': 611,\n",
              " 'nose': 612,\n",
              " 'two': 613,\n",
              " 'bananas?': 614,\n",
              " 'Slippers': 615,\n",
              " 'nice': 616,\n",
              " 'girls': 617,\n",
              " 'assholes?': 618,\n",
              " 'pegging.': 619,\n",
              " 'farmer': 620,\n",
              " 'tractor?': 621,\n",
              " \"Where's\": 622,\n",
              " 'Whatâ€™s': 623,\n",
              " 'dicks?': 624,\n",
              " 'Redditors': 625,\n",
              " 'canâ€™t': 626,\n",
              " 'joke.': 627,\n",
              " 'part': 628,\n",
              " 'eating': 629,\n",
              " 'vegetables?': 630,\n",
              " 'Finding': 631,\n",
              " 'place': 632,\n",
              " 'wheelchairs.': 633,\n",
              " 'should': 634,\n",
              " 'talk': 635,\n",
              " 'back?': 636,\n",
              " 'Fart': 637,\n",
              " \"Desiigner's\": 638,\n",
              " 'favorite': 639,\n",
              " 'car?': 640,\n",
              " 'Kia': 641,\n",
              " 'cheaper,': 642,\n",
              " 'Beer': 643,\n",
              " 'Nuts': 644,\n",
              " 'Deer': 645,\n",
              " 'Nuts?': 646,\n",
              " 'Nuts.': 647,\n",
              " '$1.49,': 648,\n",
              " 'while': 649,\n",
              " 'nuts': 650,\n",
              " 'under': 651,\n",
              " 'Buck!': 652,\n",
              " 'bird': 653,\n",
              " 'peace': 654,\n",
              " 'dove,': 655,\n",
              " \"what's\": 656,\n",
              " 'love?': 657,\n",
              " 'swallow.': 658,\n",
              " 'Hippo': 659,\n",
              " 'Zippo?': 660,\n",
              " 'heavy': 661,\n",
              " 'Zippo': 662,\n",
              " 'lighter.': 663,\n",
              " 'every': 664,\n",
              " 'infomercial': 665,\n",
              " 'Kuwait?': 666,\n",
              " 'But': 667,\n",
              " 'Kuwait,': 668,\n",
              " \"there's\": 669,\n",
              " 'more!': 670,\n",
              " 'shin': 671,\n",
              " '?': 672,\n",
              " 'Tony': 673,\n",
              " 'Fonzie': 674,\n",
              " 'stop': 675,\n",
              " 'sleeping': 676,\n",
              " 'around?': 677,\n",
              " 'AIIIIIDS.': 678,\n",
              " 'embarrasses': 679,\n",
              " 'archaeologist?': 680,\n",
              " 'Give': 681,\n",
              " 'him': 682,\n",
              " 'tampon': 683,\n",
              " 'ask': 684,\n",
              " 'period': 685,\n",
              " 'from.': 686,\n",
              " \"Why'd\": 687,\n",
              " 'vulture': 688,\n",
              " 'check': 689,\n",
              " 'bag?': 690,\n",
              " 'airline': 691,\n",
              " \"didn't\": 692,\n",
              " 'allow': 693,\n",
              " 'carrion': 694,\n",
              " 'luggage.': 695,\n",
              " 'drug': 696,\n",
              " 'addicts': 697,\n",
              " 'hang': 698,\n",
              " 'out': 699,\n",
              " 'beach?': 700,\n",
              " 'getting': 701,\n",
              " 'sand': 702,\n",
              " 'their': 703,\n",
              " 'crack.': 704,\n",
              " 'seeing': 705,\n",
              " 'Rouge': 706,\n",
              " 'week?': 707,\n",
              " 'prequel': 708,\n",
              " 'Maroon': 709,\n",
              " '5.': 710,\n",
              " 'high': 711,\n",
              " 'drunk': 712,\n",
              " 'driver?': 713,\n",
              " 'driver': 714,\n",
              " 'goes': 715,\n",
              " 'through': 716,\n",
              " 'sign,': 717,\n",
              " 'waits': 718,\n",
              " \"Wife:Isn't\": 719,\n",
              " 'hot': 720,\n",
              " 'here': 721,\n",
              " 'hun?': 722,\n",
              " 'Troll': 723,\n",
              " 'husband': 724,\n",
              " ':': 725,\n",
              " 'kinda': 726,\n",
              " 'is,': 727,\n",
              " 'ill': 728,\n",
              " 'adjust': 729,\n",
              " 'AC.': 730,\n",
              " 'Knock': 731,\n",
              " 'knock.': 732,\n",
              " \"Who's\": 733,\n",
              " 'there?': 734,\n",
              " 'Dave.': 735,\n",
              " 'Dave': 736,\n",
              " 'who?': 737,\n",
              " 'promptly': 738,\n",
              " 'burst': 739,\n",
              " 'into': 740,\n",
              " 'tears': 741,\n",
              " 'everyone': 742,\n",
              " 'world': 743,\n",
              " 'knew': 744,\n",
              " 'Mohammad': 745,\n",
              " 'Holy': 746,\n",
              " 'Land?': 747,\n",
              " 'Makkah-roni': 748,\n",
              " 'cheese!': 749,\n",
              " 'Boy': 750,\n",
              " 'Girl-': 751,\n",
              " 'Hi': 752,\n",
              " 'Sweetheart,': 753,\n",
              " 'day': 754,\n",
              " 'going?': 755,\n",
              " 'G-': 756,\n",
              " 'Pretty': 757,\n",
              " 'well,': 758,\n",
              " 'want': 759,\n",
              " 'walk': 760,\n",
              " 'B-': 761,\n",
              " 'friendzone': 762,\n",
              " 'tried': 763,\n",
              " 'escape.': 764,\n",
              " 'last': 765,\n",
              " 'minds': 766,\n",
              " '9/11': 767,\n",
              " 'jumpers?': 768,\n",
              " 'Their': 769,\n",
              " 'ankles': 770,\n",
              " 'Korean': 771,\n",
              " 'food?': 772,\n",
              " 'Seoul': 773,\n",
              " 'overweight': 774,\n",
              " 'bounty': 775,\n",
              " 'hunter?': 776,\n",
              " 'Boba': 777,\n",
              " 'Fat': 778,\n",
              " 'beaver': 779,\n",
              " 'fell': 780,\n",
              " 'water?': 781,\n",
              " 'Damn': 782,\n",
              " 'Mexico': 783,\n",
              " 'willing': 784,\n",
              " 'pay': 785,\n",
              " 'build': 786,\n",
              " 'wall': 787,\n",
              " 'US': 788,\n",
              " 'Mexico?': 789,\n",
              " 'finally': 790,\n",
              " 'Olympic': 791,\n",
              " 'team.': 792,\n",
              " 'float?': 793,\n",
              " 'Easy!': 794,\n",
              " 'Just': 795,\n",
              " 'add': 796,\n",
              " 'Root': 797,\n",
              " 'beer': 798,\n",
              " 'Ice': 799,\n",
              " 'Cream!': 800,\n",
              " 'Muslim': 801,\n",
              " 'house': 802,\n",
              " 'wives': 803,\n",
              " 'could': 804,\n",
              " 'KFC?': 805,\n",
              " 'harem,': 806,\n",
              " 'fowl.': 807,\n",
              " 'bit': 808,\n",
              " 'travel': 809,\n",
              " 'CPU': 810,\n",
              " 'HDD?': 811,\n",
              " 'took': 812,\n",
              " 'bus.': 813,\n",
              " 'streaker': 814,\n",
              " 'church?': 815,\n",
              " 'caught': 816,\n",
              " 'by': 817,\n",
              " 'organ': 818,\n",
              " 'called': 819,\n",
              " 'police': 820,\n",
              " 'work': 821,\n",
              " 'overtime?': 822,\n",
              " 'Copper': 823,\n",
              " 'Nitrate.': 824,\n",
              " 'Jewish': 825,\n",
              " 'men': 826,\n",
              " 'circumcised': 827,\n",
              " 'Cause': 828,\n",
              " 'women': 829,\n",
              " 'touch': 830,\n",
              " 'anything': 831,\n",
              " \"that's\": 832,\n",
              " '10%': 833,\n",
              " 'off.': 834,\n",
              " 'riot': 835,\n",
              " 'early?': 836,\n",
              " 'To': 837,\n",
              " 'crowd': 838,\n",
              " 'whistle-blower': 839,\n",
              " 'during': 840,\n",
              " 'Russian': 841,\n",
              " 'blizzard?': 842,\n",
              " 'Nothing,': 843,\n",
              " 'Snowden.': 844,\n",
              " 'buttock': 845,\n",
              " 'implants': 846,\n",
              " 'popular': 847,\n",
              " 'Australia?': 848,\n",
              " 'boob': 849,\n",
              " 'jobs': 850,\n",
              " 'cheaper': 851,\n",
              " 'under.': 852,\n",
              " 'would': 853,\n",
              " 'first': 854,\n",
              " 'thing': 855,\n",
              " 'D.': 856,\n",
              " 'write': 857,\n",
              " 'Death': 858,\n",
              " 'Note': 859,\n",
              " '(if': 860,\n",
              " 'exist)??': 861,\n",
              " '\"YUUUUUGEE\"': 862,\n",
              " 'horse': 863,\n",
              " 'other': 864,\n",
              " 'horse?': 865,\n",
              " 'Hay,I': 866,\n",
              " 'thought': 867,\n",
              " 'horses': 868,\n",
              " \"couldn't\": 869,\n",
              " 'speak!': 870,\n",
              " 'suicide': 871,\n",
              " 'bomber': 872,\n",
              " 'teaching': 873,\n",
              " 'class?': 874,\n",
              " 'Pay': 875,\n",
              " 'attention!': 876,\n",
              " \"I'm\": 877,\n",
              " 'only': 878,\n",
              " 'show': 879,\n",
              " 'once.': 880,\n",
              " 'chicken': 881,\n",
              " 'cat': 882,\n",
              " 'mashed': 883,\n",
              " 'potatoes?': 884,\n",
              " 'side.': 885,\n",
              " 'ass': 886,\n",
              " 'cheek': 887,\n",
              " 'other?': 888,\n",
              " 'We': 889,\n",
              " 'gotta': 890,\n",
              " 'shit': 891,\n",
              " 'together.': 892,\n",
              " 'clickbait': 893,\n",
              " 'articles': 894,\n",
              " 'answer': 895,\n",
              " 'may': 896,\n",
              " 'shock': 897,\n",
              " 'you.': 898,\n",
              " 'UK': 899,\n",
              " 'banning': 900,\n",
              " 'hummus?': 901,\n",
              " \"It's\": 902,\n",
              " 'chickpea': 903,\n",
              " 'android?': 904,\n",
              " 'Synthia': 905,\n",
              " 'Lmao': 906,\n",
              " 'Police': 907,\n",
              " 'Officer:': 908,\n",
              " '\"Can': 909,\n",
              " 'identify': 910,\n",
              " 'yourself,': 911,\n",
              " 'Sir\"?': 912,\n",
              " 'Driver': 913,\n",
              " 'pulls': 914,\n",
              " 'mirror': 915,\n",
              " 'says:': 916,\n",
              " '\"Yes,': 917,\n",
              " 'me\".': 918,\n",
              " '11': 919,\n",
              " '&': 920,\n",
              " '2?': 921,\n",
              " 'Cowboys': 922,\n",
              " 'Mexican': 923,\n",
              " 'drive?': 924,\n",
              " 'Quebrolet.': 925,\n",
              " 'brothel': 926,\n",
              " 'riddled': 927,\n",
              " 'rabies?': 928,\n",
              " 'frothel': 929,\n",
              " 'mature': 930,\n",
              " 'immature': 931,\n",
              " 'sense': 932,\n",
              " 'humor?': 933,\n",
              " 'poop.': 934,\n",
              " 'Usain': 935,\n",
              " 'Bolt': 936,\n",
              " 'standing': 937,\n",
              " 'next': 938,\n",
              " 'mom?': 939,\n",
              " 'runner': 940,\n",
              " 'scoring': 941,\n",
              " 'position.': 942,\n",
              " 'happens': 943,\n",
              " 'pirate': 944,\n",
              " '60?': 945,\n",
              " 'joins': 946,\n",
              " 'AARP': 947,\n",
              " 'live': 948,\n",
              " 'matters': 949,\n",
              " 'protester': 950,\n",
              " 'knows': 951,\n",
              " 'too': 952,\n",
              " 'dark': 953,\n",
              " 'ten': 954,\n",
              " 'night?': 955,\n",
              " 'found': 956,\n",
              " 'teepee': 957,\n",
              " 'bear': 958,\n",
              " 'teeth?': 959,\n",
              " 'Bare': 960,\n",
              " 'Grylls': 961,\n",
              " 'Tribe': 962,\n",
              " 'Called': 963,\n",
              " 'Quest': 964,\n",
              " 'margarine': 965,\n",
              " 'butter,': 966,\n",
              " 'baby.': 967,\n",
              " 'cross': 968,\n",
              " 'road?': 969,\n",
              " 'house...': 970,\n",
              " 'Knock,': 971,\n",
              " 'chicken.': 972,\n",
              " 'painter': 973,\n",
              " 'finishing': 974,\n",
              " 'portrait': 975,\n",
              " 'brother': 976,\n",
              " 'Andrew?': 977,\n",
              " 'drew': 978,\n",
              " 'snowman': 979,\n",
              " 'cons': 980,\n",
              " 'people?': 981,\n",
              " 'snowfake': 982,\n",
              " 'appropriate': 983,\n",
              " 'year.': 984,\n",
              " 'Merry': 985,\n",
              " 'Christmas': 986,\n",
              " 'reddit!': 987,\n",
              " 'permission': 988,\n",
              " 'Xmas': 989,\n",
              " 'parties': 990,\n",
              " 'attend': 991,\n",
              " 'snail': 992,\n",
              " 'sail': 993,\n",
              " 'boat?': 994,\n",
              " 'snailor': 995,\n",
              " 'Tom': 996,\n",
              " 'hanks': 997,\n",
              " 'woods?': 998,\n",
              " 'forrest': 999,\n",
              " 'dump.': 1000,\n",
              " 'herpes': 1001,\n",
              " 'pun?': 1002,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmxwivOhcZa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e22e8f2b-49c8-4a74-aca0-035203242f78"
      },
      "source": [
        "voc.index2word"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'PAD',\n",
              " 1: 'SOS',\n",
              " 2: 'EOS',\n",
              " 3: 'Did',\n",
              " 4: 'you',\n",
              " 5: 'hear',\n",
              " 6: 'about',\n",
              " 7: 'the',\n",
              " 8: 'Native',\n",
              " 9: 'American',\n",
              " 10: 'man',\n",
              " 11: 'that',\n",
              " 12: 'drank',\n",
              " 13: '200',\n",
              " 14: 'cups',\n",
              " 15: 'of',\n",
              " 16: 'tea?',\n",
              " 17: 'He',\n",
              " 18: 'nearly',\n",
              " 19: 'drown',\n",
              " 20: 'in',\n",
              " 21: 'his',\n",
              " 22: 'own',\n",
              " 23: 'tea',\n",
              " 24: 'pee.',\n",
              " 25: \"What's\",\n",
              " 26: 'best',\n",
              " 27: 'anti',\n",
              " 28: 'diarrheal',\n",
              " 29: 'prescription?',\n",
              " 30: 'Mycheexarphlexin',\n",
              " 31: 'What',\n",
              " 32: 'do',\n",
              " 33: 'call',\n",
              " 34: 'a',\n",
              " 35: 'person',\n",
              " 36: 'who',\n",
              " 37: 'is',\n",
              " 38: 'outside',\n",
              " 39: 'door',\n",
              " 40: 'and',\n",
              " 41: 'has',\n",
              " 42: 'no',\n",
              " 43: 'arms',\n",
              " 44: 'nor',\n",
              " 45: 'legs?',\n",
              " 46: 'Matt',\n",
              " 47: 'Which',\n",
              " 48: 'Star',\n",
              " 49: 'Trek',\n",
              " 50: 'character',\n",
              " 51: 'member',\n",
              " 52: 'magic',\n",
              " 53: 'circle?',\n",
              " 54: 'Jean-Luc',\n",
              " 55: 'Pickacard',\n",
              " 56: 'difference',\n",
              " 57: 'between',\n",
              " 58: 'bullet',\n",
              " 59: 'human?',\n",
              " 60: 'A',\n",
              " 61: \"doesn't\",\n",
              " 62: 'miss',\n",
              " 63: 'Harambe',\n",
              " 64: 'Why',\n",
              " 65: 'was',\n",
              " 66: 'Ethiopian',\n",
              " 67: 'baby',\n",
              " 68: 'crying?',\n",
              " 69: 'having',\n",
              " 70: 'mid-life',\n",
              " 71: 'crisis',\n",
              " 72: 'corn',\n",
              " 73: 'husker',\n",
              " 74: 'with',\n",
              " 75: 'epilepsy',\n",
              " 76: 'hooker',\n",
              " 77: 'dysentery?',\n",
              " 78: 'One',\n",
              " 79: 'shucks',\n",
              " 80: 'fits...',\n",
              " 81: 'Who',\n",
              " 82: \"2016's\",\n",
              " 83: 'biggest',\n",
              " 84: 'sellout?',\n",
              " 85: 'Kevin',\n",
              " 86: 'Durant',\n",
              " 87: 'or',\n",
              " 88: 'Bernie',\n",
              " 89: 'Sanders?',\n",
              " 90: 'little',\n",
              " 91: \"Annie's\",\n",
              " 92: 'shoe',\n",
              " 93: 'floating',\n",
              " 94: 'sea?',\n",
              " 95: 'Because',\n",
              " 96: 'shark',\n",
              " 97: 'burped.',\n",
              " 98: '',\n",
              " 99: 'married',\n",
              " 100: 'bachelor?',\n",
              " 101: 'bachelor',\n",
              " 102: 'will',\n",
              " 103: 'go',\n",
              " 104: 'to',\n",
              " 105: 'fridge,',\n",
              " 106: 'sees',\n",
              " 107: 'nothing',\n",
              " 108: 'he',\n",
              " 109: 'wants,',\n",
              " 110: 'bed',\n",
              " 111: 'bed,',\n",
              " 112: 'fridge!',\n",
              " 113: 'are',\n",
              " 114: 'there',\n",
              " 115: 'so',\n",
              " 116: 'many',\n",
              " 117: 'blood',\n",
              " 118: 'cells',\n",
              " 119: 'female',\n",
              " 120: 'prisons?',\n",
              " 121: 'sentences',\n",
              " 122: 'usually',\n",
              " 123: 'end',\n",
              " 124: 'periods.',\n",
              " 125: 'russian',\n",
              " 126: 'tree?',\n",
              " 127: 'Dimitree',\n",
              " 128: 'How',\n",
              " 129: 'it',\n",
              " 130: 'when',\n",
              " 131: 'an',\n",
              " 132: 'egg',\n",
              " 133: 'on',\n",
              " 134: 'point?',\n",
              " 135: 'Egg',\n",
              " 136: 'zactly!',\n",
              " 137: \"Where'd\",\n",
              " 138: 'dog',\n",
              " 139: 'lost',\n",
              " 140: 'tail',\n",
              " 141: 'get',\n",
              " 142: 'new',\n",
              " 143: 'one?',\n",
              " 144: 'retail',\n",
              " 145: 'store.',\n",
              " 146: 'epileptic',\n",
              " 147: 'oyster',\n",
              " 148: 'shucker',\n",
              " 149: 'diarrhea?',\n",
              " 150: 'fits',\n",
              " 151: 'went',\n",
              " 152: 'ball?',\n",
              " 153: 'pulled',\n",
              " 154: 'muscle',\n",
              " 155: 'frogs',\n",
              " 156: 'happy?',\n",
              " 157: 'They',\n",
              " 158: 'eat',\n",
              " 159: 'whatever',\n",
              " 160: 'bugs',\n",
              " 161: 'them.',\n",
              " 162: 'turn',\n",
              " 163: 'Indian',\n",
              " 164: 'woman',\n",
              " 165: 'on?',\n",
              " 166: 'Press',\n",
              " 167: 'red',\n",
              " 168: 'button.',\n",
              " 169: 'Shall',\n",
              " 170: 'I',\n",
              " 171: 'tell',\n",
              " 172: 'joke',\n",
              " 173: 'kidnappers?',\n",
              " 174: \"I'd\",\n",
              " 175: 'better',\n",
              " 176: 'not.',\n",
              " 177: 'You',\n",
              " 178: 'might',\n",
              " 179: 'carried',\n",
              " 180: 'away.',\n",
              " 181: 'Do',\n",
              " 182: 'like',\n",
              " 183: 'fish',\n",
              " 184: 'sticks?',\n",
              " 185: 'Well',\n",
              " 186: 'then,',\n",
              " 187: \"you're\",\n",
              " 188: 'gay',\n",
              " 189: 'fish.',\n",
              " 190: 'did',\n",
              " 191: '2',\n",
              " 192: 'rednecks',\n",
              " 193: 'say',\n",
              " 194: 'after',\n",
              " 195: 'breaking',\n",
              " 196: 'up?',\n",
              " 197: \"Let's\",\n",
              " 198: 'just',\n",
              " 199: 'be',\n",
              " 200: 'cousins.',\n",
              " 201: 'pizza',\n",
              " 202: 'Jew?',\n",
              " 203: 'The',\n",
              " 204: 'Pizza',\n",
              " 205: 'scream',\n",
              " 206: 'put',\n",
              " 207: 'oven.',\n",
              " 208: 'does',\n",
              " 209: 'Santa',\n",
              " 210: 'have',\n",
              " 211: 'three',\n",
              " 212: 'gardens?',\n",
              " 213: 'Q:',\n",
              " 214: 'A:',\n",
              " 215: 'So',\n",
              " 216: 'can',\n",
              " 217: '\"hoe,',\n",
              " 218: 'hoe,',\n",
              " 219: 'hoe.\"',\n",
              " 220: 'hipster',\n",
              " 221: 'burn',\n",
              " 222: 'tongue?',\n",
              " 223: 'coffee',\n",
              " 224: 'before',\n",
              " 225: 'cool.',\n",
              " 226: 'Mozart',\n",
              " 227: 'kill',\n",
              " 228: 'chickens?',\n",
              " 229: 'were',\n",
              " 230: 'yelling',\n",
              " 231: '\"Bach',\n",
              " 232: 'Bach',\n",
              " 233: 'Bach\"',\n",
              " 234: \"pirate's\",\n",
              " 235: 'worst',\n",
              " 236: 'nightmare?',\n",
              " 237: 'sunken',\n",
              " 238: 'chest',\n",
              " 239: 'booty.',\n",
              " 240: \"can't\",\n",
              " 241: 'e-mail',\n",
              " 242: 'photo',\n",
              " 243: 'Jedi?',\n",
              " 244: 'attachments',\n",
              " 245: 'forbidden.',\n",
              " 246: 'happen',\n",
              " 247: 'if',\n",
              " 248: 'inside',\n",
              " 249: 'black',\n",
              " 250: 'hole?',\n",
              " 251: \"don't\",\n",
              " 252: 'know',\n",
              " 253: 'either.',\n",
              " 254: 'It',\n",
              " 255: 'must',\n",
              " 256: 'out-of-this-world.',\n",
              " 257: 'came',\n",
              " 258: 'first,',\n",
              " 259: 'Chicken',\n",
              " 260: 'Egg?',\n",
              " 261: 'Rooster',\n",
              " 262: 'common',\n",
              " 263: 'good',\n",
              " 264: 'boyfriend',\n",
              " 265: 'lion?',\n",
              " 266: \"They're\",\n",
              " 267: 'both',\n",
              " 268: 'ready',\n",
              " 269: 'semen',\n",
              " 270: 'basement?',\n",
              " 271: 'know.',\n",
              " 272: \"That's\",\n",
              " 273: 'way',\n",
              " 274: \"it's\",\n",
              " 275: 'spelled.',\n",
              " 276: 'your',\n",
              " 277: 'wife',\n",
              " 278: 'starts',\n",
              " 279: 'smoking?',\n",
              " 280: 'Slow',\n",
              " 281: 'down',\n",
              " 282: 'apply',\n",
              " 283: 'lube',\n",
              " 284: 'Want',\n",
              " 285: 'UDP?',\n",
              " 286: 'Never',\n",
              " 287: 'mind.',\n",
              " 288: \"won't\",\n",
              " 289: 'it,',\n",
              " 290: 'care',\n",
              " 291: '[OC]',\n",
              " 292: 'Person',\n",
              " 293: '1:',\n",
              " 294: 'parents',\n",
              " 295: 'do?',\n",
              " 296: \"2:They're\",\n",
              " 297: 'doctors,',\n",
              " 298: 'what',\n",
              " 299: 'beat',\n",
              " 300: 'me',\n",
              " 301: 'pool',\n",
              " 302: 'sticks.',\n",
              " 303: '2:',\n",
              " 304: 'Oh,',\n",
              " 305: 'they',\n",
              " 306: 'really',\n",
              " 307: 'at',\n",
              " 308: 'billiards.',\n",
              " 309: 'species',\n",
              " 310: 'rappers?',\n",
              " 311: 'Dragons,',\n",
              " 312: 'because',\n",
              " 313: \"they're\",\n",
              " 314: 'always',\n",
              " 315: \"spittin'\",\n",
              " 316: 'fire.',\n",
              " 317: 'elves',\n",
              " 318: 'make',\n",
              " 319: 'toys?',\n",
              " 320: 'Leave',\n",
              " 321: 'my',\n",
              " 322: 'presents',\n",
              " 323: 'statistically',\n",
              " 324: 'times',\n",
              " 325: 'worse',\n",
              " 326: 'than',\n",
              " 327: 'war?',\n",
              " 328: 'Three',\n",
              " 329: 'wars',\n",
              " 330: 'row',\n",
              " 331: 'hares',\n",
              " 332: 'hopping',\n",
              " 333: 'backward?',\n",
              " 334: 'receding',\n",
              " 335: 'line.',\n",
              " 336: \"I've\",\n",
              " 337: 'got',\n",
              " 338: 'dead',\n",
              " 339: 'budgie',\n",
              " 340: 'for',\n",
              " 341: 'sale,',\n",
              " 342: 'anyone',\n",
              " 343: 'interested?',\n",
              " 344: \"isn't\",\n",
              " 345: 'going',\n",
              " 346: 'cheep.',\n",
              " 347: 'politically',\n",
              " 348: 'correct',\n",
              " 349: 'name',\n",
              " 350: '\"African',\n",
              " 351: 'Americans',\n",
              " 352: 'Down',\n",
              " 353: 'Syndrome\"',\n",
              " 354: 'group?',\n",
              " 355: 'Black',\n",
              " 356: 'Lives',\n",
              " 357: 'Matter',\n",
              " 358: 'Edit1:',\n",
              " 359: 'No',\n",
              " 360: 'Im',\n",
              " 361: 'not',\n",
              " 362: 'targeting',\n",
              " 363: 'people,',\n",
              " 364: 'people',\n",
              " 365: 'actual',\n",
              " 366: 'syndrome,',\n",
              " 367: 'group',\n",
              " 368: 'scarecrow',\n",
              " 369: 'promotion?',\n",
              " 370: 'outstanding',\n",
              " 371: 'field',\n",
              " 372: 'blondes',\n",
              " 373: 'take',\n",
              " 374: 'screw',\n",
              " 375: 'lightbulb?',\n",
              " 376: '5',\n",
              " 377: '1',\n",
              " 378: 'hold',\n",
              " 379: 'bulb',\n",
              " 380: '4',\n",
              " 381: 'spin',\n",
              " 382: 'ladder.',\n",
              " 383: 'NYC',\n",
              " 384: 'nanosecond?',\n",
              " 385: 'If',\n",
              " 386: 'stuck',\n",
              " 387: 'light,',\n",
              " 388: 'its',\n",
              " 389: 'time',\n",
              " 390: 'takes',\n",
              " 391: 'occupant',\n",
              " 392: 'car',\n",
              " 393: 'behind',\n",
              " 394: 'honk',\n",
              " 395: 'horn',\n",
              " 396: 'light',\n",
              " 397: 'turns',\n",
              " 398: 'green.',\n",
              " 399: 'Where',\n",
              " 400: 'Tumblr',\n",
              " 401: 'users',\n",
              " 402: 'pray?',\n",
              " 403: 'Cis-Teen',\n",
              " 404: 'Chapel',\n",
              " 405: 'left',\n",
              " 406: 'leg',\n",
              " 407: 'right',\n",
              " 408: 'leg?',\n",
              " 409: 'That',\n",
              " 410: 'one',\n",
              " 411: 'middle',\n",
              " 412: 'thinks',\n",
              " 413: \"he's\",\n",
              " 414: 'hard.',\n",
              " 415: 'below',\n",
              " 416: 'sha-na-na-na-na-na-na-na-na',\n",
              " 417: 'knees?',\n",
              " 418: 'Your',\n",
              " 419: 'Guns',\n",
              " 420: \"N'\",\n",
              " 421: 'Toeses',\n",
              " 422: \"I'll\",\n",
              " 423: 'see',\n",
              " 424: 'myself',\n",
              " 425: 'out.',\n",
              " 426: 'Humans',\n",
              " 427: 'Reproduce?',\n",
              " 428: 'Sexually',\n",
              " 429: 'B:',\n",
              " 430: 'C:',\n",
              " 431: 'Nun',\n",
              " 432: 'save',\n",
              " 433: 'herself',\n",
              " 434: 'from',\n",
              " 435: 'being',\n",
              " 436: 'poisoned?',\n",
              " 437: 'chucks.',\n",
              " 438: 'Michael',\n",
              " 439: 'J.',\n",
              " 440: 'Fox',\n",
              " 441: 'milkshakes?',\n",
              " 442: 'uses',\n",
              " 443: 'finest',\n",
              " 444: 'ingredients.',\n",
              " 445: 'Donald',\n",
              " 446: 'Trump',\n",
              " 447: 'orange?',\n",
              " 448: 'Orange',\n",
              " 449: 'thick',\n",
              " 450: 'skin',\n",
              " 451: 'actually',\n",
              " 452: 'it.',\n",
              " 453: 'job',\n",
              " 454: 'Model',\n",
              " 455: 'T',\n",
              " 456: 'factory?',\n",
              " 457: 'heard',\n",
              " 458: 'great',\n",
              " 459: 'line',\n",
              " 460: 'work.',\n",
              " 461: 'life',\n",
              " 462: 'as',\n",
              " 463: 'virgin?',\n",
              " 464: 'Its',\n",
              " 465: 'hard',\n",
              " 466: 'Caitlyn',\n",
              " 467: 'Jenner',\n",
              " 468: 'lie',\n",
              " 469: 'her',\n",
              " 470: 'kids?',\n",
              " 471: \"She's\",\n",
              " 472: 'transparent.',\n",
              " 473: 'loses',\n",
              " 474: 'tail?',\n",
              " 475: 'Goes',\n",
              " 476: 'store',\n",
              " 477: 'find',\n",
              " 478: 'another',\n",
              " 479: 'one.',\n",
              " 480: 'Mexicans',\n",
              " 481: 'drivers',\n",
              " 482: 'Ed',\n",
              " 483: 'sex',\n",
              " 484: 'same',\n",
              " 485: 'day?',\n",
              " 486: 'give',\n",
              " 487: 'donkey',\n",
              " 488: 'break.',\n",
              " 489: 'Is',\n",
              " 490: 'this',\n",
              " 491: 'InkJet',\n",
              " 492: 'any',\n",
              " 493: 'good?',\n",
              " 494: 'Sure,',\n",
              " 495: \"we've\",\n",
              " 496: 'sold',\n",
              " 497: 'royalty',\n",
              " 498: 'Princesses?',\n",
              " 499: 'Mate,',\n",
              " 500: 'prints',\n",
              " 501: 'ALL',\n",
              " 502: 'letters!',\n",
              " 503: 'Router',\n",
              " 504: 'released',\n",
              " 505: 'early',\n",
              " 506: 'prison?',\n",
              " 507: 'had',\n",
              " 508: 'connections.',\n",
              " 509: 'Whats',\n",
              " 510: 'bodybuilder',\n",
              " 511: 'whose',\n",
              " 512: 'fan',\n",
              " 513: 'X-Men?',\n",
              " 514: 'Huge',\n",
              " 515: 'Jackman',\n",
              " 516: 'failed',\n",
              " 517: 'erection?',\n",
              " 518: 'feelings.',\n",
              " 519: 'Have',\n",
              " 520: 'corduroy',\n",
              " 521: 'pillows?',\n",
              " 522: 'making',\n",
              " 523: 'headlines',\n",
              " 524: 'everywhere.',\n",
              " 525: 'Guess',\n",
              " 526: 'how',\n",
              " 527: 'girlfriends',\n",
              " 528: 'now?',\n",
              " 529: 'Well,',\n",
              " 530: 'even',\n",
              " 531: 'count,',\n",
              " 532: 'range',\n",
              " 533: 'many.',\n",
              " 534: '<1.',\n",
              " 535: 'opposite',\n",
              " 536: 'Christopher',\n",
              " 537: 'Reeve?',\n",
              " 538: 'Walken',\n",
              " 539: 'pencil',\n",
              " 540: 'marijuana?',\n",
              " 541: 'look',\n",
              " 542: 'smart,',\n",
              " 543: 'blunt.',\n",
              " 544: 'explain',\n",
              " 545: 'puns',\n",
              " 546: 'kleptomaniacs?',\n",
              " 547: 'things',\n",
              " 548: 'literally.',\n",
              " 549: 'cancer',\n",
              " 550: 'Brazilian?',\n",
              " 551: 'Cancer',\n",
              " 552: 'evolves,',\n",
              " 553: 'Brazilian',\n",
              " 554: \"doesn't.\",\n",
              " 555: 'OP',\n",
              " 556: 'eggs?',\n",
              " 557: 'Eggs',\n",
              " 558: 'laid.',\n",
              " 559: 'mix',\n",
              " 560: 'insurance',\n",
              " 561: 'company',\n",
              " 562: 'NFL',\n",
              " 563: 'quarterback?',\n",
              " 564: 'An',\n",
              " 565: 'Aflacco',\n",
              " 566: 'why',\n",
              " 567: 'Amish',\n",
              " 568: 'girl',\n",
              " 569: 'excommunicated?',\n",
              " 570: 'Too',\n",
              " 571: 'Mennonite',\n",
              " 572: 'someone',\n",
              " 573: 'unemployed',\n",
              " 574: 'gender-expert?',\n",
              " 575: 'Nothing',\n",
              " 576: 'Norwegian',\n",
              " 577: 'Beef',\n",
              " 578: 'Stew?',\n",
              " 579: 'Stew',\n",
              " 580: 'all',\n",
              " 581: 'pirates',\n",
              " 582: 'buried',\n",
              " 583: 'treasure?',\n",
              " 584: 'Thanks',\n",
              " 585: 'gold!',\n",
              " 586: 'dad',\n",
              " 587: 'Carly',\n",
              " 588: 'Rae',\n",
              " 589: 'Jepsen',\n",
              " 590: 'common?',\n",
              " 591: 'said',\n",
              " 592: 'WWI',\n",
              " 593: 'vet',\n",
              " 594: 'angry',\n",
              " 595: 'German',\n",
              " 596: 'veteran?',\n",
              " 597: \"Can't\",\n",
              " 598: 'we',\n",
              " 599: 'let',\n",
              " 600: \"Argonne's\",\n",
              " 601: \"Argonne's?\",\n",
              " 602: 'cops',\n",
              " 603: 'change',\n",
              " 604: 'bulb?',\n",
              " 605: \"don't.\",\n",
              " 606: 'room',\n",
              " 607: 'black.',\n",
              " 608: 'near-sighted',\n",
              " 609: 'gynecologist',\n",
              " 610: 'puppy',\n",
              " 611: 'wet',\n",
              " 612: 'nose',\n",
              " 613: 'two',\n",
              " 614: 'bananas?',\n",
              " 615: 'Slippers',\n",
              " 616: 'nice',\n",
              " 617: 'girls',\n",
              " 618: 'assholes?',\n",
              " 619: 'pegging.',\n",
              " 620: 'farmer',\n",
              " 621: 'tractor?',\n",
              " 622: \"Where's\",\n",
              " 623: 'Whatâ€™s',\n",
              " 624: 'dicks?',\n",
              " 625: 'Redditors',\n",
              " 626: 'canâ€™t',\n",
              " 627: 'joke.',\n",
              " 628: 'part',\n",
              " 629: 'eating',\n",
              " 630: 'vegetables?',\n",
              " 631: 'Finding',\n",
              " 632: 'place',\n",
              " 633: 'wheelchairs.',\n",
              " 634: 'should',\n",
              " 635: 'talk',\n",
              " 636: 'back?',\n",
              " 637: 'Fart',\n",
              " 638: \"Desiigner's\",\n",
              " 639: 'favorite',\n",
              " 640: 'car?',\n",
              " 641: 'Kia',\n",
              " 642: 'cheaper,',\n",
              " 643: 'Beer',\n",
              " 644: 'Nuts',\n",
              " 645: 'Deer',\n",
              " 646: 'Nuts?',\n",
              " 647: 'Nuts.',\n",
              " 648: '$1.49,',\n",
              " 649: 'while',\n",
              " 650: 'nuts',\n",
              " 651: 'under',\n",
              " 652: 'Buck!',\n",
              " 653: 'bird',\n",
              " 654: 'peace',\n",
              " 655: 'dove,',\n",
              " 656: \"what's\",\n",
              " 657: 'love?',\n",
              " 658: 'swallow.',\n",
              " 659: 'Hippo',\n",
              " 660: 'Zippo?',\n",
              " 661: 'heavy',\n",
              " 662: 'Zippo',\n",
              " 663: 'lighter.',\n",
              " 664: 'every',\n",
              " 665: 'infomercial',\n",
              " 666: 'Kuwait?',\n",
              " 667: 'But',\n",
              " 668: 'Kuwait,',\n",
              " 669: \"there's\",\n",
              " 670: 'more!',\n",
              " 671: 'shin',\n",
              " 672: '?',\n",
              " 673: 'Tony',\n",
              " 674: 'Fonzie',\n",
              " 675: 'stop',\n",
              " 676: 'sleeping',\n",
              " 677: 'around?',\n",
              " 678: 'AIIIIIDS.',\n",
              " 679: 'embarrasses',\n",
              " 680: 'archaeologist?',\n",
              " 681: 'Give',\n",
              " 682: 'him',\n",
              " 683: 'tampon',\n",
              " 684: 'ask',\n",
              " 685: 'period',\n",
              " 686: 'from.',\n",
              " 687: \"Why'd\",\n",
              " 688: 'vulture',\n",
              " 689: 'check',\n",
              " 690: 'bag?',\n",
              " 691: 'airline',\n",
              " 692: \"didn't\",\n",
              " 693: 'allow',\n",
              " 694: 'carrion',\n",
              " 695: 'luggage.',\n",
              " 696: 'drug',\n",
              " 697: 'addicts',\n",
              " 698: 'hang',\n",
              " 699: 'out',\n",
              " 700: 'beach?',\n",
              " 701: 'getting',\n",
              " 702: 'sand',\n",
              " 703: 'their',\n",
              " 704: 'crack.',\n",
              " 705: 'seeing',\n",
              " 706: 'Rouge',\n",
              " 707: 'week?',\n",
              " 708: 'prequel',\n",
              " 709: 'Maroon',\n",
              " 710: '5.',\n",
              " 711: 'high',\n",
              " 712: 'drunk',\n",
              " 713: 'driver?',\n",
              " 714: 'driver',\n",
              " 715: 'goes',\n",
              " 716: 'through',\n",
              " 717: 'sign,',\n",
              " 718: 'waits',\n",
              " 719: \"Wife:Isn't\",\n",
              " 720: 'hot',\n",
              " 721: 'here',\n",
              " 722: 'hun?',\n",
              " 723: 'Troll',\n",
              " 724: 'husband',\n",
              " 725: ':',\n",
              " 726: 'kinda',\n",
              " 727: 'is,',\n",
              " 728: 'ill',\n",
              " 729: 'adjust',\n",
              " 730: 'AC.',\n",
              " 731: 'Knock',\n",
              " 732: 'knock.',\n",
              " 733: \"Who's\",\n",
              " 734: 'there?',\n",
              " 735: 'Dave.',\n",
              " 736: 'Dave',\n",
              " 737: 'who?',\n",
              " 738: 'promptly',\n",
              " 739: 'burst',\n",
              " 740: 'into',\n",
              " 741: 'tears',\n",
              " 742: 'everyone',\n",
              " 743: 'world',\n",
              " 744: 'knew',\n",
              " 745: 'Mohammad',\n",
              " 746: 'Holy',\n",
              " 747: 'Land?',\n",
              " 748: 'Makkah-roni',\n",
              " 749: 'cheese!',\n",
              " 750: 'Boy',\n",
              " 751: 'Girl-',\n",
              " 752: 'Hi',\n",
              " 753: 'Sweetheart,',\n",
              " 754: 'day',\n",
              " 755: 'going?',\n",
              " 756: 'G-',\n",
              " 757: 'Pretty',\n",
              " 758: 'well,',\n",
              " 759: 'want',\n",
              " 760: 'walk',\n",
              " 761: 'B-',\n",
              " 762: 'friendzone',\n",
              " 763: 'tried',\n",
              " 764: 'escape.',\n",
              " 765: 'last',\n",
              " 766: 'minds',\n",
              " 767: '9/11',\n",
              " 768: 'jumpers?',\n",
              " 769: 'Their',\n",
              " 770: 'ankles',\n",
              " 771: 'Korean',\n",
              " 772: 'food?',\n",
              " 773: 'Seoul',\n",
              " 774: 'overweight',\n",
              " 775: 'bounty',\n",
              " 776: 'hunter?',\n",
              " 777: 'Boba',\n",
              " 778: 'Fat',\n",
              " 779: 'beaver',\n",
              " 780: 'fell',\n",
              " 781: 'water?',\n",
              " 782: 'Damn',\n",
              " 783: 'Mexico',\n",
              " 784: 'willing',\n",
              " 785: 'pay',\n",
              " 786: 'build',\n",
              " 787: 'wall',\n",
              " 788: 'US',\n",
              " 789: 'Mexico?',\n",
              " 790: 'finally',\n",
              " 791: 'Olympic',\n",
              " 792: 'team.',\n",
              " 793: 'float?',\n",
              " 794: 'Easy!',\n",
              " 795: 'Just',\n",
              " 796: 'add',\n",
              " 797: 'Root',\n",
              " 798: 'beer',\n",
              " 799: 'Ice',\n",
              " 800: 'Cream!',\n",
              " 801: 'Muslim',\n",
              " 802: 'house',\n",
              " 803: 'wives',\n",
              " 804: 'could',\n",
              " 805: 'KFC?',\n",
              " 806: 'harem,',\n",
              " 807: 'fowl.',\n",
              " 808: 'bit',\n",
              " 809: 'travel',\n",
              " 810: 'CPU',\n",
              " 811: 'HDD?',\n",
              " 812: 'took',\n",
              " 813: 'bus.',\n",
              " 814: 'streaker',\n",
              " 815: 'church?',\n",
              " 816: 'caught',\n",
              " 817: 'by',\n",
              " 818: 'organ',\n",
              " 819: 'called',\n",
              " 820: 'police',\n",
              " 821: 'work',\n",
              " 822: 'overtime?',\n",
              " 823: 'Copper',\n",
              " 824: 'Nitrate.',\n",
              " 825: 'Jewish',\n",
              " 826: 'men',\n",
              " 827: 'circumcised',\n",
              " 828: 'Cause',\n",
              " 829: 'women',\n",
              " 830: 'touch',\n",
              " 831: 'anything',\n",
              " 832: \"that's\",\n",
              " 833: '10%',\n",
              " 834: 'off.',\n",
              " 835: 'riot',\n",
              " 836: 'early?',\n",
              " 837: 'To',\n",
              " 838: 'crowd',\n",
              " 839: 'whistle-blower',\n",
              " 840: 'during',\n",
              " 841: 'Russian',\n",
              " 842: 'blizzard?',\n",
              " 843: 'Nothing,',\n",
              " 844: 'Snowden.',\n",
              " 845: 'buttock',\n",
              " 846: 'implants',\n",
              " 847: 'popular',\n",
              " 848: 'Australia?',\n",
              " 849: 'boob',\n",
              " 850: 'jobs',\n",
              " 851: 'cheaper',\n",
              " 852: 'under.',\n",
              " 853: 'would',\n",
              " 854: 'first',\n",
              " 855: 'thing',\n",
              " 856: 'D.',\n",
              " 857: 'write',\n",
              " 858: 'Death',\n",
              " 859: 'Note',\n",
              " 860: '(if',\n",
              " 861: 'exist)??',\n",
              " 862: '\"YUUUUUGEE\"',\n",
              " 863: 'horse',\n",
              " 864: 'other',\n",
              " 865: 'horse?',\n",
              " 866: 'Hay,I',\n",
              " 867: 'thought',\n",
              " 868: 'horses',\n",
              " 869: \"couldn't\",\n",
              " 870: 'speak!',\n",
              " 871: 'suicide',\n",
              " 872: 'bomber',\n",
              " 873: 'teaching',\n",
              " 874: 'class?',\n",
              " 875: 'Pay',\n",
              " 876: 'attention!',\n",
              " 877: \"I'm\",\n",
              " 878: 'only',\n",
              " 879: 'show',\n",
              " 880: 'once.',\n",
              " 881: 'chicken',\n",
              " 882: 'cat',\n",
              " 883: 'mashed',\n",
              " 884: 'potatoes?',\n",
              " 885: 'side.',\n",
              " 886: 'ass',\n",
              " 887: 'cheek',\n",
              " 888: 'other?',\n",
              " 889: 'We',\n",
              " 890: 'gotta',\n",
              " 891: 'shit',\n",
              " 892: 'together.',\n",
              " 893: 'clickbait',\n",
              " 894: 'articles',\n",
              " 895: 'answer',\n",
              " 896: 'may',\n",
              " 897: 'shock',\n",
              " 898: 'you.',\n",
              " 899: 'UK',\n",
              " 900: 'banning',\n",
              " 901: 'hummus?',\n",
              " 902: \"It's\",\n",
              " 903: 'chickpea',\n",
              " 904: 'android?',\n",
              " 905: 'Synthia',\n",
              " 906: 'Lmao',\n",
              " 907: 'Police',\n",
              " 908: 'Officer:',\n",
              " 909: '\"Can',\n",
              " 910: 'identify',\n",
              " 911: 'yourself,',\n",
              " 912: 'Sir\"?',\n",
              " 913: 'Driver',\n",
              " 914: 'pulls',\n",
              " 915: 'mirror',\n",
              " 916: 'says:',\n",
              " 917: '\"Yes,',\n",
              " 918: 'me\".',\n",
              " 919: '11',\n",
              " 920: '&',\n",
              " 921: '2?',\n",
              " 922: 'Cowboys',\n",
              " 923: 'Mexican',\n",
              " 924: 'drive?',\n",
              " 925: 'Quebrolet.',\n",
              " 926: 'brothel',\n",
              " 927: 'riddled',\n",
              " 928: 'rabies?',\n",
              " 929: 'frothel',\n",
              " 930: 'mature',\n",
              " 931: 'immature',\n",
              " 932: 'sense',\n",
              " 933: 'humor?',\n",
              " 934: 'poop.',\n",
              " 935: 'Usain',\n",
              " 936: 'Bolt',\n",
              " 937: 'standing',\n",
              " 938: 'next',\n",
              " 939: 'mom?',\n",
              " 940: 'runner',\n",
              " 941: 'scoring',\n",
              " 942: 'position.',\n",
              " 943: 'happens',\n",
              " 944: 'pirate',\n",
              " 945: '60?',\n",
              " 946: 'joins',\n",
              " 947: 'AARP',\n",
              " 948: 'live',\n",
              " 949: 'matters',\n",
              " 950: 'protester',\n",
              " 951: 'knows',\n",
              " 952: 'too',\n",
              " 953: 'dark',\n",
              " 954: 'ten',\n",
              " 955: 'night?',\n",
              " 956: 'found',\n",
              " 957: 'teepee',\n",
              " 958: 'bear',\n",
              " 959: 'teeth?',\n",
              " 960: 'Bare',\n",
              " 961: 'Grylls',\n",
              " 962: 'Tribe',\n",
              " 963: 'Called',\n",
              " 964: 'Quest',\n",
              " 965: 'margarine',\n",
              " 966: 'butter,',\n",
              " 967: 'baby.',\n",
              " 968: 'cross',\n",
              " 969: 'road?',\n",
              " 970: 'house...',\n",
              " 971: 'Knock,',\n",
              " 972: 'chicken.',\n",
              " 973: 'painter',\n",
              " 974: 'finishing',\n",
              " 975: 'portrait',\n",
              " 976: 'brother',\n",
              " 977: 'Andrew?',\n",
              " 978: 'drew',\n",
              " 979: 'snowman',\n",
              " 980: 'cons',\n",
              " 981: 'people?',\n",
              " 982: 'snowfake',\n",
              " 983: 'appropriate',\n",
              " 984: 'year.',\n",
              " 985: 'Merry',\n",
              " 986: 'Christmas',\n",
              " 987: 'reddit!',\n",
              " 988: 'permission',\n",
              " 989: 'Xmas',\n",
              " 990: 'parties',\n",
              " 991: 'attend',\n",
              " 992: 'snail',\n",
              " 993: 'sail',\n",
              " 994: 'boat?',\n",
              " 995: 'snailor',\n",
              " 996: 'Tom',\n",
              " 997: 'hanks',\n",
              " 998: 'woods?',\n",
              " 999: 'forrest',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MdbGSVXcbvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexfromsentence(voc,sentence):\n",
        "  return [voc.word2index[word]for word in sentence.split(' ')]+[eos_tok]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Me3pVoAdOcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zeropadding(l,fillvalue=pad_tok):\n",
        "  return list(itertools.zip_longest(*l,fillvalue=fillvalue))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVYkZqo7dhs9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4426f4ba-4b80-4adc-e6e4-b8f014a46df1"
      },
      "source": [
        "indexfromsentence(voc,clean_pairs[1][0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 7, 26, 27, 28, 29, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4az-s5SfsgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4f20e39-9520-4cb9-c736-ee792393f7da"
      },
      "source": [
        "clean_pairs[1][0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What's the best anti diarrheal prescription?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhQqr8ECBE1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing purposes\n",
        "inp = []\n",
        "out = []\n",
        "for i in clean_pairs[:10]:\n",
        "  inp.append(i[0])\n",
        "  out.append(i[1])\n",
        "indexes = [indexfromsentence(voc,s)for s in inp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0g1nzwRCPxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "37cbd644-5ce9-4c10-c551-ebbe9dbf5fba"
      },
      "source": [
        "indexes"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 2],\n",
              " [25, 7, 26, 27, 28, 29, 2],\n",
              " [31, 32, 4, 33, 34, 35, 36, 37, 38, 34, 39, 40, 41, 42, 43, 44, 45, 2],\n",
              " [47, 48, 49, 50, 37, 34, 51, 15, 7, 52, 53, 2],\n",
              " [25, 7, 56, 57, 34, 58, 40, 34, 59, 2],\n",
              " [64, 65, 7, 66, 67, 68, 2],\n",
              " [25, 7, 56, 57, 34, 72, 73, 74, 75, 40, 34, 76, 74, 77, 2],\n",
              " [81, 37, 82, 83, 84, 2],\n",
              " [64, 37, 90, 91, 92, 93, 20, 7, 94, 2],\n",
              " [64, 113, 114, 115, 116, 117, 118, 20, 119, 120, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0MUtgt3CalD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing zero padding\n",
        "test_padd = zeropadding(indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weiF5TVqCrwO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "622b6463-92a4-4f5d-d322-2a734f941873"
      },
      "source": [
        "test_padd"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 25, 31, 47, 25, 64, 25, 81, 64, 64),\n",
              " (4, 7, 32, 48, 7, 65, 7, 37, 37, 113),\n",
              " (5, 26, 4, 49, 56, 7, 56, 82, 90, 114),\n",
              " (6, 27, 33, 50, 57, 66, 57, 83, 91, 115),\n",
              " (7, 28, 34, 37, 34, 67, 34, 84, 92, 116),\n",
              " (8, 29, 35, 34, 58, 68, 72, 2, 93, 117),\n",
              " (9, 2, 36, 51, 40, 2, 73, 0, 20, 118),\n",
              " (10, 0, 37, 15, 34, 0, 74, 0, 7, 20),\n",
              " (11, 0, 38, 7, 59, 0, 75, 0, 94, 119),\n",
              " (12, 0, 34, 52, 2, 0, 40, 0, 2, 120),\n",
              " (13, 0, 39, 53, 0, 0, 34, 0, 0, 2),\n",
              " (14, 0, 40, 2, 0, 0, 76, 0, 0, 0),\n",
              " (15, 0, 41, 0, 0, 0, 74, 0, 0, 0),\n",
              " (16, 0, 42, 0, 0, 0, 77, 0, 0, 0),\n",
              " (2, 0, 43, 0, 0, 0, 2, 0, 0, 0),\n",
              " (0, 0, 44, 0, 0, 0, 0, 0, 0, 0),\n",
              " (0, 0, 45, 0, 0, 0, 0, 0, 0, 0),\n",
              " (0, 0, 2, 0, 0, 0, 0, 0, 0, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q6ev0JYCzda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#works perfectly fine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLS2__uMgcsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binarymat(l,value=pad_tok):\n",
        "  m = []\n",
        "  for i, seq in enumerate(l):\n",
        "    m.append([])\n",
        "    for token in seq:\n",
        "      if token == pad_tok:\n",
        "        m[i].append(0)\n",
        "      else:\n",
        "        m[i].append(1)\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67bBC9jRkmsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing binary matrix\n",
        "test_bm = binarymat(test_padd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0WUxQ3eDDhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "da3c8cd9-cc89-431e-8d47-35dbf07a0f86"
      },
      "source": [
        "test_bm"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
              " [1, 0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
              " [1, 0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
              " [1, 0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
              " [1, 0, 1, 1, 0, 0, 1, 0, 0, 1],\n",
              " [1, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
              " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
              " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
              " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFMX1qOvHUxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvRtohPtDFKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nicely done\n",
        "def input_var(l,voc):\n",
        "  indexes_batch = [indexfromsentence(voc,s) for s in l]\n",
        "  lengths = torch.tensor([len(indexes)for indexes in indexes_batch])\n",
        "  padlist = zeropadding(indexes_batch)\n",
        "  padvar = torch.LongTensor(padlist)\n",
        "  return padvar,lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sAhh0XVH83b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def out_var(l,voc):\n",
        "  indexes_batch = [indexfromsentence(voc,s)for s in l]\n",
        "  max_target_len = max(len(indexes)for indexes in indexes_batch)\n",
        "  padlist = zeropadding(indexes_batch)\n",
        "  mask = binarymat(padlist)\n",
        "  mask = torch.BoolTensor(mask)\n",
        "  padvar = torch.LongTensor(padlist)\n",
        "  return padvar,mask,max_target_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYtRvk1LJCbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch2traindata(voc,pair_batch):\n",
        "  pair_batch.sort(key= lambda x:len(x[0].split(' ')),reverse=True)\n",
        "  input_batch , output_batch = [],[]\n",
        "  for pair in pair_batch:\n",
        "    input_batch.append(pair[0])\n",
        "    output_batch.append(pair[1])\n",
        "  inp,lengths = input_var(input_batch,voc)\n",
        "  out,mask,max_target_len = out_var(output_batch,voc)\n",
        "  return inp,lengths,out,mask,max_target_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1EziDGmKgQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "d95ad086-83e3-4d73-89f8-4881e5a39535"
      },
      "source": [
        "#testing\n",
        "small_batch = 5\n",
        "batches = batch2traindata(voc,[random.choice(clean_pairs)for _ in range(small_batch)])\n",
        "input_variable,lengths,target_variable,mask,max_target_len = batches\n",
        "print(\"input variable\",input_variable)\n",
        "print(\"lengths:\",lengths)\n",
        "print(\"target_variable:\",target_variable)\n",
        "print(\"mask:\",mask)\n",
        "print(\"max_target_len:\",max_target_len)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input variable tensor([[   31,    31,    25,    31,    64],\n",
            "        [   37,    32,     7,    32,   240],\n",
            "        [    7,     4,    56,     4,   445],\n",
            "        [   56,    33,    57,    33,   446],\n",
            "        [   57,   131,  8252,    34,   141],\n",
            "        [    7, 47775,    40,  2512,  4445],\n",
            "        [ 5947,    20,  4081,  6985,     2],\n",
            "        [   40,    34,     2,     2,     0],\n",
            "        [10289, 10588,     0,     0,     0],\n",
            "        [11858,     2,     0,     0,     0],\n",
            "        [    2,     0,     0,     0,     0]])\n",
            "lengths: tensor([11, 10,  8,  8,  7])\n",
            "target_variable: tensor([[   78,  2800,   170,    60,    95],\n",
            "        [   37,    98,   486,  6986,    42],\n",
            "        [   15,     2,    34,     2,  4863],\n",
            "        [ 1938,     0,   891,     0,   102],\n",
            "        [   41,     0,   130,     0,  6005],\n",
            "        [   34,     0,   170,     0,   340],\n",
            "        [  458,     0,  5471,     0,  2152],\n",
            "        [11859,     0,  3900,     0,     2],\n",
            "        [   98,     0,  1483,     0,     0],\n",
            "        [    2,     0, 11236,     0,     0],\n",
            "        [    0,     0,     2,     0,     0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True, False,  True, False,  True],\n",
            "        [ True, False,  True, False,  True],\n",
            "        [ True, False,  True, False,  True],\n",
            "        [ True, False,  True, False,  True],\n",
            "        [ True, False,  True, False,  True],\n",
            "        [ True, False,  True, False, False],\n",
            "        [ True, False,  True, False, False],\n",
            "        [False, False,  True, False, False]])\n",
            "max_target_len: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ThjL9-sqbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5CxCdWgLjM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nicely done\n",
        "#model defination\n",
        "#encoder class\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,hidden_size,embedding,n_layer=1,dropout=0):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.n_layer = n_layer\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = embedding\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,n_layer,dropout=(0 if n_layer==1 else dropout),bidirectional=True)\n",
        "\n",
        "  def forward(self,input_seq,input_length,hidden=None):\n",
        "    embedded = self.embedding(input_seq)\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(embedded,input_length)\n",
        "    output,hidden = self.gru(packed,hidden)\n",
        "    output, l= nn.utils.rnn.pad_packed_sequence(output)\n",
        "    output = output[:,:,:self.hidden_size]+output[:,:,self.hidden_size:]\n",
        "    return output,hidden\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChWib6yyuxAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEgpAj1syjQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating attention class\n",
        "class Attn(nn.Module):\n",
        "  def __init__(self,model,hidden_size):\n",
        "    super(Attn,self).__init__()\n",
        "    self.model = model\n",
        "    self.hidden_size = hidden_size\n",
        "  \n",
        "  def dot_score(self,hidden,encoder_output):\n",
        "    return torch.sum(hidden*encoder_output,dim=2)\n",
        "\n",
        "  def forward(self,hidden,encoder_output):\n",
        "    attn_energies = self.dot_score(hidden,encoder_output)\n",
        "    attn_energies = attn_energies.t()\n",
        "    return f.softmax(attn_energies,dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXz00fFVz4La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating decoder class\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,att_model,embedding,hidden_size,output_size,n_layer=1,dropout=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.attn_model = attn_model\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layer = n_layer\n",
        "    self.dropout = dropout\n",
        "    self.embedding = embedding\n",
        "    self.embedding_dropout = nn.Dropout(dropout)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,n_layer,dropout=(0 if n_layer==1 else dropout))\n",
        "    self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
        "    self.out = nn.Linear(hidden_size,output_size)\n",
        "    self.attn = Attn(attn_model,hidden_size)\n",
        "\n",
        "  def forward(self,input_step,last_hidden,encoder_output):\n",
        "    embedded = self.embedding(input_step)\n",
        "    embedded = self.embedding_dropout(embedded)\n",
        "    rnn_output, hidden = self.gru(embedded,last_hidden)\n",
        "    attn_weight = self.attn(rnn_output,encoder_output)\n",
        "    context = attn_weight.bmm(encoder_output.transpose(0,1))\n",
        "    rnn_output = rnn_output.squeeze(0)\n",
        "    context = context.squeeze(1)\n",
        "    concat_input = torch.cat((rnn_output,context),1)\n",
        "    concat_output = torch.tanh(self.concat(concat_input))\n",
        "    output = self.out(concat_output)\n",
        "    output = f.softmax(output,dim=1)\n",
        "    return output,hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22cycnZK-zbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskNLLLoss(inp,target,mask):\n",
        "  n_total = mask.sum()\n",
        "  cross_entropy = -torch.log(torch.gather(inp,1,target.view(-1,1)).squeeze(1))\n",
        "  loss = cross_entropy.masked_select(mask).mean()\n",
        "  loss = loss.to(device)\n",
        "  return loss,n_total.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLt6wIfqa9NH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing encoder and decoder\n",
        "test_embedding = nn.Embedding(voc.num_word,500)\n",
        "test_encoder = Encoder(500,test_embedding,2,0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuWAUKuydLv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dc66493f-fb9d-4cb3-a5f4-19f6d3fea183"
      },
      "source": [
        "test_encoder"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (embedding): Embedding(62373, 500)\n",
              "  (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXdhOzAsefHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_encoder_out,test_encoder_hidden=test_encoder(input_variable,lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9H5Z182ByHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "b89f59c5-7c92-405e-8225-0cb4e14aabac"
      },
      "source": [
        "test_encoder_out"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0665, -0.0979,  0.0338,  ..., -0.2165, -0.0419, -0.0713],\n",
              "         [ 0.1307, -0.2450,  0.0282,  ..., -0.0622,  0.2151,  0.1171],\n",
              "         [-0.3787,  0.0605, -0.0809,  ...,  0.0219,  0.1811, -0.3115],\n",
              "         [ 0.0325, -0.4034, -0.0112,  ..., -0.0561,  0.2516,  0.0356],\n",
              "         [-0.0166,  0.0660, -0.0789,  ...,  0.0292,  0.0074,  0.0576]],\n",
              "\n",
              "        [[ 0.0287, -0.1095,  0.0083,  ..., -0.3524, -0.0048,  0.0358],\n",
              "         [ 0.0821, -0.3598,  0.0951,  ..., -0.0454,  0.3360,  0.1663],\n",
              "         [-0.2259,  0.0341, -0.1368,  ..., -0.1481,  0.0800, -0.0598],\n",
              "         [ 0.0897, -0.4409,  0.1295,  ..., -0.0824,  0.3710,  0.1510],\n",
              "         [ 0.1006,  0.3296, -0.1065,  ..., -0.1741,  0.0194, -0.1155]],\n",
              "\n",
              "        [[-0.1146,  0.0019, -0.0198,  ..., -0.3835,  0.1207,  0.0169],\n",
              "         [-0.0918, -0.1875, -0.0967,  ..., -0.1836,  0.0683,  0.0014],\n",
              "         [-0.1615,  0.0459, -0.2428,  ..., -0.1115, -0.0848,  0.2838],\n",
              "         [-0.0728, -0.1595, -0.1554,  ..., -0.2667,  0.0836,  0.1764],\n",
              "         [ 0.0718,  0.4776, -0.1197,  ..., -0.1603, -0.1169, -0.1548]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.0889, -0.2214, -0.1897,  ...,  0.1750,  0.0254,  0.2994],\n",
              "         [-0.5090,  0.0399, -0.0573,  ..., -0.0902, -0.3895,  0.2158],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 0.0996, -0.1153, -0.2090,  ...,  0.1639,  0.0145,  0.2561],\n",
              "         [-0.4124, -0.2316, -0.2954,  ..., -0.0398, -0.1878,  0.0525],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 0.0738, -0.1795, -0.3875,  ...,  0.1257,  0.1112,  0.2653],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu-NFWRcB1Eo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "82949bcd-ff6a-4b28-d578-3d428bd1a8c9"
      },
      "source": [
        "test_encoder_hidden"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2397, -0.0729,  0.2198,  ...,  0.2496,  0.3017, -0.1023],\n",
              "         [-0.2586, -0.0727,  0.1198,  ...,  0.0066, -0.1225, -0.0275],\n",
              "         [-0.2783, -0.3961,  0.2622,  ...,  0.1249,  0.3084, -0.2977],\n",
              "         [-0.3069,  0.0089,  0.1536,  ...,  0.1669,  0.1115, -0.3916],\n",
              "         [-0.0576, -0.0919,  0.2675,  ..., -0.1120,  0.0609, -0.1331]],\n",
              "\n",
              "        [[-0.3621, -0.0660,  0.5599,  ...,  0.5015, -0.2158, -0.1692],\n",
              "         [-0.3085, -0.3094,  0.3714,  ...,  0.0356, -0.1288, -0.2949],\n",
              "         [-0.2986,  0.0719, -0.4138,  ...,  0.4697, -0.1066, -0.3631],\n",
              "         [-0.2765, -0.3849,  0.3876,  ...,  0.0835, -0.1182, -0.3298],\n",
              "         [ 0.1817,  0.4274,  0.0959,  ..., -0.2325, -0.2684, -0.5300]],\n",
              "\n",
              "        [[ 0.0510, -0.1097, -0.3827,  ...,  0.0940,  0.0936,  0.2927],\n",
              "         [-0.3620, -0.1696, -0.3724,  ..., -0.0652, -0.1162,  0.1204],\n",
              "         [ 0.1701, -0.1932, -0.3249,  ...,  0.0610, -0.1432, -0.0037],\n",
              "         [-0.1259, -0.0528,  0.0151,  ...,  0.0449,  0.0308,  0.1474],\n",
              "         [ 0.0755, -0.1968, -0.3798,  ...,  0.0075,  0.1547, -0.0506]],\n",
              "\n",
              "        [[ 0.1033,  0.0294, -0.1505,  ..., -0.2427, -0.2132, -0.0058],\n",
              "         [ 0.1133, -0.1353, -0.1348,  ..., -0.0970, -0.0131,  0.1711],\n",
              "         [-0.2310,  0.1722,  0.0706,  ...,  0.0797,  0.1423, -0.1209],\n",
              "         [ 0.0384, -0.2000, -0.1098,  ..., -0.1758,  0.0659,  0.1949],\n",
              "         [-0.1075,  0.1330, -0.0018,  ..., -0.0368, -0.0831,  0.1519]]],\n",
              "       grad_fn=<StackBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmnkRX1adOr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn_model = Attn(\"dot_method\",500)\n",
        "test_decoder = Decoder(attn_model,test_embedding,500,voc.num_word,2,0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTyBG8TLeGzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5b728a63-809e-48ab-989d-af51e28950d9"
      },
      "source": [
        "test_decoder"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (attn_model): Attn()\n",
              "  (embedding): Embedding(62373, 500)\n",
              "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
              "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
              "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
              "  (out): Linear(in_features=500, out_features=62373, bias=True)\n",
              "  (attn): Attn(\n",
              "    (model): Attn()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pVv2CsOhFzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_decoder_inp = torch.LongTensor([[sos_tok for _ in range(small_batch)]])\n",
        "test_decoder_out,test_decoder_hidden=test_decoder(test_decoder_inp,test_encoder_hidden[:test_decoder.n_layer],test_encoder_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HkS3VUvCCrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "57d11cff-482c-489c-8cb3-b973d0b0c601"
      },
      "source": [
        "test_decoder_out"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.5439e-05, 1.5685e-05, 1.3778e-05,  ..., 1.5627e-05, 1.6702e-05,\n",
              "         1.7193e-05],\n",
              "        [1.4397e-05, 1.4231e-05, 1.5495e-05,  ..., 1.5323e-05, 1.5542e-05,\n",
              "         1.5327e-05],\n",
              "        [1.6406e-05, 1.5338e-05, 1.4122e-05,  ..., 1.5349e-05, 1.6031e-05,\n",
              "         1.6602e-05],\n",
              "        [1.4057e-05, 1.4948e-05, 1.5233e-05,  ..., 1.4723e-05, 1.5752e-05,\n",
              "         1.5156e-05],\n",
              "        [1.4978e-05, 1.5121e-05, 1.4538e-05,  ..., 1.4230e-05, 1.5385e-05,\n",
              "         1.7278e-05]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-O_v1LCGon",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "50d11408-a3d3-41bd-8e2e-693ace246ba5"
      },
      "source": [
        "test_decoder_hidden"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0095, -0.2213, -0.2701,  ..., -0.1046,  0.1072, -0.5149],\n",
              "         [ 0.1323,  0.0276, -0.2562,  ..., -0.3792, -0.3341,  0.0388],\n",
              "         [-0.3336, -0.3931, -0.1800,  ..., -0.0891, -0.0394, -0.4566],\n",
              "         [-0.3037, -0.1055, -0.1139,  ..., -0.2447, -0.0682, -0.2243],\n",
              "         [-0.0787, -0.0702, -0.2150,  ..., -0.3388, -0.2305, -0.1554]],\n",
              "\n",
              "        [[-0.4088, -0.2032,  0.2622,  ...,  0.3815, -0.0531, -0.2574],\n",
              "         [-0.3724, -0.2445,  0.1914,  ...,  0.0421, -0.0387, -0.3022],\n",
              "         [-0.2853, -0.1209, -0.2697,  ...,  0.2654,  0.0064, -0.2627],\n",
              "         [-0.3558, -0.3214,  0.1092,  ...,  0.0494,  0.0412, -0.3471],\n",
              "         [-0.1319,  0.0815, -0.0278,  ..., -0.1169, -0.1132, -0.3483]]],\n",
              "       grad_fn=<StackBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dSBO-baZJ35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EegjvdJXpXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining training fuction\n",
        "def train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,embedding,encoder_optim,decoder_optim,batch_size,clip,max_length=20):\n",
        "  #zero gradient\n",
        "  encoder_optim.zero_grad()\n",
        "  decoder_optim.zero_grad()\n",
        "\n",
        "  #device options\n",
        "  input_variable = input_variable.to(device)\n",
        "  lengths = lengths.to(device)\n",
        "  target_variable = target_variable.to(device)\n",
        "  mask = mask.to(device)\n",
        "\n",
        "  #initializing imp variables\n",
        "  loss = 0 \n",
        "  print_losses = []\n",
        "  n_total = 0\n",
        "\n",
        "  #forward pass\n",
        "  encoder_out,encoder_hidden = encoder(input_variable,lengths)\n",
        "  #decoder initials\n",
        "  decoder_input = torch.LongTensor([[sos_tok for _ in range(batch_size)]])\n",
        "  decoder_input = decoder_input.to(device)\n",
        "  decoder_hidden = encoder_hidden[:decoder.n_layer]\n",
        "  #determining when to use teacher forcing\n",
        "  use_teacher_forcing = True if random.random()<teacher_forcing_ratio else False\n",
        "  #forwarding sequence to decoder\n",
        "  if use_teacher_forcing:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_out,decoder_hidden = decoder(decoder_input,decoder_hidden,encoder_out)\n",
        "      decoder_input = target_variable[t].view(1,-1)\n",
        "      mask_loss,n_total =maskNLLLoss(decoder_out,target_variable[t],mask[t])\n",
        "      loss += mask_loss\n",
        "      print_losses.append(mask_loss.item()*n_total)\n",
        "      n_total +=n_total\n",
        "\n",
        "  else:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_out,decoder_hidden = decoder(decoder_input,decoder_hidden,encoder_out)\n",
        "      _ , topi = decoder_out.topk(1)\n",
        "      decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      mask_loss , n_total = maskNLLLoss(decoder_out,target_variable[t],mask[t])\n",
        "      loss += mask_loss\n",
        "      print_losses.append(mask_loss.item()*n_total)\n",
        "      n_total += n_total\n",
        "\n",
        "  loss.backward()\n",
        "  #clipping gradients for not having vanishing gradient problem\n",
        "  _ = nn.utils.clip_grad_norm_(encoder.parameters(),clip)\n",
        "  _ = nn.utils.clip_grad_norm_(decoder.parameters(),clip)\n",
        "  #adjusting weights\n",
        "  encoder_optim.step()\n",
        "  decoder_optim.step()\n",
        "  return sum(print_losses)/n_total\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IMfv-M7jJY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aff76d39-2039-4a27-c585-c79334786c30"
      },
      "source": [
        "#testing train function for batch_size = 5 for 1 epoch\n",
        "test_encoder = test_encoder.to(device)\n",
        "test_decoder = test_decoder.to(device)\n",
        "test_encoder_optim = optim.Adam(test_encoder.parameters(),lr=0.0001)\n",
        "test_decoder_optim = optim.Adam(test_decoder.parameters(),lr=0.0001)\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "train(input_variable,lengths,target_variable,mask,max_target_len,test_encoder,test_decoder,test_embedding,test_encoder_optim,test_decoder_optim,small_batch,clip)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191.62844228744507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFh5ZwMZOdxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL1FociuqN92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(model_name,pairs,voc,encoder,decoder,encoder_optim,decoder_optim,embedding,encoder_n_layer,decoder_n_layer,save_dir,n_iteration,batch_size,print_every,save_every,clip,dataset_name,loadfile):\n",
        "  training_batches = [batch2traindata(voc,[random.choice(pairs)for _ in range(batch_size)])for _ in range(n_iteration)]\n",
        "  #initializations\n",
        "  print(\"starting Initialization.......\")\n",
        "  start_iteration = 1\n",
        "  print_loss = 0\n",
        "  if loadfile:\n",
        "    start_iteration = checkpoint['iteration']+1\n",
        "\n",
        "  #training loop\n",
        "  print(\"Starting training......\")\n",
        "  for iteration in range(start_iteration,n_iteration+1):\n",
        "    training_batch = training_batches[iteration-1]\n",
        "    #extraction of fields\n",
        "    input_variable ,lengths,target_variable,mask,max_target_len = training_batch\n",
        "    #running a training iteration with batch\n",
        "    loss = train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,embedding,encoder_optim,decoder_optim,batch_size,clip)\n",
        "    print_loss +=loss\n",
        "    #print progress\n",
        "    if iteration%print_every ==0:\n",
        "      print_loss_avg = print_loss/print_every\n",
        "      print(\"Iteration:{};Percent complete:{:.1f}%; Average loss:{:.4f}\".format(iteration,iteration/n_iteration*100,print_loss_avg))\n",
        "      print_loss = 0\n",
        "\n",
        "    #saving checkpoint\n",
        "    if(iteration%save_every==0):\n",
        "      torch.save({\n",
        "          'iteration':iteration,\n",
        "          'en':encoder.state_dict(),\n",
        "          'de':decoder.state_dict(),\n",
        "          'en_opt':encoder_optim.state_dict(),\n",
        "          'de_opt':decoder_optim.state_dict(),\n",
        "          'loss':loss,\n",
        "          'voc_dict':voc.__dict__,\n",
        "          'embedding':embedding.state_dict()\n",
        "      },os.path.join(save_dir,'{}_{}.tar'.format(iteration,'checkpoint')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfX2YZCEOyR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea108811-aa26-4e00-ea2a-388dd494c5eb"
      },
      "source": [
        "#model hyperparameters\n",
        "model_name = 'joke_cb_model'\n",
        "attn_model = 'dot'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "loadfile = None\n",
        "epochs = 4000\n",
        "print(\"Building the model....\")\n",
        "embedding = nn.Embedding(voc.num_word,hidden_size)\n",
        "encoder = Encoder(hidden_size,embedding,encoder_n_layers,dropout)\n",
        "decoder = Decoder(attn_model,embedding,hidden_size,voc.num_word,decoder_n_layers,dropout)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print(\"Model is ready.......\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the model....\n",
            "Model is ready.......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh7BHS4fjEiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "09611e48-e248-49b7-ce66-2999d750b786"
      },
      "source": [
        "#training hyperparameters\n",
        "clip =50.0\n",
        "teacher_forcing_ratio = 0.5\n",
        "learning_rate = 0.00001\n",
        "decoder_learning_ratio = 2.0\n",
        "epochs = 4000\n",
        "print_every = 1\n",
        "save_every = 1000\n",
        "encoder.train()\n",
        "decoder.train()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(62373, 500)\n",
              "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
              "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
              "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
              "  (out): Linear(in_features=500, out_features=62373, bias=True)\n",
              "  (attn): Attn()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V9ekCAGj4r1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75f06ebc-9bbd-404b-cb9b-cdaabebe9306"
      },
      "source": [
        "save_dir = '/content/'\n",
        "dataset_name = 'dataset'\n",
        "print(\"Setting the optimizer.....\")\n",
        "encoder_optim = optim.Adam(encoder.parameters(),lr=learning_rate)\n",
        "decoder_optim = optim.Adam(decoder.parameters(),lr=learning_rate*decoder_learning_ratio)\n",
        "\n",
        "print(\"Starting training process.......\")\n",
        "trainIters(model_name,clean_pairs,voc,encoder,decoder,encoder_optim,decoder_optim,embedding,encoder_n_layers,decoder_n_layers,save_dir,epochs,batch_size,print_every,save_every,clip,dataset_name,loadfile)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting the optimizer.....\n",
            "Starting training process.......\n",
            "starting Initialization.......\n",
            "Starting training......\n",
            "Iteration:1;Percent complete:0.0%; Average loss:1811.1058\n",
            "Iteration:2;Percent complete:0.1%; Average loss:1543.7636\n",
            "Iteration:3;Percent complete:0.1%; Average loss:1626.4701\n",
            "Iteration:4;Percent complete:0.1%; Average loss:1706.5756\n",
            "Iteration:5;Percent complete:0.1%; Average loss:1627.8812\n",
            "Iteration:6;Percent complete:0.1%; Average loss:1483.0435\n",
            "Iteration:7;Percent complete:0.2%; Average loss:490.9572\n",
            "Iteration:8;Percent complete:0.2%; Average loss:1677.1935\n",
            "Iteration:9;Percent complete:0.2%; Average loss:503.5834\n",
            "Iteration:10;Percent complete:0.2%; Average loss:1778.4161\n",
            "Iteration:11;Percent complete:0.3%; Average loss:857.9993\n",
            "Iteration:12;Percent complete:0.3%; Average loss:1541.5554\n",
            "Iteration:13;Percent complete:0.3%; Average loss:1348.5232\n",
            "Iteration:14;Percent complete:0.4%; Average loss:852.9066\n",
            "Iteration:15;Percent complete:0.4%; Average loss:1484.2325\n",
            "Iteration:16;Percent complete:0.4%; Average loss:1509.3225\n",
            "Iteration:17;Percent complete:0.4%; Average loss:856.6284\n",
            "Iteration:18;Percent complete:0.4%; Average loss:1503.1986\n",
            "Iteration:19;Percent complete:0.5%; Average loss:811.5384\n",
            "Iteration:20;Percent complete:0.5%; Average loss:1478.7023\n",
            "Iteration:21;Percent complete:0.5%; Average loss:1396.2854\n",
            "Iteration:22;Percent complete:0.5%; Average loss:1857.0604\n",
            "Iteration:23;Percent complete:0.6%; Average loss:1395.4384\n",
            "Iteration:24;Percent complete:0.6%; Average loss:1374.8882\n",
            "Iteration:25;Percent complete:0.6%; Average loss:1673.8104\n",
            "Iteration:26;Percent complete:0.7%; Average loss:1426.2698\n",
            "Iteration:27;Percent complete:0.7%; Average loss:1492.9107\n",
            "Iteration:28;Percent complete:0.7%; Average loss:1616.4130\n",
            "Iteration:29;Percent complete:0.7%; Average loss:1553.9853\n",
            "Iteration:30;Percent complete:0.8%; Average loss:822.2169\n",
            "Iteration:31;Percent complete:0.8%; Average loss:1481.1302\n",
            "Iteration:32;Percent complete:0.8%; Average loss:1548.8394\n",
            "Iteration:33;Percent complete:0.8%; Average loss:1513.0356\n",
            "Iteration:34;Percent complete:0.9%; Average loss:786.3126\n",
            "Iteration:35;Percent complete:0.9%; Average loss:1565.1858\n",
            "Iteration:36;Percent complete:0.9%; Average loss:1546.9893\n",
            "Iteration:37;Percent complete:0.9%; Average loss:1685.6278\n",
            "Iteration:38;Percent complete:0.9%; Average loss:1587.9026\n",
            "Iteration:39;Percent complete:1.0%; Average loss:1460.2647\n",
            "Iteration:40;Percent complete:1.0%; Average loss:1564.6059\n",
            "Iteration:41;Percent complete:1.0%; Average loss:1546.7172\n",
            "Iteration:42;Percent complete:1.1%; Average loss:1562.5293\n",
            "Iteration:43;Percent complete:1.1%; Average loss:1795.1190\n",
            "Iteration:44;Percent complete:1.1%; Average loss:1531.2920\n",
            "Iteration:45;Percent complete:1.1%; Average loss:1637.8965\n",
            "Iteration:46;Percent complete:1.1%; Average loss:1423.4528\n",
            "Iteration:47;Percent complete:1.2%; Average loss:1674.3000\n",
            "Iteration:48;Percent complete:1.2%; Average loss:1381.6151\n",
            "Iteration:49;Percent complete:1.2%; Average loss:1359.5275\n",
            "Iteration:50;Percent complete:1.2%; Average loss:1649.0280\n",
            "Iteration:51;Percent complete:1.3%; Average loss:1740.7034\n",
            "Iteration:52;Percent complete:1.3%; Average loss:1780.9357\n",
            "Iteration:53;Percent complete:1.3%; Average loss:749.2710\n",
            "Iteration:54;Percent complete:1.4%; Average loss:1418.7037\n",
            "Iteration:55;Percent complete:1.4%; Average loss:1452.7434\n",
            "Iteration:56;Percent complete:1.4%; Average loss:1508.6777\n",
            "Iteration:57;Percent complete:1.4%; Average loss:1631.9261\n",
            "Iteration:58;Percent complete:1.5%; Average loss:1604.2407\n",
            "Iteration:59;Percent complete:1.5%; Average loss:868.7636\n",
            "Iteration:60;Percent complete:1.5%; Average loss:1770.1224\n",
            "Iteration:61;Percent complete:1.5%; Average loss:1714.5643\n",
            "Iteration:62;Percent complete:1.6%; Average loss:1505.3227\n",
            "Iteration:63;Percent complete:1.6%; Average loss:1557.2734\n",
            "Iteration:64;Percent complete:1.6%; Average loss:1606.9093\n",
            "Iteration:65;Percent complete:1.6%; Average loss:1513.5178\n",
            "Iteration:66;Percent complete:1.7%; Average loss:1730.9579\n",
            "Iteration:67;Percent complete:1.7%; Average loss:1532.2529\n",
            "Iteration:68;Percent complete:1.7%; Average loss:1378.8673\n",
            "Iteration:69;Percent complete:1.7%; Average loss:1466.5384\n",
            "Iteration:70;Percent complete:1.8%; Average loss:1308.6775\n",
            "Iteration:71;Percent complete:1.8%; Average loss:1507.1100\n",
            "Iteration:72;Percent complete:1.8%; Average loss:1342.6423\n",
            "Iteration:73;Percent complete:1.8%; Average loss:1697.2565\n",
            "Iteration:74;Percent complete:1.8%; Average loss:410.8272\n",
            "Iteration:75;Percent complete:1.9%; Average loss:770.0930\n",
            "Iteration:76;Percent complete:1.9%; Average loss:1560.4606\n",
            "Iteration:77;Percent complete:1.9%; Average loss:749.3912\n",
            "Iteration:78;Percent complete:1.9%; Average loss:1687.6961\n",
            "Iteration:79;Percent complete:2.0%; Average loss:1702.5092\n",
            "Iteration:80;Percent complete:2.0%; Average loss:1553.7652\n",
            "Iteration:81;Percent complete:2.0%; Average loss:1532.9373\n",
            "Iteration:82;Percent complete:2.1%; Average loss:1465.5036\n",
            "Iteration:83;Percent complete:2.1%; Average loss:818.4022\n",
            "Iteration:84;Percent complete:2.1%; Average loss:776.6457\n",
            "Iteration:85;Percent complete:2.1%; Average loss:1442.7305\n",
            "Iteration:86;Percent complete:2.1%; Average loss:1469.0283\n",
            "Iteration:87;Percent complete:2.2%; Average loss:1585.6371\n",
            "Iteration:88;Percent complete:2.2%; Average loss:1425.9722\n",
            "Iteration:89;Percent complete:2.2%; Average loss:713.4544\n",
            "Iteration:90;Percent complete:2.2%; Average loss:1514.3113\n",
            "Iteration:91;Percent complete:2.3%; Average loss:1525.6818\n",
            "Iteration:92;Percent complete:2.3%; Average loss:1423.8584\n",
            "Iteration:93;Percent complete:2.3%; Average loss:1516.1379\n",
            "Iteration:94;Percent complete:2.4%; Average loss:1574.9685\n",
            "Iteration:95;Percent complete:2.4%; Average loss:1513.9891\n",
            "Iteration:96;Percent complete:2.4%; Average loss:1437.2642\n",
            "Iteration:97;Percent complete:2.4%; Average loss:1497.2463\n",
            "Iteration:98;Percent complete:2.5%; Average loss:1179.6070\n",
            "Iteration:99;Percent complete:2.5%; Average loss:713.2904\n",
            "Iteration:100;Percent complete:2.5%; Average loss:501.0870\n",
            "Iteration:101;Percent complete:2.5%; Average loss:1441.6993\n",
            "Iteration:102;Percent complete:2.5%; Average loss:1460.1273\n",
            "Iteration:103;Percent complete:2.6%; Average loss:1451.8742\n",
            "Iteration:104;Percent complete:2.6%; Average loss:1455.9987\n",
            "Iteration:105;Percent complete:2.6%; Average loss:1554.8409\n",
            "Iteration:106;Percent complete:2.6%; Average loss:1700.2046\n",
            "Iteration:107;Percent complete:2.7%; Average loss:1384.3217\n",
            "Iteration:108;Percent complete:2.7%; Average loss:1555.2406\n",
            "Iteration:109;Percent complete:2.7%; Average loss:725.3691\n",
            "Iteration:110;Percent complete:2.8%; Average loss:755.8184\n",
            "Iteration:111;Percent complete:2.8%; Average loss:1584.2644\n",
            "Iteration:112;Percent complete:2.8%; Average loss:867.8206\n",
            "Iteration:113;Percent complete:2.8%; Average loss:1481.0485\n",
            "Iteration:114;Percent complete:2.9%; Average loss:1598.3293\n",
            "Iteration:115;Percent complete:2.9%; Average loss:1492.4178\n",
            "Iteration:116;Percent complete:2.9%; Average loss:739.1627\n",
            "Iteration:117;Percent complete:2.9%; Average loss:756.1483\n",
            "Iteration:118;Percent complete:2.9%; Average loss:1737.6472\n",
            "Iteration:119;Percent complete:3.0%; Average loss:1481.6107\n",
            "Iteration:120;Percent complete:3.0%; Average loss:1509.1835\n",
            "Iteration:121;Percent complete:3.0%; Average loss:1380.5105\n",
            "Iteration:122;Percent complete:3.0%; Average loss:818.5027\n",
            "Iteration:123;Percent complete:3.1%; Average loss:1609.2690\n",
            "Iteration:124;Percent complete:3.1%; Average loss:1484.5439\n",
            "Iteration:125;Percent complete:3.1%; Average loss:792.3517\n",
            "Iteration:126;Percent complete:3.1%; Average loss:599.6161\n",
            "Iteration:127;Percent complete:3.2%; Average loss:1587.2830\n",
            "Iteration:128;Percent complete:3.2%; Average loss:1604.9010\n",
            "Iteration:129;Percent complete:3.2%; Average loss:748.4618\n",
            "Iteration:130;Percent complete:3.2%; Average loss:1533.1860\n",
            "Iteration:131;Percent complete:3.3%; Average loss:1494.4493\n",
            "Iteration:132;Percent complete:3.3%; Average loss:757.9232\n",
            "Iteration:133;Percent complete:3.3%; Average loss:783.5564\n",
            "Iteration:134;Percent complete:3.4%; Average loss:1697.6355\n",
            "Iteration:135;Percent complete:3.4%; Average loss:530.3037\n",
            "Iteration:136;Percent complete:3.4%; Average loss:1530.0703\n",
            "Iteration:137;Percent complete:3.4%; Average loss:787.3909\n",
            "Iteration:138;Percent complete:3.5%; Average loss:1468.0033\n",
            "Iteration:139;Percent complete:3.5%; Average loss:1254.8248\n",
            "Iteration:140;Percent complete:3.5%; Average loss:1475.1127\n",
            "Iteration:141;Percent complete:3.5%; Average loss:1710.0355\n",
            "Iteration:142;Percent complete:3.5%; Average loss:1619.9537\n",
            "Iteration:143;Percent complete:3.6%; Average loss:813.4876\n",
            "Iteration:144;Percent complete:3.6%; Average loss:1634.9724\n",
            "Iteration:145;Percent complete:3.6%; Average loss:751.9016\n",
            "Iteration:146;Percent complete:3.6%; Average loss:1454.3685\n",
            "Iteration:147;Percent complete:3.7%; Average loss:1539.1239\n",
            "Iteration:148;Percent complete:3.7%; Average loss:1621.1838\n",
            "Iteration:149;Percent complete:3.7%; Average loss:1676.3103\n",
            "Iteration:150;Percent complete:3.8%; Average loss:1346.6090\n",
            "Iteration:151;Percent complete:3.8%; Average loss:1678.0685\n",
            "Iteration:152;Percent complete:3.8%; Average loss:1643.3006\n",
            "Iteration:153;Percent complete:3.8%; Average loss:1404.4657\n",
            "Iteration:154;Percent complete:3.9%; Average loss:1483.4364\n",
            "Iteration:155;Percent complete:3.9%; Average loss:1539.1717\n",
            "Iteration:156;Percent complete:3.9%; Average loss:1495.7861\n",
            "Iteration:157;Percent complete:3.9%; Average loss:1458.9350\n",
            "Iteration:158;Percent complete:4.0%; Average loss:1574.9887\n",
            "Iteration:159;Percent complete:4.0%; Average loss:1548.3207\n",
            "Iteration:160;Percent complete:4.0%; Average loss:1365.1170\n",
            "Iteration:161;Percent complete:4.0%; Average loss:1427.3957\n",
            "Iteration:162;Percent complete:4.0%; Average loss:1359.6863\n",
            "Iteration:163;Percent complete:4.1%; Average loss:761.2587\n",
            "Iteration:164;Percent complete:4.1%; Average loss:791.5201\n",
            "Iteration:165;Percent complete:4.1%; Average loss:1504.6524\n",
            "Iteration:166;Percent complete:4.2%; Average loss:1276.8597\n",
            "Iteration:167;Percent complete:4.2%; Average loss:1242.4646\n",
            "Iteration:168;Percent complete:4.2%; Average loss:1368.3240\n",
            "Iteration:169;Percent complete:4.2%; Average loss:1387.8834\n",
            "Iteration:170;Percent complete:4.2%; Average loss:1568.3752\n",
            "Iteration:171;Percent complete:4.3%; Average loss:1351.9995\n",
            "Iteration:172;Percent complete:4.3%; Average loss:1602.5736\n",
            "Iteration:173;Percent complete:4.3%; Average loss:1284.9647\n",
            "Iteration:174;Percent complete:4.3%; Average loss:1479.3179\n",
            "Iteration:175;Percent complete:4.4%; Average loss:834.7095\n",
            "Iteration:176;Percent complete:4.4%; Average loss:1403.5592\n",
            "Iteration:177;Percent complete:4.4%; Average loss:1583.9411\n",
            "Iteration:178;Percent complete:4.5%; Average loss:1395.7398\n",
            "Iteration:179;Percent complete:4.5%; Average loss:1376.2797\n",
            "Iteration:180;Percent complete:4.5%; Average loss:1631.8974\n",
            "Iteration:181;Percent complete:4.5%; Average loss:1641.3005\n",
            "Iteration:182;Percent complete:4.5%; Average loss:1404.9675\n",
            "Iteration:183;Percent complete:4.6%; Average loss:561.1286\n",
            "Iteration:184;Percent complete:4.6%; Average loss:1440.9153\n",
            "Iteration:185;Percent complete:4.6%; Average loss:1279.3051\n",
            "Iteration:186;Percent complete:4.7%; Average loss:1517.8297\n",
            "Iteration:187;Percent complete:4.7%; Average loss:1550.0667\n",
            "Iteration:188;Percent complete:4.7%; Average loss:1583.6262\n",
            "Iteration:189;Percent complete:4.7%; Average loss:1479.4443\n",
            "Iteration:190;Percent complete:4.8%; Average loss:1488.8209\n",
            "Iteration:191;Percent complete:4.8%; Average loss:829.8520\n",
            "Iteration:192;Percent complete:4.8%; Average loss:1342.8164\n",
            "Iteration:193;Percent complete:4.8%; Average loss:749.7601\n",
            "Iteration:194;Percent complete:4.9%; Average loss:1674.7071\n",
            "Iteration:195;Percent complete:4.9%; Average loss:1551.8796\n",
            "Iteration:196;Percent complete:4.9%; Average loss:1779.4704\n",
            "Iteration:197;Percent complete:4.9%; Average loss:1453.3315\n",
            "Iteration:198;Percent complete:5.0%; Average loss:791.3131\n",
            "Iteration:199;Percent complete:5.0%; Average loss:699.0830\n",
            "Iteration:200;Percent complete:5.0%; Average loss:1545.7310\n",
            "Iteration:201;Percent complete:5.0%; Average loss:1466.3624\n",
            "Iteration:202;Percent complete:5.1%; Average loss:1414.2053\n",
            "Iteration:203;Percent complete:5.1%; Average loss:775.3433\n",
            "Iteration:204;Percent complete:5.1%; Average loss:1305.8806\n",
            "Iteration:205;Percent complete:5.1%; Average loss:747.3333\n",
            "Iteration:206;Percent complete:5.1%; Average loss:717.9034\n",
            "Iteration:207;Percent complete:5.2%; Average loss:1612.3295\n",
            "Iteration:208;Percent complete:5.2%; Average loss:1357.8152\n",
            "Iteration:209;Percent complete:5.2%; Average loss:1494.2292\n",
            "Iteration:210;Percent complete:5.2%; Average loss:1332.3665\n",
            "Iteration:211;Percent complete:5.3%; Average loss:819.9418\n",
            "Iteration:212;Percent complete:5.3%; Average loss:1464.9005\n",
            "Iteration:213;Percent complete:5.3%; Average loss:1347.3088\n",
            "Iteration:214;Percent complete:5.3%; Average loss:1560.0462\n",
            "Iteration:215;Percent complete:5.4%; Average loss:1481.8694\n",
            "Iteration:216;Percent complete:5.4%; Average loss:779.9690\n",
            "Iteration:217;Percent complete:5.4%; Average loss:1504.3424\n",
            "Iteration:218;Percent complete:5.5%; Average loss:1475.0262\n",
            "Iteration:219;Percent complete:5.5%; Average loss:803.5616\n",
            "Iteration:220;Percent complete:5.5%; Average loss:1624.3406\n",
            "Iteration:221;Percent complete:5.5%; Average loss:1399.4402\n",
            "Iteration:222;Percent complete:5.5%; Average loss:1427.8940\n",
            "Iteration:223;Percent complete:5.6%; Average loss:1308.6475\n",
            "Iteration:224;Percent complete:5.6%; Average loss:1371.5305\n",
            "Iteration:225;Percent complete:5.6%; Average loss:1814.1016\n",
            "Iteration:226;Percent complete:5.7%; Average loss:1638.1626\n",
            "Iteration:227;Percent complete:5.7%; Average loss:735.9602\n",
            "Iteration:228;Percent complete:5.7%; Average loss:1609.4287\n",
            "Iteration:229;Percent complete:5.7%; Average loss:1644.2516\n",
            "Iteration:230;Percent complete:5.8%; Average loss:1346.1533\n",
            "Iteration:231;Percent complete:5.8%; Average loss:1509.6384\n",
            "Iteration:232;Percent complete:5.8%; Average loss:1461.6191\n",
            "Iteration:233;Percent complete:5.8%; Average loss:1450.1264\n",
            "Iteration:234;Percent complete:5.9%; Average loss:567.1690\n",
            "Iteration:235;Percent complete:5.9%; Average loss:1355.0158\n",
            "Iteration:236;Percent complete:5.9%; Average loss:796.2738\n",
            "Iteration:237;Percent complete:5.9%; Average loss:1706.1343\n",
            "Iteration:238;Percent complete:5.9%; Average loss:1410.2765\n",
            "Iteration:239;Percent complete:6.0%; Average loss:812.2860\n",
            "Iteration:240;Percent complete:6.0%; Average loss:705.8761\n",
            "Iteration:241;Percent complete:6.0%; Average loss:1348.1713\n",
            "Iteration:242;Percent complete:6.0%; Average loss:1586.1185\n",
            "Iteration:243;Percent complete:6.1%; Average loss:1425.7117\n",
            "Iteration:244;Percent complete:6.1%; Average loss:1487.2900\n",
            "Iteration:245;Percent complete:6.1%; Average loss:1605.3767\n",
            "Iteration:246;Percent complete:6.2%; Average loss:1441.9138\n",
            "Iteration:247;Percent complete:6.2%; Average loss:740.3424\n",
            "Iteration:248;Percent complete:6.2%; Average loss:1558.6343\n",
            "Iteration:249;Percent complete:6.2%; Average loss:726.5451\n",
            "Iteration:250;Percent complete:6.2%; Average loss:1346.9364\n",
            "Iteration:251;Percent complete:6.3%; Average loss:1386.3891\n",
            "Iteration:252;Percent complete:6.3%; Average loss:1431.6628\n",
            "Iteration:253;Percent complete:6.3%; Average loss:448.3231\n",
            "Iteration:254;Percent complete:6.3%; Average loss:884.4327\n",
            "Iteration:255;Percent complete:6.4%; Average loss:1574.2166\n",
            "Iteration:256;Percent complete:6.4%; Average loss:494.4284\n",
            "Iteration:257;Percent complete:6.4%; Average loss:1351.4502\n",
            "Iteration:258;Percent complete:6.5%; Average loss:1398.4546\n",
            "Iteration:259;Percent complete:6.5%; Average loss:1485.3031\n",
            "Iteration:260;Percent complete:6.5%; Average loss:1607.1957\n",
            "Iteration:261;Percent complete:6.5%; Average loss:1430.5805\n",
            "Iteration:262;Percent complete:6.6%; Average loss:1356.7261\n",
            "Iteration:263;Percent complete:6.6%; Average loss:1441.5883\n",
            "Iteration:264;Percent complete:6.6%; Average loss:1535.8610\n",
            "Iteration:265;Percent complete:6.6%; Average loss:672.9573\n",
            "Iteration:266;Percent complete:6.7%; Average loss:490.6495\n",
            "Iteration:267;Percent complete:6.7%; Average loss:603.7565\n",
            "Iteration:268;Percent complete:6.7%; Average loss:1493.3899\n",
            "Iteration:269;Percent complete:6.7%; Average loss:1384.6405\n",
            "Iteration:270;Percent complete:6.8%; Average loss:1661.2256\n",
            "Iteration:271;Percent complete:6.8%; Average loss:1564.4086\n",
            "Iteration:272;Percent complete:6.8%; Average loss:462.0041\n",
            "Iteration:273;Percent complete:6.8%; Average loss:712.5660\n",
            "Iteration:274;Percent complete:6.9%; Average loss:1534.4173\n",
            "Iteration:275;Percent complete:6.9%; Average loss:1470.5013\n",
            "Iteration:276;Percent complete:6.9%; Average loss:1487.7482\n",
            "Iteration:277;Percent complete:6.9%; Average loss:1470.7948\n",
            "Iteration:278;Percent complete:7.0%; Average loss:1417.5230\n",
            "Iteration:279;Percent complete:7.0%; Average loss:1526.6531\n",
            "Iteration:280;Percent complete:7.0%; Average loss:1401.3330\n",
            "Iteration:281;Percent complete:7.0%; Average loss:834.7493\n",
            "Iteration:282;Percent complete:7.0%; Average loss:703.1649\n",
            "Iteration:283;Percent complete:7.1%; Average loss:1295.6137\n",
            "Iteration:284;Percent complete:7.1%; Average loss:1680.5064\n",
            "Iteration:285;Percent complete:7.1%; Average loss:1575.5565\n",
            "Iteration:286;Percent complete:7.1%; Average loss:1400.2303\n",
            "Iteration:287;Percent complete:7.2%; Average loss:1434.7882\n",
            "Iteration:288;Percent complete:7.2%; Average loss:1515.8833\n",
            "Iteration:289;Percent complete:7.2%; Average loss:1550.3564\n",
            "Iteration:290;Percent complete:7.2%; Average loss:1419.0556\n",
            "Iteration:291;Percent complete:7.3%; Average loss:1506.7446\n",
            "Iteration:292;Percent complete:7.3%; Average loss:1395.8636\n",
            "Iteration:293;Percent complete:7.3%; Average loss:666.6279\n",
            "Iteration:294;Percent complete:7.3%; Average loss:1395.3933\n",
            "Iteration:295;Percent complete:7.4%; Average loss:625.0128\n",
            "Iteration:296;Percent complete:7.4%; Average loss:1321.7387\n",
            "Iteration:297;Percent complete:7.4%; Average loss:1569.6559\n",
            "Iteration:298;Percent complete:7.4%; Average loss:489.9112\n",
            "Iteration:299;Percent complete:7.5%; Average loss:1391.7509\n",
            "Iteration:300;Percent complete:7.5%; Average loss:1258.1279\n",
            "Iteration:301;Percent complete:7.5%; Average loss:1567.4791\n",
            "Iteration:302;Percent complete:7.5%; Average loss:1499.7092\n",
            "Iteration:303;Percent complete:7.6%; Average loss:1377.8541\n",
            "Iteration:304;Percent complete:7.6%; Average loss:1463.1387\n",
            "Iteration:305;Percent complete:7.6%; Average loss:1488.7650\n",
            "Iteration:306;Percent complete:7.6%; Average loss:1633.4602\n",
            "Iteration:307;Percent complete:7.7%; Average loss:1450.8098\n",
            "Iteration:308;Percent complete:7.7%; Average loss:1440.9278\n",
            "Iteration:309;Percent complete:7.7%; Average loss:1348.5861\n",
            "Iteration:310;Percent complete:7.8%; Average loss:1253.7997\n",
            "Iteration:311;Percent complete:7.8%; Average loss:1415.8281\n",
            "Iteration:312;Percent complete:7.8%; Average loss:1351.8184\n",
            "Iteration:313;Percent complete:7.8%; Average loss:1281.8328\n",
            "Iteration:314;Percent complete:7.8%; Average loss:795.7902\n",
            "Iteration:315;Percent complete:7.9%; Average loss:1277.5106\n",
            "Iteration:316;Percent complete:7.9%; Average loss:1452.3987\n",
            "Iteration:317;Percent complete:7.9%; Average loss:1574.9897\n",
            "Iteration:318;Percent complete:8.0%; Average loss:783.6594\n",
            "Iteration:319;Percent complete:8.0%; Average loss:687.8153\n",
            "Iteration:320;Percent complete:8.0%; Average loss:1528.1562\n",
            "Iteration:321;Percent complete:8.0%; Average loss:1425.5789\n",
            "Iteration:322;Percent complete:8.1%; Average loss:781.3003\n",
            "Iteration:323;Percent complete:8.1%; Average loss:1593.9191\n",
            "Iteration:324;Percent complete:8.1%; Average loss:1380.7501\n",
            "Iteration:325;Percent complete:8.1%; Average loss:1363.7756\n",
            "Iteration:326;Percent complete:8.2%; Average loss:1389.3102\n",
            "Iteration:327;Percent complete:8.2%; Average loss:1766.0296\n",
            "Iteration:328;Percent complete:8.2%; Average loss:1404.6051\n",
            "Iteration:329;Percent complete:8.2%; Average loss:1638.4242\n",
            "Iteration:330;Percent complete:8.2%; Average loss:680.0812\n",
            "Iteration:331;Percent complete:8.3%; Average loss:1573.3397\n",
            "Iteration:332;Percent complete:8.3%; Average loss:1368.2255\n",
            "Iteration:333;Percent complete:8.3%; Average loss:1451.8925\n",
            "Iteration:334;Percent complete:8.3%; Average loss:752.6098\n",
            "Iteration:335;Percent complete:8.4%; Average loss:1460.7056\n",
            "Iteration:336;Percent complete:8.4%; Average loss:731.2905\n",
            "Iteration:337;Percent complete:8.4%; Average loss:1331.0486\n",
            "Iteration:338;Percent complete:8.5%; Average loss:1595.6861\n",
            "Iteration:339;Percent complete:8.5%; Average loss:840.5735\n",
            "Iteration:340;Percent complete:8.5%; Average loss:899.9497\n",
            "Iteration:341;Percent complete:8.5%; Average loss:1254.1160\n",
            "Iteration:342;Percent complete:8.6%; Average loss:1269.4651\n",
            "Iteration:343;Percent complete:8.6%; Average loss:1204.6641\n",
            "Iteration:344;Percent complete:8.6%; Average loss:322.0278\n",
            "Iteration:345;Percent complete:8.6%; Average loss:1112.3139\n",
            "Iteration:346;Percent complete:8.6%; Average loss:1519.2113\n",
            "Iteration:347;Percent complete:8.7%; Average loss:491.8024\n",
            "Iteration:348;Percent complete:8.7%; Average loss:1456.9741\n",
            "Iteration:349;Percent complete:8.7%; Average loss:844.1513\n",
            "Iteration:350;Percent complete:8.8%; Average loss:1426.8512\n",
            "Iteration:351;Percent complete:8.8%; Average loss:1457.9998\n",
            "Iteration:352;Percent complete:8.8%; Average loss:679.0289\n",
            "Iteration:353;Percent complete:8.8%; Average loss:1406.9548\n",
            "Iteration:354;Percent complete:8.8%; Average loss:1511.9995\n",
            "Iteration:355;Percent complete:8.9%; Average loss:688.2356\n",
            "Iteration:356;Percent complete:8.9%; Average loss:543.6198\n",
            "Iteration:357;Percent complete:8.9%; Average loss:1580.3845\n",
            "Iteration:358;Percent complete:8.9%; Average loss:1569.8962\n",
            "Iteration:359;Percent complete:9.0%; Average loss:1377.9867\n",
            "Iteration:360;Percent complete:9.0%; Average loss:1852.8698\n",
            "Iteration:361;Percent complete:9.0%; Average loss:1612.9914\n",
            "Iteration:362;Percent complete:9.0%; Average loss:680.4503\n",
            "Iteration:363;Percent complete:9.1%; Average loss:1556.2150\n",
            "Iteration:364;Percent complete:9.1%; Average loss:1252.7248\n",
            "Iteration:365;Percent complete:9.1%; Average loss:1403.3912\n",
            "Iteration:366;Percent complete:9.2%; Average loss:1502.6511\n",
            "Iteration:367;Percent complete:9.2%; Average loss:1532.0998\n",
            "Iteration:368;Percent complete:9.2%; Average loss:1240.7144\n",
            "Iteration:369;Percent complete:9.2%; Average loss:1447.5333\n",
            "Iteration:370;Percent complete:9.2%; Average loss:1431.6519\n",
            "Iteration:371;Percent complete:9.3%; Average loss:1489.4210\n",
            "Iteration:372;Percent complete:9.3%; Average loss:682.1787\n",
            "Iteration:373;Percent complete:9.3%; Average loss:676.6528\n",
            "Iteration:374;Percent complete:9.3%; Average loss:1466.6265\n",
            "Iteration:375;Percent complete:9.4%; Average loss:1470.8302\n",
            "Iteration:376;Percent complete:9.4%; Average loss:1411.6677\n",
            "Iteration:377;Percent complete:9.4%; Average loss:1463.2688\n",
            "Iteration:378;Percent complete:9.4%; Average loss:1499.9696\n",
            "Iteration:379;Percent complete:9.5%; Average loss:1125.1083\n",
            "Iteration:380;Percent complete:9.5%; Average loss:1335.6979\n",
            "Iteration:381;Percent complete:9.5%; Average loss:1627.4101\n",
            "Iteration:382;Percent complete:9.6%; Average loss:1415.2027\n",
            "Iteration:383;Percent complete:9.6%; Average loss:1326.3662\n",
            "Iteration:384;Percent complete:9.6%; Average loss:1321.9045\n",
            "Iteration:385;Percent complete:9.6%; Average loss:703.2545\n",
            "Iteration:386;Percent complete:9.7%; Average loss:1449.1110\n",
            "Iteration:387;Percent complete:9.7%; Average loss:1356.7617\n",
            "Iteration:388;Percent complete:9.7%; Average loss:1414.2689\n",
            "Iteration:389;Percent complete:9.7%; Average loss:821.6620\n",
            "Iteration:390;Percent complete:9.8%; Average loss:776.9213\n",
            "Iteration:391;Percent complete:9.8%; Average loss:1423.1958\n",
            "Iteration:392;Percent complete:9.8%; Average loss:1420.8728\n",
            "Iteration:393;Percent complete:9.8%; Average loss:1646.3319\n",
            "Iteration:394;Percent complete:9.8%; Average loss:449.7924\n",
            "Iteration:395;Percent complete:9.9%; Average loss:1384.4748\n",
            "Iteration:396;Percent complete:9.9%; Average loss:799.4748\n",
            "Iteration:397;Percent complete:9.9%; Average loss:744.2805\n",
            "Iteration:398;Percent complete:10.0%; Average loss:1373.4994\n",
            "Iteration:399;Percent complete:10.0%; Average loss:1564.6245\n",
            "Iteration:400;Percent complete:10.0%; Average loss:403.7057\n",
            "Iteration:401;Percent complete:10.0%; Average loss:372.5390\n",
            "Iteration:402;Percent complete:10.1%; Average loss:1476.7504\n",
            "Iteration:403;Percent complete:10.1%; Average loss:1441.8914\n",
            "Iteration:404;Percent complete:10.1%; Average loss:1515.4608\n",
            "Iteration:405;Percent complete:10.1%; Average loss:1643.0100\n",
            "Iteration:406;Percent complete:10.2%; Average loss:1462.7185\n",
            "Iteration:407;Percent complete:10.2%; Average loss:1520.2597\n",
            "Iteration:408;Percent complete:10.2%; Average loss:1639.8468\n",
            "Iteration:409;Percent complete:10.2%; Average loss:807.0760\n",
            "Iteration:410;Percent complete:10.2%; Average loss:700.1143\n",
            "Iteration:411;Percent complete:10.3%; Average loss:1424.6061\n",
            "Iteration:412;Percent complete:10.3%; Average loss:1598.0088\n",
            "Iteration:413;Percent complete:10.3%; Average loss:1626.4107\n",
            "Iteration:414;Percent complete:10.3%; Average loss:571.9512\n",
            "Iteration:415;Percent complete:10.4%; Average loss:1518.9386\n",
            "Iteration:416;Percent complete:10.4%; Average loss:1464.4573\n",
            "Iteration:417;Percent complete:10.4%; Average loss:1527.1208\n",
            "Iteration:418;Percent complete:10.4%; Average loss:1507.1484\n",
            "Iteration:419;Percent complete:10.5%; Average loss:689.9494\n",
            "Iteration:420;Percent complete:10.5%; Average loss:1596.7633\n",
            "Iteration:421;Percent complete:10.5%; Average loss:1537.6321\n",
            "Iteration:422;Percent complete:10.5%; Average loss:1486.4369\n",
            "Iteration:423;Percent complete:10.6%; Average loss:1352.7797\n",
            "Iteration:424;Percent complete:10.6%; Average loss:1482.3168\n",
            "Iteration:425;Percent complete:10.6%; Average loss:1450.4575\n",
            "Iteration:426;Percent complete:10.7%; Average loss:1515.2101\n",
            "Iteration:427;Percent complete:10.7%; Average loss:653.5489\n",
            "Iteration:428;Percent complete:10.7%; Average loss:1434.1467\n",
            "Iteration:429;Percent complete:10.7%; Average loss:1521.9947\n",
            "Iteration:430;Percent complete:10.8%; Average loss:1458.6413\n",
            "Iteration:431;Percent complete:10.8%; Average loss:735.6121\n",
            "Iteration:432;Percent complete:10.8%; Average loss:1446.9464\n",
            "Iteration:433;Percent complete:10.8%; Average loss:1406.7871\n",
            "Iteration:434;Percent complete:10.8%; Average loss:1393.8088\n",
            "Iteration:435;Percent complete:10.9%; Average loss:1372.6285\n",
            "Iteration:436;Percent complete:10.9%; Average loss:1438.8757\n",
            "Iteration:437;Percent complete:10.9%; Average loss:772.1515\n",
            "Iteration:438;Percent complete:10.9%; Average loss:1317.9470\n",
            "Iteration:439;Percent complete:11.0%; Average loss:1330.4092\n",
            "Iteration:440;Percent complete:11.0%; Average loss:1603.7303\n",
            "Iteration:441;Percent complete:11.0%; Average loss:711.9460\n",
            "Iteration:442;Percent complete:11.1%; Average loss:1590.1413\n",
            "Iteration:443;Percent complete:11.1%; Average loss:1474.5356\n",
            "Iteration:444;Percent complete:11.1%; Average loss:1392.5152\n",
            "Iteration:445;Percent complete:11.1%; Average loss:1280.1240\n",
            "Iteration:446;Percent complete:11.2%; Average loss:1579.6459\n",
            "Iteration:447;Percent complete:11.2%; Average loss:1834.9936\n",
            "Iteration:448;Percent complete:11.2%; Average loss:831.0898\n",
            "Iteration:449;Percent complete:11.2%; Average loss:1422.8740\n",
            "Iteration:450;Percent complete:11.2%; Average loss:1360.6048\n",
            "Iteration:451;Percent complete:11.3%; Average loss:516.2159\n",
            "Iteration:452;Percent complete:11.3%; Average loss:1439.5223\n",
            "Iteration:453;Percent complete:11.3%; Average loss:1407.3806\n",
            "Iteration:454;Percent complete:11.3%; Average loss:1635.7326\n",
            "Iteration:455;Percent complete:11.4%; Average loss:1410.0019\n",
            "Iteration:456;Percent complete:11.4%; Average loss:1381.0324\n",
            "Iteration:457;Percent complete:11.4%; Average loss:1309.2797\n",
            "Iteration:458;Percent complete:11.5%; Average loss:1513.4276\n",
            "Iteration:459;Percent complete:11.5%; Average loss:771.1729\n",
            "Iteration:460;Percent complete:11.5%; Average loss:1388.9513\n",
            "Iteration:461;Percent complete:11.5%; Average loss:1547.6272\n",
            "Iteration:462;Percent complete:11.6%; Average loss:1640.3976\n",
            "Iteration:463;Percent complete:11.6%; Average loss:1378.5923\n",
            "Iteration:464;Percent complete:11.6%; Average loss:1537.7008\n",
            "Iteration:465;Percent complete:11.6%; Average loss:719.0671\n",
            "Iteration:466;Percent complete:11.7%; Average loss:1410.7232\n",
            "Iteration:467;Percent complete:11.7%; Average loss:1441.9253\n",
            "Iteration:468;Percent complete:11.7%; Average loss:771.8573\n",
            "Iteration:469;Percent complete:11.7%; Average loss:1435.2347\n",
            "Iteration:470;Percent complete:11.8%; Average loss:1438.1285\n",
            "Iteration:471;Percent complete:11.8%; Average loss:1579.8672\n",
            "Iteration:472;Percent complete:11.8%; Average loss:1444.8179\n",
            "Iteration:473;Percent complete:11.8%; Average loss:1490.7015\n",
            "Iteration:474;Percent complete:11.8%; Average loss:1340.3057\n",
            "Iteration:475;Percent complete:11.9%; Average loss:1340.9571\n",
            "Iteration:476;Percent complete:11.9%; Average loss:1597.0435\n",
            "Iteration:477;Percent complete:11.9%; Average loss:1283.6996\n",
            "Iteration:478;Percent complete:11.9%; Average loss:1210.8613\n",
            "Iteration:479;Percent complete:12.0%; Average loss:1551.5734\n",
            "Iteration:480;Percent complete:12.0%; Average loss:1432.0054\n",
            "Iteration:481;Percent complete:12.0%; Average loss:1304.4436\n",
            "Iteration:482;Percent complete:12.0%; Average loss:1406.7637\n",
            "Iteration:483;Percent complete:12.1%; Average loss:1351.9129\n",
            "Iteration:484;Percent complete:12.1%; Average loss:1195.4783\n",
            "Iteration:485;Percent complete:12.1%; Average loss:1443.8424\n",
            "Iteration:486;Percent complete:12.2%; Average loss:649.8476\n",
            "Iteration:487;Percent complete:12.2%; Average loss:1568.3872\n",
            "Iteration:488;Percent complete:12.2%; Average loss:1551.0704\n",
            "Iteration:489;Percent complete:12.2%; Average loss:1580.5331\n",
            "Iteration:490;Percent complete:12.2%; Average loss:728.1654\n",
            "Iteration:491;Percent complete:12.3%; Average loss:675.8614\n",
            "Iteration:492;Percent complete:12.3%; Average loss:1469.0667\n",
            "Iteration:493;Percent complete:12.3%; Average loss:1557.1002\n",
            "Iteration:494;Percent complete:12.3%; Average loss:1315.5941\n",
            "Iteration:495;Percent complete:12.4%; Average loss:1452.3592\n",
            "Iteration:496;Percent complete:12.4%; Average loss:1249.7554\n",
            "Iteration:497;Percent complete:12.4%; Average loss:483.6505\n",
            "Iteration:498;Percent complete:12.4%; Average loss:1322.9948\n",
            "Iteration:499;Percent complete:12.5%; Average loss:718.6500\n",
            "Iteration:500;Percent complete:12.5%; Average loss:782.8357\n",
            "Iteration:501;Percent complete:12.5%; Average loss:771.5672\n",
            "Iteration:502;Percent complete:12.6%; Average loss:1579.7366\n",
            "Iteration:503;Percent complete:12.6%; Average loss:784.4133\n",
            "Iteration:504;Percent complete:12.6%; Average loss:1503.5519\n",
            "Iteration:505;Percent complete:12.6%; Average loss:403.5797\n",
            "Iteration:506;Percent complete:12.7%; Average loss:1348.4670\n",
            "Iteration:507;Percent complete:12.7%; Average loss:1509.0229\n",
            "Iteration:508;Percent complete:12.7%; Average loss:1603.2806\n",
            "Iteration:509;Percent complete:12.7%; Average loss:1528.5517\n",
            "Iteration:510;Percent complete:12.8%; Average loss:654.7077\n",
            "Iteration:511;Percent complete:12.8%; Average loss:1618.2786\n",
            "Iteration:512;Percent complete:12.8%; Average loss:1308.6762\n",
            "Iteration:513;Percent complete:12.8%; Average loss:1355.4154\n",
            "Iteration:514;Percent complete:12.8%; Average loss:738.6722\n",
            "Iteration:515;Percent complete:12.9%; Average loss:1457.8150\n",
            "Iteration:516;Percent complete:12.9%; Average loss:1488.1632\n",
            "Iteration:517;Percent complete:12.9%; Average loss:1376.7979\n",
            "Iteration:518;Percent complete:13.0%; Average loss:1466.5724\n",
            "Iteration:519;Percent complete:13.0%; Average loss:824.6614\n",
            "Iteration:520;Percent complete:13.0%; Average loss:1482.2794\n",
            "Iteration:521;Percent complete:13.0%; Average loss:726.5394\n",
            "Iteration:522;Percent complete:13.1%; Average loss:1519.2158\n",
            "Iteration:523;Percent complete:13.1%; Average loss:1499.1654\n",
            "Iteration:524;Percent complete:13.1%; Average loss:905.1920\n",
            "Iteration:525;Percent complete:13.1%; Average loss:710.6210\n",
            "Iteration:526;Percent complete:13.2%; Average loss:722.0839\n",
            "Iteration:527;Percent complete:13.2%; Average loss:1586.1850\n",
            "Iteration:528;Percent complete:13.2%; Average loss:1519.8894\n",
            "Iteration:529;Percent complete:13.2%; Average loss:1512.7658\n",
            "Iteration:530;Percent complete:13.2%; Average loss:1403.7852\n",
            "Iteration:531;Percent complete:13.3%; Average loss:1546.6760\n",
            "Iteration:532;Percent complete:13.3%; Average loss:1470.1956\n",
            "Iteration:533;Percent complete:13.3%; Average loss:736.3220\n",
            "Iteration:534;Percent complete:13.4%; Average loss:1515.5432\n",
            "Iteration:535;Percent complete:13.4%; Average loss:763.9428\n",
            "Iteration:536;Percent complete:13.4%; Average loss:1542.1274\n",
            "Iteration:537;Percent complete:13.4%; Average loss:1377.4561\n",
            "Iteration:538;Percent complete:13.5%; Average loss:1449.1445\n",
            "Iteration:539;Percent complete:13.5%; Average loss:862.0503\n",
            "Iteration:540;Percent complete:13.5%; Average loss:1619.2225\n",
            "Iteration:541;Percent complete:13.5%; Average loss:1563.2119\n",
            "Iteration:542;Percent complete:13.6%; Average loss:1230.2100\n",
            "Iteration:543;Percent complete:13.6%; Average loss:709.7465\n",
            "Iteration:544;Percent complete:13.6%; Average loss:766.5167\n",
            "Iteration:545;Percent complete:13.6%; Average loss:1342.5909\n",
            "Iteration:546;Percent complete:13.7%; Average loss:1394.5323\n",
            "Iteration:547;Percent complete:13.7%; Average loss:1424.1207\n",
            "Iteration:548;Percent complete:13.7%; Average loss:1687.0333\n",
            "Iteration:549;Percent complete:13.7%; Average loss:1631.8097\n",
            "Iteration:550;Percent complete:13.8%; Average loss:1432.7822\n",
            "Iteration:551;Percent complete:13.8%; Average loss:1529.5568\n",
            "Iteration:552;Percent complete:13.8%; Average loss:1372.6501\n",
            "Iteration:553;Percent complete:13.8%; Average loss:1366.6500\n",
            "Iteration:554;Percent complete:13.9%; Average loss:1329.3620\n",
            "Iteration:555;Percent complete:13.9%; Average loss:563.4219\n",
            "Iteration:556;Percent complete:13.9%; Average loss:1636.3753\n",
            "Iteration:557;Percent complete:13.9%; Average loss:783.7479\n",
            "Iteration:558;Percent complete:14.0%; Average loss:821.4933\n",
            "Iteration:559;Percent complete:14.0%; Average loss:1262.6025\n",
            "Iteration:560;Percent complete:14.0%; Average loss:750.5225\n",
            "Iteration:561;Percent complete:14.0%; Average loss:761.1379\n",
            "Iteration:562;Percent complete:14.1%; Average loss:1525.4889\n",
            "Iteration:563;Percent complete:14.1%; Average loss:822.3749\n",
            "Iteration:564;Percent complete:14.1%; Average loss:1605.1123\n",
            "Iteration:565;Percent complete:14.1%; Average loss:1572.0682\n",
            "Iteration:566;Percent complete:14.1%; Average loss:684.9443\n",
            "Iteration:567;Percent complete:14.2%; Average loss:1395.4905\n",
            "Iteration:568;Percent complete:14.2%; Average loss:1553.4080\n",
            "Iteration:569;Percent complete:14.2%; Average loss:1501.4507\n",
            "Iteration:570;Percent complete:14.2%; Average loss:1251.3021\n",
            "Iteration:571;Percent complete:14.3%; Average loss:1450.0669\n",
            "Iteration:572;Percent complete:14.3%; Average loss:1509.1445\n",
            "Iteration:573;Percent complete:14.3%; Average loss:1335.5606\n",
            "Iteration:574;Percent complete:14.3%; Average loss:1455.6427\n",
            "Iteration:575;Percent complete:14.4%; Average loss:741.2200\n",
            "Iteration:576;Percent complete:14.4%; Average loss:1574.6534\n",
            "Iteration:577;Percent complete:14.4%; Average loss:1406.1413\n",
            "Iteration:578;Percent complete:14.4%; Average loss:1621.4287\n",
            "Iteration:579;Percent complete:14.5%; Average loss:1234.8171\n",
            "Iteration:580;Percent complete:14.5%; Average loss:1532.7917\n",
            "Iteration:581;Percent complete:14.5%; Average loss:410.7373\n",
            "Iteration:582;Percent complete:14.5%; Average loss:1310.6780\n",
            "Iteration:583;Percent complete:14.6%; Average loss:1379.0514\n",
            "Iteration:584;Percent complete:14.6%; Average loss:1349.6508\n",
            "Iteration:585;Percent complete:14.6%; Average loss:1720.9728\n",
            "Iteration:586;Percent complete:14.6%; Average loss:510.3219\n",
            "Iteration:587;Percent complete:14.7%; Average loss:1248.4559\n",
            "Iteration:588;Percent complete:14.7%; Average loss:1228.8285\n",
            "Iteration:589;Percent complete:14.7%; Average loss:1317.0111\n",
            "Iteration:590;Percent complete:14.8%; Average loss:1411.0260\n",
            "Iteration:591;Percent complete:14.8%; Average loss:1508.3282\n",
            "Iteration:592;Percent complete:14.8%; Average loss:1234.9385\n",
            "Iteration:593;Percent complete:14.8%; Average loss:506.9030\n",
            "Iteration:594;Percent complete:14.8%; Average loss:736.5731\n",
            "Iteration:595;Percent complete:14.9%; Average loss:1502.9150\n",
            "Iteration:596;Percent complete:14.9%; Average loss:1435.5059\n",
            "Iteration:597;Percent complete:14.9%; Average loss:1593.1970\n",
            "Iteration:598;Percent complete:14.9%; Average loss:1447.5477\n",
            "Iteration:599;Percent complete:15.0%; Average loss:799.5025\n",
            "Iteration:600;Percent complete:15.0%; Average loss:1417.7864\n",
            "Iteration:601;Percent complete:15.0%; Average loss:741.4953\n",
            "Iteration:602;Percent complete:15.0%; Average loss:1439.9833\n",
            "Iteration:603;Percent complete:15.1%; Average loss:1424.1202\n",
            "Iteration:604;Percent complete:15.1%; Average loss:1623.5285\n",
            "Iteration:605;Percent complete:15.1%; Average loss:1590.3960\n",
            "Iteration:606;Percent complete:15.2%; Average loss:465.3394\n",
            "Iteration:607;Percent complete:15.2%; Average loss:1406.5204\n",
            "Iteration:608;Percent complete:15.2%; Average loss:1235.0348\n",
            "Iteration:609;Percent complete:15.2%; Average loss:1347.0182\n",
            "Iteration:610;Percent complete:15.2%; Average loss:1536.7513\n",
            "Iteration:611;Percent complete:15.3%; Average loss:1625.7098\n",
            "Iteration:612;Percent complete:15.3%; Average loss:665.7867\n",
            "Iteration:613;Percent complete:15.3%; Average loss:1424.4779\n",
            "Iteration:614;Percent complete:15.3%; Average loss:786.8989\n",
            "Iteration:615;Percent complete:15.4%; Average loss:1390.0736\n",
            "Iteration:616;Percent complete:15.4%; Average loss:739.2438\n",
            "Iteration:617;Percent complete:15.4%; Average loss:509.7142\n",
            "Iteration:618;Percent complete:15.4%; Average loss:1328.8798\n",
            "Iteration:619;Percent complete:15.5%; Average loss:481.7582\n",
            "Iteration:620;Percent complete:15.5%; Average loss:1597.7484\n",
            "Iteration:621;Percent complete:15.5%; Average loss:1674.7047\n",
            "Iteration:622;Percent complete:15.6%; Average loss:735.8452\n",
            "Iteration:623;Percent complete:15.6%; Average loss:374.2024\n",
            "Iteration:624;Percent complete:15.6%; Average loss:1395.4283\n",
            "Iteration:625;Percent complete:15.6%; Average loss:1428.0620\n",
            "Iteration:626;Percent complete:15.7%; Average loss:1545.2206\n",
            "Iteration:627;Percent complete:15.7%; Average loss:1639.2400\n",
            "Iteration:628;Percent complete:15.7%; Average loss:1399.2168\n",
            "Iteration:629;Percent complete:15.7%; Average loss:710.6642\n",
            "Iteration:630;Percent complete:15.8%; Average loss:1198.9633\n",
            "Iteration:631;Percent complete:15.8%; Average loss:703.0531\n",
            "Iteration:632;Percent complete:15.8%; Average loss:1354.0090\n",
            "Iteration:633;Percent complete:15.8%; Average loss:1434.6681\n",
            "Iteration:634;Percent complete:15.8%; Average loss:1361.3447\n",
            "Iteration:635;Percent complete:15.9%; Average loss:1550.6319\n",
            "Iteration:636;Percent complete:15.9%; Average loss:1397.6915\n",
            "Iteration:637;Percent complete:15.9%; Average loss:1323.1395\n",
            "Iteration:638;Percent complete:16.0%; Average loss:1337.5019\n",
            "Iteration:639;Percent complete:16.0%; Average loss:1260.4404\n",
            "Iteration:640;Percent complete:16.0%; Average loss:674.6884\n",
            "Iteration:641;Percent complete:16.0%; Average loss:780.1842\n",
            "Iteration:642;Percent complete:16.1%; Average loss:1530.2425\n",
            "Iteration:643;Percent complete:16.1%; Average loss:1444.1437\n",
            "Iteration:644;Percent complete:16.1%; Average loss:1433.0718\n",
            "Iteration:645;Percent complete:16.1%; Average loss:1406.7580\n",
            "Iteration:646;Percent complete:16.2%; Average loss:1274.0946\n",
            "Iteration:647;Percent complete:16.2%; Average loss:628.9726\n",
            "Iteration:648;Percent complete:16.2%; Average loss:655.7457\n",
            "Iteration:649;Percent complete:16.2%; Average loss:1301.0830\n",
            "Iteration:650;Percent complete:16.2%; Average loss:698.5476\n",
            "Iteration:651;Percent complete:16.3%; Average loss:1343.4361\n",
            "Iteration:652;Percent complete:16.3%; Average loss:1567.3400\n",
            "Iteration:653;Percent complete:16.3%; Average loss:1444.0377\n",
            "Iteration:654;Percent complete:16.4%; Average loss:570.3793\n",
            "Iteration:655;Percent complete:16.4%; Average loss:1565.8592\n",
            "Iteration:656;Percent complete:16.4%; Average loss:785.6013\n",
            "Iteration:657;Percent complete:16.4%; Average loss:1330.2313\n",
            "Iteration:658;Percent complete:16.4%; Average loss:1620.0772\n",
            "Iteration:659;Percent complete:16.5%; Average loss:1562.0866\n",
            "Iteration:660;Percent complete:16.5%; Average loss:1397.1677\n",
            "Iteration:661;Percent complete:16.5%; Average loss:1528.7849\n",
            "Iteration:662;Percent complete:16.6%; Average loss:1345.8016\n",
            "Iteration:663;Percent complete:16.6%; Average loss:1464.6208\n",
            "Iteration:664;Percent complete:16.6%; Average loss:1521.5218\n",
            "Iteration:665;Percent complete:16.6%; Average loss:1659.1846\n",
            "Iteration:666;Percent complete:16.7%; Average loss:1435.1160\n",
            "Iteration:667;Percent complete:16.7%; Average loss:1450.3980\n",
            "Iteration:668;Percent complete:16.7%; Average loss:680.8583\n",
            "Iteration:669;Percent complete:16.7%; Average loss:1412.5447\n",
            "Iteration:670;Percent complete:16.8%; Average loss:1352.2661\n",
            "Iteration:671;Percent complete:16.8%; Average loss:767.4365\n",
            "Iteration:672;Percent complete:16.8%; Average loss:1544.6402\n",
            "Iteration:673;Percent complete:16.8%; Average loss:1620.5977\n",
            "Iteration:674;Percent complete:16.9%; Average loss:1538.4043\n",
            "Iteration:675;Percent complete:16.9%; Average loss:808.0498\n",
            "Iteration:676;Percent complete:16.9%; Average loss:1468.5933\n",
            "Iteration:677;Percent complete:16.9%; Average loss:719.5315\n",
            "Iteration:678;Percent complete:17.0%; Average loss:1479.3418\n",
            "Iteration:679;Percent complete:17.0%; Average loss:1223.8512\n",
            "Iteration:680;Percent complete:17.0%; Average loss:1505.9969\n",
            "Iteration:681;Percent complete:17.0%; Average loss:1517.4429\n",
            "Iteration:682;Percent complete:17.1%; Average loss:1339.3919\n",
            "Iteration:683;Percent complete:17.1%; Average loss:1391.9908\n",
            "Iteration:684;Percent complete:17.1%; Average loss:1513.1210\n",
            "Iteration:685;Percent complete:17.1%; Average loss:683.3068\n",
            "Iteration:686;Percent complete:17.2%; Average loss:867.3131\n",
            "Iteration:687;Percent complete:17.2%; Average loss:1434.7902\n",
            "Iteration:688;Percent complete:17.2%; Average loss:1548.4667\n",
            "Iteration:689;Percent complete:17.2%; Average loss:1345.3030\n",
            "Iteration:690;Percent complete:17.2%; Average loss:1309.5398\n",
            "Iteration:691;Percent complete:17.3%; Average loss:739.9169\n",
            "Iteration:692;Percent complete:17.3%; Average loss:1465.7888\n",
            "Iteration:693;Percent complete:17.3%; Average loss:1364.2081\n",
            "Iteration:694;Percent complete:17.3%; Average loss:1200.2526\n",
            "Iteration:695;Percent complete:17.4%; Average loss:1357.8427\n",
            "Iteration:696;Percent complete:17.4%; Average loss:1619.7309\n",
            "Iteration:697;Percent complete:17.4%; Average loss:1260.0454\n",
            "Iteration:698;Percent complete:17.4%; Average loss:1434.7700\n",
            "Iteration:699;Percent complete:17.5%; Average loss:1402.9858\n",
            "Iteration:700;Percent complete:17.5%; Average loss:1511.8316\n",
            "Iteration:701;Percent complete:17.5%; Average loss:1363.9468\n",
            "Iteration:702;Percent complete:17.5%; Average loss:1433.9117\n",
            "Iteration:703;Percent complete:17.6%; Average loss:1607.6402\n",
            "Iteration:704;Percent complete:17.6%; Average loss:332.8414\n",
            "Iteration:705;Percent complete:17.6%; Average loss:1552.4120\n",
            "Iteration:706;Percent complete:17.6%; Average loss:1515.4083\n",
            "Iteration:707;Percent complete:17.7%; Average loss:1227.8731\n",
            "Iteration:708;Percent complete:17.7%; Average loss:1487.8589\n",
            "Iteration:709;Percent complete:17.7%; Average loss:1407.1433\n",
            "Iteration:710;Percent complete:17.8%; Average loss:1508.5441\n",
            "Iteration:711;Percent complete:17.8%; Average loss:1518.9341\n",
            "Iteration:712;Percent complete:17.8%; Average loss:1423.2654\n",
            "Iteration:713;Percent complete:17.8%; Average loss:1502.8141\n",
            "Iteration:714;Percent complete:17.8%; Average loss:1543.7382\n",
            "Iteration:715;Percent complete:17.9%; Average loss:1323.3104\n",
            "Iteration:716;Percent complete:17.9%; Average loss:1401.4610\n",
            "Iteration:717;Percent complete:17.9%; Average loss:1600.5439\n",
            "Iteration:718;Percent complete:17.9%; Average loss:1449.3272\n",
            "Iteration:719;Percent complete:18.0%; Average loss:1466.8929\n",
            "Iteration:720;Percent complete:18.0%; Average loss:1558.2434\n",
            "Iteration:721;Percent complete:18.0%; Average loss:1451.5218\n",
            "Iteration:722;Percent complete:18.1%; Average loss:1487.2772\n",
            "Iteration:723;Percent complete:18.1%; Average loss:881.0156\n",
            "Iteration:724;Percent complete:18.1%; Average loss:1503.3876\n",
            "Iteration:725;Percent complete:18.1%; Average loss:1398.3933\n",
            "Iteration:726;Percent complete:18.1%; Average loss:1338.2621\n",
            "Iteration:727;Percent complete:18.2%; Average loss:1439.4747\n",
            "Iteration:728;Percent complete:18.2%; Average loss:1452.6711\n",
            "Iteration:729;Percent complete:18.2%; Average loss:1462.8966\n",
            "Iteration:730;Percent complete:18.2%; Average loss:693.2497\n",
            "Iteration:731;Percent complete:18.3%; Average loss:1473.9617\n",
            "Iteration:732;Percent complete:18.3%; Average loss:1315.1711\n",
            "Iteration:733;Percent complete:18.3%; Average loss:1381.2158\n",
            "Iteration:734;Percent complete:18.4%; Average loss:1245.5979\n",
            "Iteration:735;Percent complete:18.4%; Average loss:766.1530\n",
            "Iteration:736;Percent complete:18.4%; Average loss:1527.8023\n",
            "Iteration:737;Percent complete:18.4%; Average loss:1194.6617\n",
            "Iteration:738;Percent complete:18.4%; Average loss:1500.5458\n",
            "Iteration:739;Percent complete:18.5%; Average loss:709.5116\n",
            "Iteration:740;Percent complete:18.5%; Average loss:1318.4767\n",
            "Iteration:741;Percent complete:18.5%; Average loss:913.0947\n",
            "Iteration:742;Percent complete:18.6%; Average loss:1134.4369\n",
            "Iteration:743;Percent complete:18.6%; Average loss:1265.6619\n",
            "Iteration:744;Percent complete:18.6%; Average loss:1402.3830\n",
            "Iteration:745;Percent complete:18.6%; Average loss:1509.3423\n",
            "Iteration:746;Percent complete:18.6%; Average loss:1403.9901\n",
            "Iteration:747;Percent complete:18.7%; Average loss:1546.6091\n",
            "Iteration:748;Percent complete:18.7%; Average loss:674.5859\n",
            "Iteration:749;Percent complete:18.7%; Average loss:1318.8679\n",
            "Iteration:750;Percent complete:18.8%; Average loss:1646.3021\n",
            "Iteration:751;Percent complete:18.8%; Average loss:1561.4692\n",
            "Iteration:752;Percent complete:18.8%; Average loss:1665.8394\n",
            "Iteration:753;Percent complete:18.8%; Average loss:1498.5277\n",
            "Iteration:754;Percent complete:18.9%; Average loss:1282.0718\n",
            "Iteration:755;Percent complete:18.9%; Average loss:1253.1080\n",
            "Iteration:756;Percent complete:18.9%; Average loss:1432.7993\n",
            "Iteration:757;Percent complete:18.9%; Average loss:708.7281\n",
            "Iteration:758;Percent complete:18.9%; Average loss:1444.8394\n",
            "Iteration:759;Percent complete:19.0%; Average loss:1470.4173\n",
            "Iteration:760;Percent complete:19.0%; Average loss:1478.9119\n",
            "Iteration:761;Percent complete:19.0%; Average loss:1286.1318\n",
            "Iteration:762;Percent complete:19.1%; Average loss:1251.9255\n",
            "Iteration:763;Percent complete:19.1%; Average loss:1599.5172\n",
            "Iteration:764;Percent complete:19.1%; Average loss:1491.3105\n",
            "Iteration:765;Percent complete:19.1%; Average loss:1871.3019\n",
            "Iteration:766;Percent complete:19.1%; Average loss:1553.5839\n",
            "Iteration:767;Percent complete:19.2%; Average loss:1391.8134\n",
            "Iteration:768;Percent complete:19.2%; Average loss:685.2412\n",
            "Iteration:769;Percent complete:19.2%; Average loss:1283.5503\n",
            "Iteration:770;Percent complete:19.2%; Average loss:1460.1124\n",
            "Iteration:771;Percent complete:19.3%; Average loss:1385.1373\n",
            "Iteration:772;Percent complete:19.3%; Average loss:703.2054\n",
            "Iteration:773;Percent complete:19.3%; Average loss:1527.1781\n",
            "Iteration:774;Percent complete:19.4%; Average loss:1446.5604\n",
            "Iteration:775;Percent complete:19.4%; Average loss:1530.3523\n",
            "Iteration:776;Percent complete:19.4%; Average loss:1303.6410\n",
            "Iteration:777;Percent complete:19.4%; Average loss:1459.9948\n",
            "Iteration:778;Percent complete:19.4%; Average loss:1507.8221\n",
            "Iteration:779;Percent complete:19.5%; Average loss:748.9090\n",
            "Iteration:780;Percent complete:19.5%; Average loss:1410.0955\n",
            "Iteration:781;Percent complete:19.5%; Average loss:1181.7303\n",
            "Iteration:782;Percent complete:19.6%; Average loss:659.2110\n",
            "Iteration:783;Percent complete:19.6%; Average loss:1352.2854\n",
            "Iteration:784;Percent complete:19.6%; Average loss:1703.6540\n",
            "Iteration:785;Percent complete:19.6%; Average loss:746.9764\n",
            "Iteration:786;Percent complete:19.7%; Average loss:1462.7319\n",
            "Iteration:787;Percent complete:19.7%; Average loss:779.8799\n",
            "Iteration:788;Percent complete:19.7%; Average loss:1338.3889\n",
            "Iteration:789;Percent complete:19.7%; Average loss:1402.6569\n",
            "Iteration:790;Percent complete:19.8%; Average loss:1376.1827\n",
            "Iteration:791;Percent complete:19.8%; Average loss:1481.0271\n",
            "Iteration:792;Percent complete:19.8%; Average loss:1326.0450\n",
            "Iteration:793;Percent complete:19.8%; Average loss:1462.1347\n",
            "Iteration:794;Percent complete:19.9%; Average loss:719.9799\n",
            "Iteration:795;Percent complete:19.9%; Average loss:1588.8674\n",
            "Iteration:796;Percent complete:19.9%; Average loss:810.6824\n",
            "Iteration:797;Percent complete:19.9%; Average loss:748.3243\n",
            "Iteration:798;Percent complete:20.0%; Average loss:1487.1583\n",
            "Iteration:799;Percent complete:20.0%; Average loss:1417.8961\n",
            "Iteration:800;Percent complete:20.0%; Average loss:1583.8121\n",
            "Iteration:801;Percent complete:20.0%; Average loss:1342.6850\n",
            "Iteration:802;Percent complete:20.1%; Average loss:1455.8805\n",
            "Iteration:803;Percent complete:20.1%; Average loss:1199.5791\n",
            "Iteration:804;Percent complete:20.1%; Average loss:1189.6024\n",
            "Iteration:805;Percent complete:20.1%; Average loss:1423.7886\n",
            "Iteration:806;Percent complete:20.2%; Average loss:1284.9760\n",
            "Iteration:807;Percent complete:20.2%; Average loss:503.0254\n",
            "Iteration:808;Percent complete:20.2%; Average loss:793.2116\n",
            "Iteration:809;Percent complete:20.2%; Average loss:1503.2723\n",
            "Iteration:810;Percent complete:20.2%; Average loss:1443.4503\n",
            "Iteration:811;Percent complete:20.3%; Average loss:1424.9201\n",
            "Iteration:812;Percent complete:20.3%; Average loss:1496.6362\n",
            "Iteration:813;Percent complete:20.3%; Average loss:1344.3588\n",
            "Iteration:814;Percent complete:20.3%; Average loss:1443.2634\n",
            "Iteration:815;Percent complete:20.4%; Average loss:1421.0045\n",
            "Iteration:816;Percent complete:20.4%; Average loss:721.7954\n",
            "Iteration:817;Percent complete:20.4%; Average loss:1198.7049\n",
            "Iteration:818;Percent complete:20.4%; Average loss:1501.6038\n",
            "Iteration:819;Percent complete:20.5%; Average loss:1545.1155\n",
            "Iteration:820;Percent complete:20.5%; Average loss:667.5391\n",
            "Iteration:821;Percent complete:20.5%; Average loss:1455.0515\n",
            "Iteration:822;Percent complete:20.5%; Average loss:1338.7758\n",
            "Iteration:823;Percent complete:20.6%; Average loss:1174.6372\n",
            "Iteration:824;Percent complete:20.6%; Average loss:461.9825\n",
            "Iteration:825;Percent complete:20.6%; Average loss:1615.7643\n",
            "Iteration:826;Percent complete:20.6%; Average loss:765.8970\n",
            "Iteration:827;Percent complete:20.7%; Average loss:798.3687\n",
            "Iteration:828;Percent complete:20.7%; Average loss:1472.8960\n",
            "Iteration:829;Percent complete:20.7%; Average loss:1407.1494\n",
            "Iteration:830;Percent complete:20.8%; Average loss:1634.9076\n",
            "Iteration:831;Percent complete:20.8%; Average loss:1447.2441\n",
            "Iteration:832;Percent complete:20.8%; Average loss:1496.0185\n",
            "Iteration:833;Percent complete:20.8%; Average loss:1411.1165\n",
            "Iteration:834;Percent complete:20.8%; Average loss:1292.3685\n",
            "Iteration:835;Percent complete:20.9%; Average loss:1633.8541\n",
            "Iteration:836;Percent complete:20.9%; Average loss:1393.4644\n",
            "Iteration:837;Percent complete:20.9%; Average loss:1320.7769\n",
            "Iteration:838;Percent complete:20.9%; Average loss:1322.4754\n",
            "Iteration:839;Percent complete:21.0%; Average loss:1501.7081\n",
            "Iteration:840;Percent complete:21.0%; Average loss:701.6440\n",
            "Iteration:841;Percent complete:21.0%; Average loss:1573.8823\n",
            "Iteration:842;Percent complete:21.1%; Average loss:1312.5059\n",
            "Iteration:843;Percent complete:21.1%; Average loss:803.4577\n",
            "Iteration:844;Percent complete:21.1%; Average loss:741.5725\n",
            "Iteration:845;Percent complete:21.1%; Average loss:1543.3139\n",
            "Iteration:846;Percent complete:21.1%; Average loss:633.8810\n",
            "Iteration:847;Percent complete:21.2%; Average loss:1317.9222\n",
            "Iteration:848;Percent complete:21.2%; Average loss:1681.7714\n",
            "Iteration:849;Percent complete:21.2%; Average loss:1613.6875\n",
            "Iteration:850;Percent complete:21.2%; Average loss:1400.5142\n",
            "Iteration:851;Percent complete:21.3%; Average loss:1468.9379\n",
            "Iteration:852;Percent complete:21.3%; Average loss:1406.8241\n",
            "Iteration:853;Percent complete:21.3%; Average loss:1406.9015\n",
            "Iteration:854;Percent complete:21.3%; Average loss:1468.8203\n",
            "Iteration:855;Percent complete:21.4%; Average loss:1566.3048\n",
            "Iteration:856;Percent complete:21.4%; Average loss:1441.4696\n",
            "Iteration:857;Percent complete:21.4%; Average loss:1604.3354\n",
            "Iteration:858;Percent complete:21.4%; Average loss:1451.1137\n",
            "Iteration:859;Percent complete:21.5%; Average loss:1253.5006\n",
            "Iteration:860;Percent complete:21.5%; Average loss:1354.4029\n",
            "Iteration:861;Percent complete:21.5%; Average loss:1275.3454\n",
            "Iteration:862;Percent complete:21.6%; Average loss:1451.6672\n",
            "Iteration:863;Percent complete:21.6%; Average loss:1382.6842\n",
            "Iteration:864;Percent complete:21.6%; Average loss:755.8288\n",
            "Iteration:865;Percent complete:21.6%; Average loss:1365.4419\n",
            "Iteration:866;Percent complete:21.6%; Average loss:1404.5593\n",
            "Iteration:867;Percent complete:21.7%; Average loss:1388.0324\n",
            "Iteration:868;Percent complete:21.7%; Average loss:1454.6455\n",
            "Iteration:869;Percent complete:21.7%; Average loss:1490.5562\n",
            "Iteration:870;Percent complete:21.8%; Average loss:1378.4568\n",
            "Iteration:871;Percent complete:21.8%; Average loss:1371.9346\n",
            "Iteration:872;Percent complete:21.8%; Average loss:1366.7919\n",
            "Iteration:873;Percent complete:21.8%; Average loss:1391.1776\n",
            "Iteration:874;Percent complete:21.9%; Average loss:1595.8104\n",
            "Iteration:875;Percent complete:21.9%; Average loss:779.3291\n",
            "Iteration:876;Percent complete:21.9%; Average loss:1451.6180\n",
            "Iteration:877;Percent complete:21.9%; Average loss:1440.9208\n",
            "Iteration:878;Percent complete:21.9%; Average loss:1716.9410\n",
            "Iteration:879;Percent complete:22.0%; Average loss:1412.5911\n",
            "Iteration:880;Percent complete:22.0%; Average loss:1364.2546\n",
            "Iteration:881;Percent complete:22.0%; Average loss:1514.9789\n",
            "Iteration:882;Percent complete:22.1%; Average loss:1393.4087\n",
            "Iteration:883;Percent complete:22.1%; Average loss:1534.0783\n",
            "Iteration:884;Percent complete:22.1%; Average loss:1390.0666\n",
            "Iteration:885;Percent complete:22.1%; Average loss:709.5303\n",
            "Iteration:886;Percent complete:22.1%; Average loss:1529.4374\n",
            "Iteration:887;Percent complete:22.2%; Average loss:704.2761\n",
            "Iteration:888;Percent complete:22.2%; Average loss:758.4227\n",
            "Iteration:889;Percent complete:22.2%; Average loss:1302.3954\n",
            "Iteration:890;Percent complete:22.2%; Average loss:1439.8280\n",
            "Iteration:891;Percent complete:22.3%; Average loss:1513.5311\n",
            "Iteration:892;Percent complete:22.3%; Average loss:1474.1324\n",
            "Iteration:893;Percent complete:22.3%; Average loss:1416.1421\n",
            "Iteration:894;Percent complete:22.4%; Average loss:1434.0428\n",
            "Iteration:895;Percent complete:22.4%; Average loss:1401.0742\n",
            "Iteration:896;Percent complete:22.4%; Average loss:1624.9756\n",
            "Iteration:897;Percent complete:22.4%; Average loss:1315.1100\n",
            "Iteration:898;Percent complete:22.4%; Average loss:1540.0244\n",
            "Iteration:899;Percent complete:22.5%; Average loss:621.0082\n",
            "Iteration:900;Percent complete:22.5%; Average loss:536.2284\n",
            "Iteration:901;Percent complete:22.5%; Average loss:436.2389\n",
            "Iteration:902;Percent complete:22.6%; Average loss:698.5896\n",
            "Iteration:903;Percent complete:22.6%; Average loss:1584.2080\n",
            "Iteration:904;Percent complete:22.6%; Average loss:1469.1511\n",
            "Iteration:905;Percent complete:22.6%; Average loss:670.1596\n",
            "Iteration:906;Percent complete:22.7%; Average loss:1464.9164\n",
            "Iteration:907;Percent complete:22.7%; Average loss:1248.5647\n",
            "Iteration:908;Percent complete:22.7%; Average loss:1592.0095\n",
            "Iteration:909;Percent complete:22.7%; Average loss:1364.8010\n",
            "Iteration:910;Percent complete:22.8%; Average loss:1345.0457\n",
            "Iteration:911;Percent complete:22.8%; Average loss:1329.3391\n",
            "Iteration:912;Percent complete:22.8%; Average loss:1356.0873\n",
            "Iteration:913;Percent complete:22.8%; Average loss:1607.1693\n",
            "Iteration:914;Percent complete:22.9%; Average loss:1360.8811\n",
            "Iteration:915;Percent complete:22.9%; Average loss:1368.0800\n",
            "Iteration:916;Percent complete:22.9%; Average loss:460.7204\n",
            "Iteration:917;Percent complete:22.9%; Average loss:702.4254\n",
            "Iteration:918;Percent complete:22.9%; Average loss:653.4050\n",
            "Iteration:919;Percent complete:23.0%; Average loss:1613.8682\n",
            "Iteration:920;Percent complete:23.0%; Average loss:471.6025\n",
            "Iteration:921;Percent complete:23.0%; Average loss:706.4122\n",
            "Iteration:922;Percent complete:23.1%; Average loss:1205.0478\n",
            "Iteration:923;Percent complete:23.1%; Average loss:701.1444\n",
            "Iteration:924;Percent complete:23.1%; Average loss:1433.8842\n",
            "Iteration:925;Percent complete:23.1%; Average loss:654.8737\n",
            "Iteration:926;Percent complete:23.2%; Average loss:1266.7268\n",
            "Iteration:927;Percent complete:23.2%; Average loss:763.1661\n",
            "Iteration:928;Percent complete:23.2%; Average loss:1328.6842\n",
            "Iteration:929;Percent complete:23.2%; Average loss:722.1640\n",
            "Iteration:930;Percent complete:23.2%; Average loss:1594.3759\n",
            "Iteration:931;Percent complete:23.3%; Average loss:695.4476\n",
            "Iteration:932;Percent complete:23.3%; Average loss:1325.3882\n",
            "Iteration:933;Percent complete:23.3%; Average loss:1582.4375\n",
            "Iteration:934;Percent complete:23.4%; Average loss:684.6610\n",
            "Iteration:935;Percent complete:23.4%; Average loss:1318.0688\n",
            "Iteration:936;Percent complete:23.4%; Average loss:1427.2108\n",
            "Iteration:937;Percent complete:23.4%; Average loss:1236.4874\n",
            "Iteration:938;Percent complete:23.4%; Average loss:484.8070\n",
            "Iteration:939;Percent complete:23.5%; Average loss:690.8689\n",
            "Iteration:940;Percent complete:23.5%; Average loss:382.0995\n",
            "Iteration:941;Percent complete:23.5%; Average loss:1451.3351\n",
            "Iteration:942;Percent complete:23.5%; Average loss:1410.7147\n",
            "Iteration:943;Percent complete:23.6%; Average loss:690.0258\n",
            "Iteration:944;Percent complete:23.6%; Average loss:1525.0053\n",
            "Iteration:945;Percent complete:23.6%; Average loss:793.6047\n",
            "Iteration:946;Percent complete:23.6%; Average loss:1491.0212\n",
            "Iteration:947;Percent complete:23.7%; Average loss:1211.4738\n",
            "Iteration:948;Percent complete:23.7%; Average loss:1456.3334\n",
            "Iteration:949;Percent complete:23.7%; Average loss:1257.0130\n",
            "Iteration:950;Percent complete:23.8%; Average loss:1357.2649\n",
            "Iteration:951;Percent complete:23.8%; Average loss:1391.6275\n",
            "Iteration:952;Percent complete:23.8%; Average loss:1536.7758\n",
            "Iteration:953;Percent complete:23.8%; Average loss:786.6448\n",
            "Iteration:954;Percent complete:23.8%; Average loss:1265.2698\n",
            "Iteration:955;Percent complete:23.9%; Average loss:1414.8600\n",
            "Iteration:956;Percent complete:23.9%; Average loss:1552.0387\n",
            "Iteration:957;Percent complete:23.9%; Average loss:742.5615\n",
            "Iteration:958;Percent complete:23.9%; Average loss:1411.5112\n",
            "Iteration:959;Percent complete:24.0%; Average loss:1380.8299\n",
            "Iteration:960;Percent complete:24.0%; Average loss:1592.1751\n",
            "Iteration:961;Percent complete:24.0%; Average loss:1243.6385\n",
            "Iteration:962;Percent complete:24.1%; Average loss:1347.4073\n",
            "Iteration:963;Percent complete:24.1%; Average loss:1358.2319\n",
            "Iteration:964;Percent complete:24.1%; Average loss:828.5101\n",
            "Iteration:965;Percent complete:24.1%; Average loss:1288.7966\n",
            "Iteration:966;Percent complete:24.1%; Average loss:1382.4553\n",
            "Iteration:967;Percent complete:24.2%; Average loss:1334.9673\n",
            "Iteration:968;Percent complete:24.2%; Average loss:1306.8141\n",
            "Iteration:969;Percent complete:24.2%; Average loss:1330.7848\n",
            "Iteration:970;Percent complete:24.2%; Average loss:1631.5787\n",
            "Iteration:971;Percent complete:24.3%; Average loss:1330.5299\n",
            "Iteration:972;Percent complete:24.3%; Average loss:1614.0769\n",
            "Iteration:973;Percent complete:24.3%; Average loss:893.4848\n",
            "Iteration:974;Percent complete:24.3%; Average loss:752.5459\n",
            "Iteration:975;Percent complete:24.4%; Average loss:1477.9238\n",
            "Iteration:976;Percent complete:24.4%; Average loss:1414.6469\n",
            "Iteration:977;Percent complete:24.4%; Average loss:847.5820\n",
            "Iteration:978;Percent complete:24.4%; Average loss:496.5820\n",
            "Iteration:979;Percent complete:24.5%; Average loss:1261.7205\n",
            "Iteration:980;Percent complete:24.5%; Average loss:731.6947\n",
            "Iteration:981;Percent complete:24.5%; Average loss:1505.9732\n",
            "Iteration:982;Percent complete:24.6%; Average loss:1381.4962\n",
            "Iteration:983;Percent complete:24.6%; Average loss:668.9094\n",
            "Iteration:984;Percent complete:24.6%; Average loss:1451.2577\n",
            "Iteration:985;Percent complete:24.6%; Average loss:772.4978\n",
            "Iteration:986;Percent complete:24.6%; Average loss:1407.4039\n",
            "Iteration:987;Percent complete:24.7%; Average loss:1323.6636\n",
            "Iteration:988;Percent complete:24.7%; Average loss:1598.3801\n",
            "Iteration:989;Percent complete:24.7%; Average loss:814.2929\n",
            "Iteration:990;Percent complete:24.8%; Average loss:1490.6428\n",
            "Iteration:991;Percent complete:24.8%; Average loss:1307.4277\n",
            "Iteration:992;Percent complete:24.8%; Average loss:1412.4028\n",
            "Iteration:993;Percent complete:24.8%; Average loss:1461.6114\n",
            "Iteration:994;Percent complete:24.9%; Average loss:1510.9531\n",
            "Iteration:995;Percent complete:24.9%; Average loss:1588.6913\n",
            "Iteration:996;Percent complete:24.9%; Average loss:1359.4815\n",
            "Iteration:997;Percent complete:24.9%; Average loss:1422.4632\n",
            "Iteration:998;Percent complete:24.9%; Average loss:1438.1879\n",
            "Iteration:999;Percent complete:25.0%; Average loss:1323.0016\n",
            "Iteration:1000;Percent complete:25.0%; Average loss:731.1883\n",
            "Iteration:1001;Percent complete:25.0%; Average loss:1358.8579\n",
            "Iteration:1002;Percent complete:25.1%; Average loss:1509.8567\n",
            "Iteration:1003;Percent complete:25.1%; Average loss:1419.6871\n",
            "Iteration:1004;Percent complete:25.1%; Average loss:1397.4280\n",
            "Iteration:1005;Percent complete:25.1%; Average loss:1259.8301\n",
            "Iteration:1006;Percent complete:25.1%; Average loss:1328.3333\n",
            "Iteration:1007;Percent complete:25.2%; Average loss:1294.2395\n",
            "Iteration:1008;Percent complete:25.2%; Average loss:1710.7494\n",
            "Iteration:1009;Percent complete:25.2%; Average loss:1405.2785\n",
            "Iteration:1010;Percent complete:25.2%; Average loss:1360.0896\n",
            "Iteration:1011;Percent complete:25.3%; Average loss:1587.0471\n",
            "Iteration:1012;Percent complete:25.3%; Average loss:1355.4793\n",
            "Iteration:1013;Percent complete:25.3%; Average loss:1438.6201\n",
            "Iteration:1014;Percent complete:25.4%; Average loss:678.9335\n",
            "Iteration:1015;Percent complete:25.4%; Average loss:1542.9385\n",
            "Iteration:1016;Percent complete:25.4%; Average loss:1368.7579\n",
            "Iteration:1017;Percent complete:25.4%; Average loss:1296.6337\n",
            "Iteration:1018;Percent complete:25.4%; Average loss:1619.1848\n",
            "Iteration:1019;Percent complete:25.5%; Average loss:785.4356\n",
            "Iteration:1020;Percent complete:25.5%; Average loss:1450.5812\n",
            "Iteration:1021;Percent complete:25.5%; Average loss:765.3912\n",
            "Iteration:1022;Percent complete:25.6%; Average loss:1420.0371\n",
            "Iteration:1023;Percent complete:25.6%; Average loss:1253.0235\n",
            "Iteration:1024;Percent complete:25.6%; Average loss:1336.7463\n",
            "Iteration:1025;Percent complete:25.6%; Average loss:1471.0411\n",
            "Iteration:1026;Percent complete:25.7%; Average loss:1450.1843\n",
            "Iteration:1027;Percent complete:25.7%; Average loss:1411.3706\n",
            "Iteration:1028;Percent complete:25.7%; Average loss:475.3423\n",
            "Iteration:1029;Percent complete:25.7%; Average loss:677.8914\n",
            "Iteration:1030;Percent complete:25.8%; Average loss:1457.7064\n",
            "Iteration:1031;Percent complete:25.8%; Average loss:1535.6344\n",
            "Iteration:1032;Percent complete:25.8%; Average loss:1722.3658\n",
            "Iteration:1033;Percent complete:25.8%; Average loss:1554.1300\n",
            "Iteration:1034;Percent complete:25.9%; Average loss:1562.7391\n",
            "Iteration:1035;Percent complete:25.9%; Average loss:715.7610\n",
            "Iteration:1036;Percent complete:25.9%; Average loss:1289.8452\n",
            "Iteration:1037;Percent complete:25.9%; Average loss:1330.1749\n",
            "Iteration:1038;Percent complete:25.9%; Average loss:1250.2090\n",
            "Iteration:1039;Percent complete:26.0%; Average loss:1555.6419\n",
            "Iteration:1040;Percent complete:26.0%; Average loss:1347.7094\n",
            "Iteration:1041;Percent complete:26.0%; Average loss:699.5478\n",
            "Iteration:1042;Percent complete:26.1%; Average loss:1438.4675\n",
            "Iteration:1043;Percent complete:26.1%; Average loss:712.1709\n",
            "Iteration:1044;Percent complete:26.1%; Average loss:1356.5170\n",
            "Iteration:1045;Percent complete:26.1%; Average loss:1493.8740\n",
            "Iteration:1046;Percent complete:26.2%; Average loss:707.9607\n",
            "Iteration:1047;Percent complete:26.2%; Average loss:1508.3637\n",
            "Iteration:1048;Percent complete:26.2%; Average loss:1376.4335\n",
            "Iteration:1049;Percent complete:26.2%; Average loss:1447.7233\n",
            "Iteration:1050;Percent complete:26.2%; Average loss:1559.7554\n",
            "Iteration:1051;Percent complete:26.3%; Average loss:1445.5236\n",
            "Iteration:1052;Percent complete:26.3%; Average loss:1387.5570\n",
            "Iteration:1053;Percent complete:26.3%; Average loss:633.4681\n",
            "Iteration:1054;Percent complete:26.4%; Average loss:1485.1453\n",
            "Iteration:1055;Percent complete:26.4%; Average loss:1395.1061\n",
            "Iteration:1056;Percent complete:26.4%; Average loss:1462.4437\n",
            "Iteration:1057;Percent complete:26.4%; Average loss:1615.4338\n",
            "Iteration:1058;Percent complete:26.5%; Average loss:1386.2478\n",
            "Iteration:1059;Percent complete:26.5%; Average loss:737.2761\n",
            "Iteration:1060;Percent complete:26.5%; Average loss:1311.4397\n",
            "Iteration:1061;Percent complete:26.5%; Average loss:1318.9775\n",
            "Iteration:1062;Percent complete:26.6%; Average loss:1404.0382\n",
            "Iteration:1063;Percent complete:26.6%; Average loss:1260.0863\n",
            "Iteration:1064;Percent complete:26.6%; Average loss:1506.6844\n",
            "Iteration:1065;Percent complete:26.6%; Average loss:1338.0369\n",
            "Iteration:1066;Percent complete:26.7%; Average loss:1503.2693\n",
            "Iteration:1067;Percent complete:26.7%; Average loss:1368.6051\n",
            "Iteration:1068;Percent complete:26.7%; Average loss:1396.0819\n",
            "Iteration:1069;Percent complete:26.7%; Average loss:1511.7875\n",
            "Iteration:1070;Percent complete:26.8%; Average loss:1225.6236\n",
            "Iteration:1071;Percent complete:26.8%; Average loss:1577.4050\n",
            "Iteration:1072;Percent complete:26.8%; Average loss:539.5849\n",
            "Iteration:1073;Percent complete:26.8%; Average loss:1304.7732\n",
            "Iteration:1074;Percent complete:26.9%; Average loss:1477.2918\n",
            "Iteration:1075;Percent complete:26.9%; Average loss:1257.9896\n",
            "Iteration:1076;Percent complete:26.9%; Average loss:1391.4684\n",
            "Iteration:1077;Percent complete:26.9%; Average loss:1351.2040\n",
            "Iteration:1078;Percent complete:27.0%; Average loss:1193.3645\n",
            "Iteration:1079;Percent complete:27.0%; Average loss:1422.9302\n",
            "Iteration:1080;Percent complete:27.0%; Average loss:1285.3804\n",
            "Iteration:1081;Percent complete:27.0%; Average loss:743.0445\n",
            "Iteration:1082;Percent complete:27.1%; Average loss:1258.4016\n",
            "Iteration:1083;Percent complete:27.1%; Average loss:792.9514\n",
            "Iteration:1084;Percent complete:27.1%; Average loss:1370.5667\n",
            "Iteration:1085;Percent complete:27.1%; Average loss:1248.9403\n",
            "Iteration:1086;Percent complete:27.2%; Average loss:799.1280\n",
            "Iteration:1087;Percent complete:27.2%; Average loss:1405.2730\n",
            "Iteration:1088;Percent complete:27.2%; Average loss:1249.3741\n",
            "Iteration:1089;Percent complete:27.2%; Average loss:1477.7546\n",
            "Iteration:1090;Percent complete:27.3%; Average loss:1227.8242\n",
            "Iteration:1091;Percent complete:27.3%; Average loss:1292.6469\n",
            "Iteration:1092;Percent complete:27.3%; Average loss:1508.9368\n",
            "Iteration:1093;Percent complete:27.3%; Average loss:1392.8880\n",
            "Iteration:1094;Percent complete:27.4%; Average loss:1271.4009\n",
            "Iteration:1095;Percent complete:27.4%; Average loss:1466.9588\n",
            "Iteration:1096;Percent complete:27.4%; Average loss:1519.3197\n",
            "Iteration:1097;Percent complete:27.4%; Average loss:669.0849\n",
            "Iteration:1098;Percent complete:27.5%; Average loss:808.8470\n",
            "Iteration:1099;Percent complete:27.5%; Average loss:1212.5926\n",
            "Iteration:1100;Percent complete:27.5%; Average loss:1708.7710\n",
            "Iteration:1101;Percent complete:27.5%; Average loss:726.1558\n",
            "Iteration:1102;Percent complete:27.6%; Average loss:1571.5640\n",
            "Iteration:1103;Percent complete:27.6%; Average loss:1377.2879\n",
            "Iteration:1104;Percent complete:27.6%; Average loss:1305.6959\n",
            "Iteration:1105;Percent complete:27.6%; Average loss:1283.7207\n",
            "Iteration:1106;Percent complete:27.7%; Average loss:1537.6283\n",
            "Iteration:1107;Percent complete:27.7%; Average loss:710.9019\n",
            "Iteration:1108;Percent complete:27.7%; Average loss:1420.0050\n",
            "Iteration:1109;Percent complete:27.7%; Average loss:802.9484\n",
            "Iteration:1110;Percent complete:27.8%; Average loss:1205.8255\n",
            "Iteration:1111;Percent complete:27.8%; Average loss:1492.5076\n",
            "Iteration:1112;Percent complete:27.8%; Average loss:1581.5239\n",
            "Iteration:1113;Percent complete:27.8%; Average loss:1487.2804\n",
            "Iteration:1114;Percent complete:27.9%; Average loss:1470.2415\n",
            "Iteration:1115;Percent complete:27.9%; Average loss:789.5906\n",
            "Iteration:1116;Percent complete:27.9%; Average loss:1438.9460\n",
            "Iteration:1117;Percent complete:27.9%; Average loss:1281.0044\n",
            "Iteration:1118;Percent complete:28.0%; Average loss:766.5585\n",
            "Iteration:1119;Percent complete:28.0%; Average loss:1340.2142\n",
            "Iteration:1120;Percent complete:28.0%; Average loss:1541.2921\n",
            "Iteration:1121;Percent complete:28.0%; Average loss:697.4106\n",
            "Iteration:1122;Percent complete:28.1%; Average loss:1476.1970\n",
            "Iteration:1123;Percent complete:28.1%; Average loss:1299.7062\n",
            "Iteration:1124;Percent complete:28.1%; Average loss:740.1557\n",
            "Iteration:1125;Percent complete:28.1%; Average loss:1486.3886\n",
            "Iteration:1126;Percent complete:28.1%; Average loss:1319.3738\n",
            "Iteration:1127;Percent complete:28.2%; Average loss:1414.2406\n",
            "Iteration:1128;Percent complete:28.2%; Average loss:1495.7912\n",
            "Iteration:1129;Percent complete:28.2%; Average loss:1473.7085\n",
            "Iteration:1130;Percent complete:28.2%; Average loss:1606.5443\n",
            "Iteration:1131;Percent complete:28.3%; Average loss:1439.0867\n",
            "Iteration:1132;Percent complete:28.3%; Average loss:1430.0296\n",
            "Iteration:1133;Percent complete:28.3%; Average loss:1474.5912\n",
            "Iteration:1134;Percent complete:28.3%; Average loss:1336.1731\n",
            "Iteration:1135;Percent complete:28.4%; Average loss:1342.7655\n",
            "Iteration:1136;Percent complete:28.4%; Average loss:1444.7757\n",
            "Iteration:1137;Percent complete:28.4%; Average loss:1275.5259\n",
            "Iteration:1138;Percent complete:28.4%; Average loss:366.1355\n",
            "Iteration:1139;Percent complete:28.5%; Average loss:1394.3871\n",
            "Iteration:1140;Percent complete:28.5%; Average loss:670.2206\n",
            "Iteration:1141;Percent complete:28.5%; Average loss:718.6677\n",
            "Iteration:1142;Percent complete:28.5%; Average loss:1351.9608\n",
            "Iteration:1143;Percent complete:28.6%; Average loss:1565.8358\n",
            "Iteration:1144;Percent complete:28.6%; Average loss:1469.6309\n",
            "Iteration:1145;Percent complete:28.6%; Average loss:1331.4287\n",
            "Iteration:1146;Percent complete:28.6%; Average loss:1325.8688\n",
            "Iteration:1147;Percent complete:28.7%; Average loss:1362.0390\n",
            "Iteration:1148;Percent complete:28.7%; Average loss:755.3496\n",
            "Iteration:1149;Percent complete:28.7%; Average loss:1564.1485\n",
            "Iteration:1150;Percent complete:28.7%; Average loss:1609.1929\n",
            "Iteration:1151;Percent complete:28.8%; Average loss:1432.8181\n",
            "Iteration:1152;Percent complete:28.8%; Average loss:1606.8847\n",
            "Iteration:1153;Percent complete:28.8%; Average loss:811.6499\n",
            "Iteration:1154;Percent complete:28.8%; Average loss:1552.0077\n",
            "Iteration:1155;Percent complete:28.9%; Average loss:1430.0467\n",
            "Iteration:1156;Percent complete:28.9%; Average loss:712.4903\n",
            "Iteration:1157;Percent complete:28.9%; Average loss:534.6135\n",
            "Iteration:1158;Percent complete:28.9%; Average loss:731.5648\n",
            "Iteration:1159;Percent complete:29.0%; Average loss:1263.7305\n",
            "Iteration:1160;Percent complete:29.0%; Average loss:1410.9989\n",
            "Iteration:1161;Percent complete:29.0%; Average loss:1326.4375\n",
            "Iteration:1162;Percent complete:29.0%; Average loss:1365.4156\n",
            "Iteration:1163;Percent complete:29.1%; Average loss:699.4565\n",
            "Iteration:1164;Percent complete:29.1%; Average loss:1266.6698\n",
            "Iteration:1165;Percent complete:29.1%; Average loss:1540.9982\n",
            "Iteration:1166;Percent complete:29.1%; Average loss:605.7250\n",
            "Iteration:1167;Percent complete:29.2%; Average loss:1489.2155\n",
            "Iteration:1168;Percent complete:29.2%; Average loss:1628.0587\n",
            "Iteration:1169;Percent complete:29.2%; Average loss:1408.1971\n",
            "Iteration:1170;Percent complete:29.2%; Average loss:1571.5365\n",
            "Iteration:1171;Percent complete:29.3%; Average loss:1431.7374\n",
            "Iteration:1172;Percent complete:29.3%; Average loss:1382.4977\n",
            "Iteration:1173;Percent complete:29.3%; Average loss:1284.4396\n",
            "Iteration:1174;Percent complete:29.3%; Average loss:1326.9955\n",
            "Iteration:1175;Percent complete:29.4%; Average loss:1547.4947\n",
            "Iteration:1176;Percent complete:29.4%; Average loss:579.0452\n",
            "Iteration:1177;Percent complete:29.4%; Average loss:1434.3955\n",
            "Iteration:1178;Percent complete:29.4%; Average loss:702.8450\n",
            "Iteration:1179;Percent complete:29.5%; Average loss:726.0042\n",
            "Iteration:1180;Percent complete:29.5%; Average loss:1280.5189\n",
            "Iteration:1181;Percent complete:29.5%; Average loss:1455.9262\n",
            "Iteration:1182;Percent complete:29.5%; Average loss:1540.1495\n",
            "Iteration:1183;Percent complete:29.6%; Average loss:1427.5787\n",
            "Iteration:1184;Percent complete:29.6%; Average loss:1625.3336\n",
            "Iteration:1185;Percent complete:29.6%; Average loss:772.9467\n",
            "Iteration:1186;Percent complete:29.6%; Average loss:1410.9382\n",
            "Iteration:1187;Percent complete:29.7%; Average loss:1381.3245\n",
            "Iteration:1188;Percent complete:29.7%; Average loss:1434.9384\n",
            "Iteration:1189;Percent complete:29.7%; Average loss:1364.9522\n",
            "Iteration:1190;Percent complete:29.8%; Average loss:1288.2090\n",
            "Iteration:1191;Percent complete:29.8%; Average loss:1449.7369\n",
            "Iteration:1192;Percent complete:29.8%; Average loss:1309.6481\n",
            "Iteration:1193;Percent complete:29.8%; Average loss:1609.5606\n",
            "Iteration:1194;Percent complete:29.8%; Average loss:1508.1755\n",
            "Iteration:1195;Percent complete:29.9%; Average loss:1391.3651\n",
            "Iteration:1196;Percent complete:29.9%; Average loss:1472.4434\n",
            "Iteration:1197;Percent complete:29.9%; Average loss:1324.9675\n",
            "Iteration:1198;Percent complete:29.9%; Average loss:710.8669\n",
            "Iteration:1199;Percent complete:30.0%; Average loss:815.8069\n",
            "Iteration:1200;Percent complete:30.0%; Average loss:1298.9519\n",
            "Iteration:1201;Percent complete:30.0%; Average loss:776.8595\n",
            "Iteration:1202;Percent complete:30.0%; Average loss:1408.2012\n",
            "Iteration:1203;Percent complete:30.1%; Average loss:1323.5793\n",
            "Iteration:1204;Percent complete:30.1%; Average loss:1441.1979\n",
            "Iteration:1205;Percent complete:30.1%; Average loss:1292.2083\n",
            "Iteration:1206;Percent complete:30.1%; Average loss:703.0635\n",
            "Iteration:1207;Percent complete:30.2%; Average loss:1650.0105\n",
            "Iteration:1208;Percent complete:30.2%; Average loss:1536.6486\n",
            "Iteration:1209;Percent complete:30.2%; Average loss:643.5369\n",
            "Iteration:1210;Percent complete:30.2%; Average loss:667.4574\n",
            "Iteration:1211;Percent complete:30.3%; Average loss:1578.3969\n",
            "Iteration:1212;Percent complete:30.3%; Average loss:1398.7133\n",
            "Iteration:1213;Percent complete:30.3%; Average loss:765.2772\n",
            "Iteration:1214;Percent complete:30.3%; Average loss:1496.0539\n",
            "Iteration:1215;Percent complete:30.4%; Average loss:658.1271\n",
            "Iteration:1216;Percent complete:30.4%; Average loss:1587.3641\n",
            "Iteration:1217;Percent complete:30.4%; Average loss:1596.6285\n",
            "Iteration:1218;Percent complete:30.4%; Average loss:1257.8626\n",
            "Iteration:1219;Percent complete:30.5%; Average loss:1499.7790\n",
            "Iteration:1220;Percent complete:30.5%; Average loss:1456.5528\n",
            "Iteration:1221;Percent complete:30.5%; Average loss:1409.9617\n",
            "Iteration:1222;Percent complete:30.6%; Average loss:709.3260\n",
            "Iteration:1223;Percent complete:30.6%; Average loss:1529.3013\n",
            "Iteration:1224;Percent complete:30.6%; Average loss:509.0261\n",
            "Iteration:1225;Percent complete:30.6%; Average loss:1439.0259\n",
            "Iteration:1226;Percent complete:30.6%; Average loss:679.6781\n",
            "Iteration:1227;Percent complete:30.7%; Average loss:1498.8727\n",
            "Iteration:1228;Percent complete:30.7%; Average loss:1460.5850\n",
            "Iteration:1229;Percent complete:30.7%; Average loss:681.3024\n",
            "Iteration:1230;Percent complete:30.8%; Average loss:1486.4484\n",
            "Iteration:1231;Percent complete:30.8%; Average loss:1635.7227\n",
            "Iteration:1232;Percent complete:30.8%; Average loss:1303.2085\n",
            "Iteration:1233;Percent complete:30.8%; Average loss:1685.3250\n",
            "Iteration:1234;Percent complete:30.9%; Average loss:1493.4179\n",
            "Iteration:1235;Percent complete:30.9%; Average loss:1454.5174\n",
            "Iteration:1236;Percent complete:30.9%; Average loss:1510.8642\n",
            "Iteration:1237;Percent complete:30.9%; Average loss:1626.8528\n",
            "Iteration:1238;Percent complete:30.9%; Average loss:1381.0890\n",
            "Iteration:1239;Percent complete:31.0%; Average loss:1299.3222\n",
            "Iteration:1240;Percent complete:31.0%; Average loss:747.8812\n",
            "Iteration:1241;Percent complete:31.0%; Average loss:1302.7746\n",
            "Iteration:1242;Percent complete:31.1%; Average loss:1312.3606\n",
            "Iteration:1243;Percent complete:31.1%; Average loss:1290.5410\n",
            "Iteration:1244;Percent complete:31.1%; Average loss:1386.0948\n",
            "Iteration:1245;Percent complete:31.1%; Average loss:798.8530\n",
            "Iteration:1246;Percent complete:31.1%; Average loss:1454.8057\n",
            "Iteration:1247;Percent complete:31.2%; Average loss:1497.9270\n",
            "Iteration:1248;Percent complete:31.2%; Average loss:731.9159\n",
            "Iteration:1249;Percent complete:31.2%; Average loss:1429.5810\n",
            "Iteration:1250;Percent complete:31.2%; Average loss:690.3317\n",
            "Iteration:1251;Percent complete:31.3%; Average loss:683.4369\n",
            "Iteration:1252;Percent complete:31.3%; Average loss:1654.5052\n",
            "Iteration:1253;Percent complete:31.3%; Average loss:1342.2059\n",
            "Iteration:1254;Percent complete:31.4%; Average loss:1334.0557\n",
            "Iteration:1255;Percent complete:31.4%; Average loss:1602.9220\n",
            "Iteration:1256;Percent complete:31.4%; Average loss:733.0146\n",
            "Iteration:1257;Percent complete:31.4%; Average loss:1429.9375\n",
            "Iteration:1258;Percent complete:31.4%; Average loss:1437.3942\n",
            "Iteration:1259;Percent complete:31.5%; Average loss:1309.4536\n",
            "Iteration:1260;Percent complete:31.5%; Average loss:756.3494\n",
            "Iteration:1261;Percent complete:31.5%; Average loss:1515.9362\n",
            "Iteration:1262;Percent complete:31.6%; Average loss:1468.8013\n",
            "Iteration:1263;Percent complete:31.6%; Average loss:1491.8296\n",
            "Iteration:1264;Percent complete:31.6%; Average loss:1348.4610\n",
            "Iteration:1265;Percent complete:31.6%; Average loss:1339.5555\n",
            "Iteration:1266;Percent complete:31.6%; Average loss:1529.5718\n",
            "Iteration:1267;Percent complete:31.7%; Average loss:1500.8610\n",
            "Iteration:1268;Percent complete:31.7%; Average loss:1342.6480\n",
            "Iteration:1269;Percent complete:31.7%; Average loss:1435.3631\n",
            "Iteration:1270;Percent complete:31.8%; Average loss:1408.2628\n",
            "Iteration:1271;Percent complete:31.8%; Average loss:1352.1450\n",
            "Iteration:1272;Percent complete:31.8%; Average loss:652.6210\n",
            "Iteration:1273;Percent complete:31.8%; Average loss:1339.0332\n",
            "Iteration:1274;Percent complete:31.9%; Average loss:1469.3861\n",
            "Iteration:1275;Percent complete:31.9%; Average loss:819.5969\n",
            "Iteration:1276;Percent complete:31.9%; Average loss:481.5894\n",
            "Iteration:1277;Percent complete:31.9%; Average loss:1177.5545\n",
            "Iteration:1278;Percent complete:31.9%; Average loss:1401.4182\n",
            "Iteration:1279;Percent complete:32.0%; Average loss:1568.2734\n",
            "Iteration:1280;Percent complete:32.0%; Average loss:1276.3290\n",
            "Iteration:1281;Percent complete:32.0%; Average loss:1410.4611\n",
            "Iteration:1282;Percent complete:32.0%; Average loss:1480.0214\n",
            "Iteration:1283;Percent complete:32.1%; Average loss:1521.3422\n",
            "Iteration:1284;Percent complete:32.1%; Average loss:650.8086\n",
            "Iteration:1285;Percent complete:32.1%; Average loss:1420.8347\n",
            "Iteration:1286;Percent complete:32.1%; Average loss:1513.5193\n",
            "Iteration:1287;Percent complete:32.2%; Average loss:1496.9672\n",
            "Iteration:1288;Percent complete:32.2%; Average loss:1420.6020\n",
            "Iteration:1289;Percent complete:32.2%; Average loss:1452.7395\n",
            "Iteration:1290;Percent complete:32.2%; Average loss:1316.5385\n",
            "Iteration:1291;Percent complete:32.3%; Average loss:1406.2973\n",
            "Iteration:1292;Percent complete:32.3%; Average loss:1457.3386\n",
            "Iteration:1293;Percent complete:32.3%; Average loss:1409.9585\n",
            "Iteration:1294;Percent complete:32.4%; Average loss:1573.3761\n",
            "Iteration:1295;Percent complete:32.4%; Average loss:1442.1889\n",
            "Iteration:1296;Percent complete:32.4%; Average loss:1488.8988\n",
            "Iteration:1297;Percent complete:32.4%; Average loss:1338.8135\n",
            "Iteration:1298;Percent complete:32.5%; Average loss:572.3048\n",
            "Iteration:1299;Percent complete:32.5%; Average loss:1217.1514\n",
            "Iteration:1300;Percent complete:32.5%; Average loss:730.5875\n",
            "Iteration:1301;Percent complete:32.5%; Average loss:1410.8800\n",
            "Iteration:1302;Percent complete:32.6%; Average loss:1264.3785\n",
            "Iteration:1303;Percent complete:32.6%; Average loss:1460.4737\n",
            "Iteration:1304;Percent complete:32.6%; Average loss:1309.7314\n",
            "Iteration:1305;Percent complete:32.6%; Average loss:1384.1946\n",
            "Iteration:1306;Percent complete:32.6%; Average loss:775.3928\n",
            "Iteration:1307;Percent complete:32.7%; Average loss:1618.4133\n",
            "Iteration:1308;Percent complete:32.7%; Average loss:1636.5214\n",
            "Iteration:1309;Percent complete:32.7%; Average loss:1482.2551\n",
            "Iteration:1310;Percent complete:32.8%; Average loss:1664.1897\n",
            "Iteration:1311;Percent complete:32.8%; Average loss:1380.2677\n",
            "Iteration:1312;Percent complete:32.8%; Average loss:1709.9415\n",
            "Iteration:1313;Percent complete:32.8%; Average loss:1347.4787\n",
            "Iteration:1314;Percent complete:32.9%; Average loss:1520.8063\n",
            "Iteration:1315;Percent complete:32.9%; Average loss:705.0361\n",
            "Iteration:1316;Percent complete:32.9%; Average loss:1419.7767\n",
            "Iteration:1317;Percent complete:32.9%; Average loss:1368.0633\n",
            "Iteration:1318;Percent complete:33.0%; Average loss:1357.4185\n",
            "Iteration:1319;Percent complete:33.0%; Average loss:1538.3673\n",
            "Iteration:1320;Percent complete:33.0%; Average loss:1431.3942\n",
            "Iteration:1321;Percent complete:33.0%; Average loss:1428.8092\n",
            "Iteration:1322;Percent complete:33.1%; Average loss:1283.7917\n",
            "Iteration:1323;Percent complete:33.1%; Average loss:486.1594\n",
            "Iteration:1324;Percent complete:33.1%; Average loss:1327.9244\n",
            "Iteration:1325;Percent complete:33.1%; Average loss:705.3232\n",
            "Iteration:1326;Percent complete:33.1%; Average loss:1526.9222\n",
            "Iteration:1327;Percent complete:33.2%; Average loss:1263.0231\n",
            "Iteration:1328;Percent complete:33.2%; Average loss:1401.9953\n",
            "Iteration:1329;Percent complete:33.2%; Average loss:1396.6736\n",
            "Iteration:1330;Percent complete:33.2%; Average loss:1349.8214\n",
            "Iteration:1331;Percent complete:33.3%; Average loss:1457.3188\n",
            "Iteration:1332;Percent complete:33.3%; Average loss:1476.5029\n",
            "Iteration:1333;Percent complete:33.3%; Average loss:690.5312\n",
            "Iteration:1334;Percent complete:33.4%; Average loss:712.7396\n",
            "Iteration:1335;Percent complete:33.4%; Average loss:1414.2639\n",
            "Iteration:1336;Percent complete:33.4%; Average loss:1431.1778\n",
            "Iteration:1337;Percent complete:33.4%; Average loss:1266.8718\n",
            "Iteration:1338;Percent complete:33.5%; Average loss:1346.9028\n",
            "Iteration:1339;Percent complete:33.5%; Average loss:1463.8527\n",
            "Iteration:1340;Percent complete:33.5%; Average loss:1603.4036\n",
            "Iteration:1341;Percent complete:33.5%; Average loss:755.8770\n",
            "Iteration:1342;Percent complete:33.6%; Average loss:1252.2229\n",
            "Iteration:1343;Percent complete:33.6%; Average loss:1257.4431\n",
            "Iteration:1344;Percent complete:33.6%; Average loss:1365.8848\n",
            "Iteration:1345;Percent complete:33.6%; Average loss:1480.7326\n",
            "Iteration:1346;Percent complete:33.7%; Average loss:607.1898\n",
            "Iteration:1347;Percent complete:33.7%; Average loss:1274.5691\n",
            "Iteration:1348;Percent complete:33.7%; Average loss:1316.7966\n",
            "Iteration:1349;Percent complete:33.7%; Average loss:1509.6455\n",
            "Iteration:1350;Percent complete:33.8%; Average loss:1436.3350\n",
            "Iteration:1351;Percent complete:33.8%; Average loss:1421.8355\n",
            "Iteration:1352;Percent complete:33.8%; Average loss:737.1644\n",
            "Iteration:1353;Percent complete:33.8%; Average loss:1420.8851\n",
            "Iteration:1354;Percent complete:33.9%; Average loss:1381.8556\n",
            "Iteration:1355;Percent complete:33.9%; Average loss:1620.4417\n",
            "Iteration:1356;Percent complete:33.9%; Average loss:812.6140\n",
            "Iteration:1357;Percent complete:33.9%; Average loss:1289.0097\n",
            "Iteration:1358;Percent complete:34.0%; Average loss:1320.7513\n",
            "Iteration:1359;Percent complete:34.0%; Average loss:791.8190\n",
            "Iteration:1360;Percent complete:34.0%; Average loss:1284.7150\n",
            "Iteration:1361;Percent complete:34.0%; Average loss:1255.6386\n",
            "Iteration:1362;Percent complete:34.1%; Average loss:1312.0528\n",
            "Iteration:1363;Percent complete:34.1%; Average loss:1386.6419\n",
            "Iteration:1364;Percent complete:34.1%; Average loss:1553.1190\n",
            "Iteration:1365;Percent complete:34.1%; Average loss:1294.6631\n",
            "Iteration:1366;Percent complete:34.2%; Average loss:1381.4746\n",
            "Iteration:1367;Percent complete:34.2%; Average loss:1379.6312\n",
            "Iteration:1368;Percent complete:34.2%; Average loss:1374.7867\n",
            "Iteration:1369;Percent complete:34.2%; Average loss:1432.1821\n",
            "Iteration:1370;Percent complete:34.2%; Average loss:1552.7968\n",
            "Iteration:1371;Percent complete:34.3%; Average loss:1503.6318\n",
            "Iteration:1372;Percent complete:34.3%; Average loss:1343.8934\n",
            "Iteration:1373;Percent complete:34.3%; Average loss:1457.2986\n",
            "Iteration:1374;Percent complete:34.4%; Average loss:1552.4990\n",
            "Iteration:1375;Percent complete:34.4%; Average loss:866.3388\n",
            "Iteration:1376;Percent complete:34.4%; Average loss:746.1775\n",
            "Iteration:1377;Percent complete:34.4%; Average loss:1234.8327\n",
            "Iteration:1378;Percent complete:34.4%; Average loss:1218.6716\n",
            "Iteration:1379;Percent complete:34.5%; Average loss:1466.7776\n",
            "Iteration:1380;Percent complete:34.5%; Average loss:1344.3659\n",
            "Iteration:1381;Percent complete:34.5%; Average loss:1461.3144\n",
            "Iteration:1382;Percent complete:34.5%; Average loss:1376.6438\n",
            "Iteration:1383;Percent complete:34.6%; Average loss:1371.4031\n",
            "Iteration:1384;Percent complete:34.6%; Average loss:1511.7419\n",
            "Iteration:1385;Percent complete:34.6%; Average loss:1562.0194\n",
            "Iteration:1386;Percent complete:34.6%; Average loss:512.0725\n",
            "Iteration:1387;Percent complete:34.7%; Average loss:1472.9890\n",
            "Iteration:1388;Percent complete:34.7%; Average loss:669.0545\n",
            "Iteration:1389;Percent complete:34.7%; Average loss:1329.2279\n",
            "Iteration:1390;Percent complete:34.8%; Average loss:1469.4778\n",
            "Iteration:1391;Percent complete:34.8%; Average loss:594.0935\n",
            "Iteration:1392;Percent complete:34.8%; Average loss:1578.8334\n",
            "Iteration:1393;Percent complete:34.8%; Average loss:1494.9311\n",
            "Iteration:1394;Percent complete:34.8%; Average loss:1370.9565\n",
            "Iteration:1395;Percent complete:34.9%; Average loss:1500.9666\n",
            "Iteration:1396;Percent complete:34.9%; Average loss:1470.0371\n",
            "Iteration:1397;Percent complete:34.9%; Average loss:1435.1890\n",
            "Iteration:1398;Percent complete:34.9%; Average loss:1393.0238\n",
            "Iteration:1399;Percent complete:35.0%; Average loss:1410.2336\n",
            "Iteration:1400;Percent complete:35.0%; Average loss:1487.2343\n",
            "Iteration:1401;Percent complete:35.0%; Average loss:1489.9028\n",
            "Iteration:1402;Percent complete:35.0%; Average loss:1219.7069\n",
            "Iteration:1403;Percent complete:35.1%; Average loss:691.2288\n",
            "Iteration:1404;Percent complete:35.1%; Average loss:1550.3491\n",
            "Iteration:1405;Percent complete:35.1%; Average loss:710.1548\n",
            "Iteration:1406;Percent complete:35.1%; Average loss:1259.0915\n",
            "Iteration:1407;Percent complete:35.2%; Average loss:1291.0221\n",
            "Iteration:1408;Percent complete:35.2%; Average loss:1308.6220\n",
            "Iteration:1409;Percent complete:35.2%; Average loss:1302.5689\n",
            "Iteration:1410;Percent complete:35.2%; Average loss:770.6987\n",
            "Iteration:1411;Percent complete:35.3%; Average loss:1706.3684\n",
            "Iteration:1412;Percent complete:35.3%; Average loss:758.1343\n",
            "Iteration:1413;Percent complete:35.3%; Average loss:1227.7462\n",
            "Iteration:1414;Percent complete:35.4%; Average loss:1380.5912\n",
            "Iteration:1415;Percent complete:35.4%; Average loss:1478.5206\n",
            "Iteration:1416;Percent complete:35.4%; Average loss:1509.2017\n",
            "Iteration:1417;Percent complete:35.4%; Average loss:1155.5456\n",
            "Iteration:1418;Percent complete:35.4%; Average loss:1557.8341\n",
            "Iteration:1419;Percent complete:35.5%; Average loss:1274.6948\n",
            "Iteration:1420;Percent complete:35.5%; Average loss:1458.0128\n",
            "Iteration:1421;Percent complete:35.5%; Average loss:1557.6427\n",
            "Iteration:1422;Percent complete:35.5%; Average loss:1348.6151\n",
            "Iteration:1423;Percent complete:35.6%; Average loss:1340.2504\n",
            "Iteration:1424;Percent complete:35.6%; Average loss:1323.6678\n",
            "Iteration:1425;Percent complete:35.6%; Average loss:733.9844\n",
            "Iteration:1426;Percent complete:35.6%; Average loss:1547.7091\n",
            "Iteration:1427;Percent complete:35.7%; Average loss:679.0311\n",
            "Iteration:1428;Percent complete:35.7%; Average loss:1505.0831\n",
            "Iteration:1429;Percent complete:35.7%; Average loss:489.0932\n",
            "Iteration:1430;Percent complete:35.8%; Average loss:1209.7574\n",
            "Iteration:1431;Percent complete:35.8%; Average loss:1677.7800\n",
            "Iteration:1432;Percent complete:35.8%; Average loss:1441.3847\n",
            "Iteration:1433;Percent complete:35.8%; Average loss:1435.3271\n",
            "Iteration:1434;Percent complete:35.9%; Average loss:397.7876\n",
            "Iteration:1435;Percent complete:35.9%; Average loss:1379.7398\n",
            "Iteration:1436;Percent complete:35.9%; Average loss:1387.7310\n",
            "Iteration:1437;Percent complete:35.9%; Average loss:1375.7738\n",
            "Iteration:1438;Percent complete:35.9%; Average loss:1383.9176\n",
            "Iteration:1439;Percent complete:36.0%; Average loss:1348.2223\n",
            "Iteration:1440;Percent complete:36.0%; Average loss:1207.3908\n",
            "Iteration:1441;Percent complete:36.0%; Average loss:1440.2191\n",
            "Iteration:1442;Percent complete:36.0%; Average loss:1267.5247\n",
            "Iteration:1443;Percent complete:36.1%; Average loss:1270.5089\n",
            "Iteration:1444;Percent complete:36.1%; Average loss:1382.5697\n",
            "Iteration:1445;Percent complete:36.1%; Average loss:1549.1215\n",
            "Iteration:1446;Percent complete:36.1%; Average loss:1633.2767\n",
            "Iteration:1447;Percent complete:36.2%; Average loss:1582.9246\n",
            "Iteration:1448;Percent complete:36.2%; Average loss:854.0008\n",
            "Iteration:1449;Percent complete:36.2%; Average loss:627.1023\n",
            "Iteration:1450;Percent complete:36.2%; Average loss:422.7397\n",
            "Iteration:1451;Percent complete:36.3%; Average loss:1386.3044\n",
            "Iteration:1452;Percent complete:36.3%; Average loss:718.0500\n",
            "Iteration:1453;Percent complete:36.3%; Average loss:697.3354\n",
            "Iteration:1454;Percent complete:36.4%; Average loss:591.1969\n",
            "Iteration:1455;Percent complete:36.4%; Average loss:1459.4529\n",
            "Iteration:1456;Percent complete:36.4%; Average loss:1354.9242\n",
            "Iteration:1457;Percent complete:36.4%; Average loss:1655.1206\n",
            "Iteration:1458;Percent complete:36.4%; Average loss:1603.7775\n",
            "Iteration:1459;Percent complete:36.5%; Average loss:1512.5786\n",
            "Iteration:1460;Percent complete:36.5%; Average loss:701.4341\n",
            "Iteration:1461;Percent complete:36.5%; Average loss:1338.0569\n",
            "Iteration:1462;Percent complete:36.5%; Average loss:1410.9385\n",
            "Iteration:1463;Percent complete:36.6%; Average loss:1460.5392\n",
            "Iteration:1464;Percent complete:36.6%; Average loss:1303.4027\n",
            "Iteration:1465;Percent complete:36.6%; Average loss:1330.7138\n",
            "Iteration:1466;Percent complete:36.6%; Average loss:645.5342\n",
            "Iteration:1467;Percent complete:36.7%; Average loss:770.4680\n",
            "Iteration:1468;Percent complete:36.7%; Average loss:1593.1341\n",
            "Iteration:1469;Percent complete:36.7%; Average loss:1160.5547\n",
            "Iteration:1470;Percent complete:36.8%; Average loss:1435.6088\n",
            "Iteration:1471;Percent complete:36.8%; Average loss:834.8256\n",
            "Iteration:1472;Percent complete:36.8%; Average loss:646.1514\n",
            "Iteration:1473;Percent complete:36.8%; Average loss:1322.1723\n",
            "Iteration:1474;Percent complete:36.9%; Average loss:1376.8981\n",
            "Iteration:1475;Percent complete:36.9%; Average loss:1336.5141\n",
            "Iteration:1476;Percent complete:36.9%; Average loss:1360.7635\n",
            "Iteration:1477;Percent complete:36.9%; Average loss:1339.2209\n",
            "Iteration:1478;Percent complete:37.0%; Average loss:1396.4018\n",
            "Iteration:1479;Percent complete:37.0%; Average loss:1418.6607\n",
            "Iteration:1480;Percent complete:37.0%; Average loss:1318.2102\n",
            "Iteration:1481;Percent complete:37.0%; Average loss:1419.2787\n",
            "Iteration:1482;Percent complete:37.0%; Average loss:1407.8673\n",
            "Iteration:1483;Percent complete:37.1%; Average loss:1349.6817\n",
            "Iteration:1484;Percent complete:37.1%; Average loss:1460.3477\n",
            "Iteration:1485;Percent complete:37.1%; Average loss:1387.6245\n",
            "Iteration:1486;Percent complete:37.1%; Average loss:1372.4832\n",
            "Iteration:1487;Percent complete:37.2%; Average loss:1240.3055\n",
            "Iteration:1488;Percent complete:37.2%; Average loss:1327.6076\n",
            "Iteration:1489;Percent complete:37.2%; Average loss:1257.2143\n",
            "Iteration:1490;Percent complete:37.2%; Average loss:1345.6636\n",
            "Iteration:1491;Percent complete:37.3%; Average loss:1341.0485\n",
            "Iteration:1492;Percent complete:37.3%; Average loss:1378.9273\n",
            "Iteration:1493;Percent complete:37.3%; Average loss:332.2238\n",
            "Iteration:1494;Percent complete:37.4%; Average loss:1250.0762\n",
            "Iteration:1495;Percent complete:37.4%; Average loss:1305.3187\n",
            "Iteration:1496;Percent complete:37.4%; Average loss:1457.1843\n",
            "Iteration:1497;Percent complete:37.4%; Average loss:1580.4224\n",
            "Iteration:1498;Percent complete:37.5%; Average loss:622.3814\n",
            "Iteration:1499;Percent complete:37.5%; Average loss:1174.4657\n",
            "Iteration:1500;Percent complete:37.5%; Average loss:1563.9404\n",
            "Iteration:1501;Percent complete:37.5%; Average loss:1395.0229\n",
            "Iteration:1502;Percent complete:37.5%; Average loss:820.1016\n",
            "Iteration:1503;Percent complete:37.6%; Average loss:1476.1304\n",
            "Iteration:1504;Percent complete:37.6%; Average loss:1348.1202\n",
            "Iteration:1505;Percent complete:37.6%; Average loss:604.0500\n",
            "Iteration:1506;Percent complete:37.6%; Average loss:666.1906\n",
            "Iteration:1507;Percent complete:37.7%; Average loss:419.7436\n",
            "Iteration:1508;Percent complete:37.7%; Average loss:1641.1420\n",
            "Iteration:1509;Percent complete:37.7%; Average loss:727.6332\n",
            "Iteration:1510;Percent complete:37.8%; Average loss:1431.1987\n",
            "Iteration:1511;Percent complete:37.8%; Average loss:1192.6548\n",
            "Iteration:1512;Percent complete:37.8%; Average loss:1228.1849\n",
            "Iteration:1513;Percent complete:37.8%; Average loss:1160.9371\n",
            "Iteration:1514;Percent complete:37.9%; Average loss:1540.8116\n",
            "Iteration:1515;Percent complete:37.9%; Average loss:1488.1401\n",
            "Iteration:1516;Percent complete:37.9%; Average loss:1415.2873\n",
            "Iteration:1517;Percent complete:37.9%; Average loss:1375.0045\n",
            "Iteration:1518;Percent complete:38.0%; Average loss:1221.5525\n",
            "Iteration:1519;Percent complete:38.0%; Average loss:1733.8593\n",
            "Iteration:1520;Percent complete:38.0%; Average loss:1153.6498\n",
            "Iteration:1521;Percent complete:38.0%; Average loss:1233.0910\n",
            "Iteration:1522;Percent complete:38.0%; Average loss:1461.8658\n",
            "Iteration:1523;Percent complete:38.1%; Average loss:1381.1693\n",
            "Iteration:1524;Percent complete:38.1%; Average loss:1413.8602\n",
            "Iteration:1525;Percent complete:38.1%; Average loss:1407.3310\n",
            "Iteration:1526;Percent complete:38.1%; Average loss:677.8837\n",
            "Iteration:1527;Percent complete:38.2%; Average loss:793.7605\n",
            "Iteration:1528;Percent complete:38.2%; Average loss:1325.0042\n",
            "Iteration:1529;Percent complete:38.2%; Average loss:476.3205\n",
            "Iteration:1530;Percent complete:38.2%; Average loss:1420.4192\n",
            "Iteration:1531;Percent complete:38.3%; Average loss:714.2258\n",
            "Iteration:1532;Percent complete:38.3%; Average loss:1411.9815\n",
            "Iteration:1533;Percent complete:38.3%; Average loss:1512.3620\n",
            "Iteration:1534;Percent complete:38.4%; Average loss:1390.3448\n",
            "Iteration:1535;Percent complete:38.4%; Average loss:1340.4304\n",
            "Iteration:1536;Percent complete:38.4%; Average loss:1320.7666\n",
            "Iteration:1537;Percent complete:38.4%; Average loss:1404.1695\n",
            "Iteration:1538;Percent complete:38.5%; Average loss:1327.2837\n",
            "Iteration:1539;Percent complete:38.5%; Average loss:1442.2730\n",
            "Iteration:1540;Percent complete:38.5%; Average loss:1378.4844\n",
            "Iteration:1541;Percent complete:38.5%; Average loss:1536.5637\n",
            "Iteration:1542;Percent complete:38.6%; Average loss:681.8265\n",
            "Iteration:1543;Percent complete:38.6%; Average loss:706.2100\n",
            "Iteration:1544;Percent complete:38.6%; Average loss:1343.0745\n",
            "Iteration:1545;Percent complete:38.6%; Average loss:1321.4437\n",
            "Iteration:1546;Percent complete:38.6%; Average loss:747.9254\n",
            "Iteration:1547;Percent complete:38.7%; Average loss:1609.7324\n",
            "Iteration:1548;Percent complete:38.7%; Average loss:666.1653\n",
            "Iteration:1549;Percent complete:38.7%; Average loss:725.6601\n",
            "Iteration:1550;Percent complete:38.8%; Average loss:1447.7675\n",
            "Iteration:1551;Percent complete:38.8%; Average loss:1501.4184\n",
            "Iteration:1552;Percent complete:38.8%; Average loss:1194.2895\n",
            "Iteration:1553;Percent complete:38.8%; Average loss:800.8655\n",
            "Iteration:1554;Percent complete:38.9%; Average loss:1475.6177\n",
            "Iteration:1555;Percent complete:38.9%; Average loss:1451.2174\n",
            "Iteration:1556;Percent complete:38.9%; Average loss:1350.6389\n",
            "Iteration:1557;Percent complete:38.9%; Average loss:1502.3509\n",
            "Iteration:1558;Percent complete:39.0%; Average loss:1445.2168\n",
            "Iteration:1559;Percent complete:39.0%; Average loss:1532.3327\n",
            "Iteration:1560;Percent complete:39.0%; Average loss:1414.6763\n",
            "Iteration:1561;Percent complete:39.0%; Average loss:1408.5634\n",
            "Iteration:1562;Percent complete:39.1%; Average loss:1304.8175\n",
            "Iteration:1563;Percent complete:39.1%; Average loss:1380.4403\n",
            "Iteration:1564;Percent complete:39.1%; Average loss:1434.1884\n",
            "Iteration:1565;Percent complete:39.1%; Average loss:473.2183\n",
            "Iteration:1566;Percent complete:39.1%; Average loss:1314.1767\n",
            "Iteration:1567;Percent complete:39.2%; Average loss:1329.3440\n",
            "Iteration:1568;Percent complete:39.2%; Average loss:1347.4747\n",
            "Iteration:1569;Percent complete:39.2%; Average loss:1454.5979\n",
            "Iteration:1570;Percent complete:39.2%; Average loss:764.0453\n",
            "Iteration:1571;Percent complete:39.3%; Average loss:1619.4047\n",
            "Iteration:1572;Percent complete:39.3%; Average loss:1508.6063\n",
            "Iteration:1573;Percent complete:39.3%; Average loss:1515.5473\n",
            "Iteration:1574;Percent complete:39.4%; Average loss:1396.1415\n",
            "Iteration:1575;Percent complete:39.4%; Average loss:1411.5296\n",
            "Iteration:1576;Percent complete:39.4%; Average loss:1321.7245\n",
            "Iteration:1577;Percent complete:39.4%; Average loss:756.4427\n",
            "Iteration:1578;Percent complete:39.5%; Average loss:1385.3613\n",
            "Iteration:1579;Percent complete:39.5%; Average loss:1174.5673\n",
            "Iteration:1580;Percent complete:39.5%; Average loss:1332.5324\n",
            "Iteration:1581;Percent complete:39.5%; Average loss:1389.0000\n",
            "Iteration:1582;Percent complete:39.6%; Average loss:534.7864\n",
            "Iteration:1583;Percent complete:39.6%; Average loss:694.8287\n",
            "Iteration:1584;Percent complete:39.6%; Average loss:711.7083\n",
            "Iteration:1585;Percent complete:39.6%; Average loss:1289.8832\n",
            "Iteration:1586;Percent complete:39.6%; Average loss:815.5468\n",
            "Iteration:1587;Percent complete:39.7%; Average loss:624.4714\n",
            "Iteration:1588;Percent complete:39.7%; Average loss:1417.9631\n",
            "Iteration:1589;Percent complete:39.7%; Average loss:1403.3870\n",
            "Iteration:1590;Percent complete:39.8%; Average loss:1391.8632\n",
            "Iteration:1591;Percent complete:39.8%; Average loss:1285.1366\n",
            "Iteration:1592;Percent complete:39.8%; Average loss:1332.9778\n",
            "Iteration:1593;Percent complete:39.8%; Average loss:1441.7106\n",
            "Iteration:1594;Percent complete:39.9%; Average loss:1292.4889\n",
            "Iteration:1595;Percent complete:39.9%; Average loss:1491.9107\n",
            "Iteration:1596;Percent complete:39.9%; Average loss:739.3621\n",
            "Iteration:1597;Percent complete:39.9%; Average loss:1353.2684\n",
            "Iteration:1598;Percent complete:40.0%; Average loss:1105.8537\n",
            "Iteration:1599;Percent complete:40.0%; Average loss:1369.9457\n",
            "Iteration:1600;Percent complete:40.0%; Average loss:1660.2288\n",
            "Iteration:1601;Percent complete:40.0%; Average loss:681.6723\n",
            "Iteration:1602;Percent complete:40.1%; Average loss:1418.2560\n",
            "Iteration:1603;Percent complete:40.1%; Average loss:1193.8357\n",
            "Iteration:1604;Percent complete:40.1%; Average loss:1335.7912\n",
            "Iteration:1605;Percent complete:40.1%; Average loss:1384.5653\n",
            "Iteration:1606;Percent complete:40.2%; Average loss:722.7832\n",
            "Iteration:1607;Percent complete:40.2%; Average loss:1400.0983\n",
            "Iteration:1608;Percent complete:40.2%; Average loss:498.7722\n",
            "Iteration:1609;Percent complete:40.2%; Average loss:1358.9160\n",
            "Iteration:1610;Percent complete:40.2%; Average loss:1286.3567\n",
            "Iteration:1611;Percent complete:40.3%; Average loss:737.2483\n",
            "Iteration:1612;Percent complete:40.3%; Average loss:1447.5543\n",
            "Iteration:1613;Percent complete:40.3%; Average loss:1578.9907\n",
            "Iteration:1614;Percent complete:40.4%; Average loss:677.5367\n",
            "Iteration:1615;Percent complete:40.4%; Average loss:1604.3905\n",
            "Iteration:1616;Percent complete:40.4%; Average loss:1142.4509\n",
            "Iteration:1617;Percent complete:40.4%; Average loss:1240.1081\n",
            "Iteration:1618;Percent complete:40.5%; Average loss:1411.3571\n",
            "Iteration:1619;Percent complete:40.5%; Average loss:571.4985\n",
            "Iteration:1620;Percent complete:40.5%; Average loss:1550.2535\n",
            "Iteration:1621;Percent complete:40.5%; Average loss:1284.7810\n",
            "Iteration:1622;Percent complete:40.6%; Average loss:1321.3641\n",
            "Iteration:1623;Percent complete:40.6%; Average loss:1232.3555\n",
            "Iteration:1624;Percent complete:40.6%; Average loss:1385.1603\n",
            "Iteration:1625;Percent complete:40.6%; Average loss:1145.5398\n",
            "Iteration:1626;Percent complete:40.6%; Average loss:1327.2159\n",
            "Iteration:1627;Percent complete:40.7%; Average loss:1429.8510\n",
            "Iteration:1628;Percent complete:40.7%; Average loss:1551.4601\n",
            "Iteration:1629;Percent complete:40.7%; Average loss:1293.9280\n",
            "Iteration:1630;Percent complete:40.8%; Average loss:1333.9768\n",
            "Iteration:1631;Percent complete:40.8%; Average loss:1323.3845\n",
            "Iteration:1632;Percent complete:40.8%; Average loss:1215.2786\n",
            "Iteration:1633;Percent complete:40.8%; Average loss:1296.7357\n",
            "Iteration:1634;Percent complete:40.8%; Average loss:1462.8789\n",
            "Iteration:1635;Percent complete:40.9%; Average loss:1314.3523\n",
            "Iteration:1636;Percent complete:40.9%; Average loss:1527.6835\n",
            "Iteration:1637;Percent complete:40.9%; Average loss:748.5619\n",
            "Iteration:1638;Percent complete:40.9%; Average loss:1417.1864\n",
            "Iteration:1639;Percent complete:41.0%; Average loss:1428.3900\n",
            "Iteration:1640;Percent complete:41.0%; Average loss:1623.9163\n",
            "Iteration:1641;Percent complete:41.0%; Average loss:1383.2149\n",
            "Iteration:1642;Percent complete:41.0%; Average loss:1372.7064\n",
            "Iteration:1643;Percent complete:41.1%; Average loss:1333.7780\n",
            "Iteration:1644;Percent complete:41.1%; Average loss:1295.6258\n",
            "Iteration:1645;Percent complete:41.1%; Average loss:512.4179\n",
            "Iteration:1646;Percent complete:41.1%; Average loss:1348.4745\n",
            "Iteration:1647;Percent complete:41.2%; Average loss:734.7776\n",
            "Iteration:1648;Percent complete:41.2%; Average loss:1246.0130\n",
            "Iteration:1649;Percent complete:41.2%; Average loss:1370.2792\n",
            "Iteration:1650;Percent complete:41.2%; Average loss:1324.1624\n",
            "Iteration:1651;Percent complete:41.3%; Average loss:619.3103\n",
            "Iteration:1652;Percent complete:41.3%; Average loss:1352.4623\n",
            "Iteration:1653;Percent complete:41.3%; Average loss:689.0605\n",
            "Iteration:1654;Percent complete:41.3%; Average loss:646.0091\n",
            "Iteration:1655;Percent complete:41.4%; Average loss:736.9669\n",
            "Iteration:1656;Percent complete:41.4%; Average loss:1324.5621\n",
            "Iteration:1657;Percent complete:41.4%; Average loss:1433.8286\n",
            "Iteration:1658;Percent complete:41.4%; Average loss:763.3666\n",
            "Iteration:1659;Percent complete:41.5%; Average loss:1451.8291\n",
            "Iteration:1660;Percent complete:41.5%; Average loss:1488.0004\n",
            "Iteration:1661;Percent complete:41.5%; Average loss:1340.0059\n",
            "Iteration:1662;Percent complete:41.5%; Average loss:1443.4822\n",
            "Iteration:1663;Percent complete:41.6%; Average loss:1475.0411\n",
            "Iteration:1664;Percent complete:41.6%; Average loss:730.5619\n",
            "Iteration:1665;Percent complete:41.6%; Average loss:1441.5497\n",
            "Iteration:1666;Percent complete:41.6%; Average loss:1455.9028\n",
            "Iteration:1667;Percent complete:41.7%; Average loss:1397.2137\n",
            "Iteration:1668;Percent complete:41.7%; Average loss:1669.0880\n",
            "Iteration:1669;Percent complete:41.7%; Average loss:1354.3577\n",
            "Iteration:1670;Percent complete:41.8%; Average loss:1319.9868\n",
            "Iteration:1671;Percent complete:41.8%; Average loss:1255.1335\n",
            "Iteration:1672;Percent complete:41.8%; Average loss:1475.1616\n",
            "Iteration:1673;Percent complete:41.8%; Average loss:1259.1993\n",
            "Iteration:1674;Percent complete:41.9%; Average loss:1336.9830\n",
            "Iteration:1675;Percent complete:41.9%; Average loss:1510.7915\n",
            "Iteration:1676;Percent complete:41.9%; Average loss:1411.3390\n",
            "Iteration:1677;Percent complete:41.9%; Average loss:1284.7464\n",
            "Iteration:1678;Percent complete:41.9%; Average loss:1203.1159\n",
            "Iteration:1679;Percent complete:42.0%; Average loss:1150.7320\n",
            "Iteration:1680;Percent complete:42.0%; Average loss:1364.7826\n",
            "Iteration:1681;Percent complete:42.0%; Average loss:1370.2673\n",
            "Iteration:1682;Percent complete:42.0%; Average loss:1394.1879\n",
            "Iteration:1683;Percent complete:42.1%; Average loss:1294.6128\n",
            "Iteration:1684;Percent complete:42.1%; Average loss:1375.2583\n",
            "Iteration:1685;Percent complete:42.1%; Average loss:441.7690\n",
            "Iteration:1686;Percent complete:42.1%; Average loss:769.7150\n",
            "Iteration:1687;Percent complete:42.2%; Average loss:1311.6526\n",
            "Iteration:1688;Percent complete:42.2%; Average loss:519.9667\n",
            "Iteration:1689;Percent complete:42.2%; Average loss:1440.3391\n",
            "Iteration:1690;Percent complete:42.2%; Average loss:1435.2963\n",
            "Iteration:1691;Percent complete:42.3%; Average loss:1404.6566\n",
            "Iteration:1692;Percent complete:42.3%; Average loss:1310.4511\n",
            "Iteration:1693;Percent complete:42.3%; Average loss:1438.7609\n",
            "Iteration:1694;Percent complete:42.4%; Average loss:1369.6261\n",
            "Iteration:1695;Percent complete:42.4%; Average loss:1437.1388\n",
            "Iteration:1696;Percent complete:42.4%; Average loss:536.8862\n",
            "Iteration:1697;Percent complete:42.4%; Average loss:1345.7133\n",
            "Iteration:1698;Percent complete:42.4%; Average loss:1452.6260\n",
            "Iteration:1699;Percent complete:42.5%; Average loss:680.5036\n",
            "Iteration:1700;Percent complete:42.5%; Average loss:1625.9490\n",
            "Iteration:1701;Percent complete:42.5%; Average loss:598.8213\n",
            "Iteration:1702;Percent complete:42.5%; Average loss:716.6173\n",
            "Iteration:1703;Percent complete:42.6%; Average loss:456.4264\n",
            "Iteration:1704;Percent complete:42.6%; Average loss:696.5051\n",
            "Iteration:1705;Percent complete:42.6%; Average loss:1439.7064\n",
            "Iteration:1706;Percent complete:42.6%; Average loss:1532.5295\n",
            "Iteration:1707;Percent complete:42.7%; Average loss:1482.1042\n",
            "Iteration:1708;Percent complete:42.7%; Average loss:786.8826\n",
            "Iteration:1709;Percent complete:42.7%; Average loss:1349.6141\n",
            "Iteration:1710;Percent complete:42.8%; Average loss:785.6130\n",
            "Iteration:1711;Percent complete:42.8%; Average loss:1263.2723\n",
            "Iteration:1712;Percent complete:42.8%; Average loss:717.0323\n",
            "Iteration:1713;Percent complete:42.8%; Average loss:663.8383\n",
            "Iteration:1714;Percent complete:42.9%; Average loss:1367.9374\n",
            "Iteration:1715;Percent complete:42.9%; Average loss:1333.7556\n",
            "Iteration:1716;Percent complete:42.9%; Average loss:1225.5184\n",
            "Iteration:1717;Percent complete:42.9%; Average loss:1355.7815\n",
            "Iteration:1718;Percent complete:43.0%; Average loss:1474.1744\n",
            "Iteration:1719;Percent complete:43.0%; Average loss:1288.1609\n",
            "Iteration:1720;Percent complete:43.0%; Average loss:1515.9211\n",
            "Iteration:1721;Percent complete:43.0%; Average loss:806.4100\n",
            "Iteration:1722;Percent complete:43.0%; Average loss:1459.3180\n",
            "Iteration:1723;Percent complete:43.1%; Average loss:1707.9264\n",
            "Iteration:1724;Percent complete:43.1%; Average loss:1251.1844\n",
            "Iteration:1725;Percent complete:43.1%; Average loss:1271.9989\n",
            "Iteration:1726;Percent complete:43.1%; Average loss:469.3902\n",
            "Iteration:1727;Percent complete:43.2%; Average loss:1343.7562\n",
            "Iteration:1728;Percent complete:43.2%; Average loss:1347.8809\n",
            "Iteration:1729;Percent complete:43.2%; Average loss:756.7886\n",
            "Iteration:1730;Percent complete:43.2%; Average loss:1460.7808\n",
            "Iteration:1731;Percent complete:43.3%; Average loss:1519.8297\n",
            "Iteration:1732;Percent complete:43.3%; Average loss:666.0220\n",
            "Iteration:1733;Percent complete:43.3%; Average loss:1379.9089\n",
            "Iteration:1734;Percent complete:43.4%; Average loss:1348.8837\n",
            "Iteration:1735;Percent complete:43.4%; Average loss:1464.9682\n",
            "Iteration:1736;Percent complete:43.4%; Average loss:1247.1917\n",
            "Iteration:1737;Percent complete:43.4%; Average loss:1254.5409\n",
            "Iteration:1738;Percent complete:43.5%; Average loss:703.0218\n",
            "Iteration:1739;Percent complete:43.5%; Average loss:770.9547\n",
            "Iteration:1740;Percent complete:43.5%; Average loss:1433.5934\n",
            "Iteration:1741;Percent complete:43.5%; Average loss:710.8637\n",
            "Iteration:1742;Percent complete:43.5%; Average loss:1407.7492\n",
            "Iteration:1743;Percent complete:43.6%; Average loss:1463.2441\n",
            "Iteration:1744;Percent complete:43.6%; Average loss:716.8838\n",
            "Iteration:1745;Percent complete:43.6%; Average loss:634.8316\n",
            "Iteration:1746;Percent complete:43.6%; Average loss:622.0667\n",
            "Iteration:1747;Percent complete:43.7%; Average loss:1712.7444\n",
            "Iteration:1748;Percent complete:43.7%; Average loss:1290.0015\n",
            "Iteration:1749;Percent complete:43.7%; Average loss:1185.9121\n",
            "Iteration:1750;Percent complete:43.8%; Average loss:723.9046\n",
            "Iteration:1751;Percent complete:43.8%; Average loss:1562.2557\n",
            "Iteration:1752;Percent complete:43.8%; Average loss:1531.9618\n",
            "Iteration:1753;Percent complete:43.8%; Average loss:1363.3404\n",
            "Iteration:1754;Percent complete:43.9%; Average loss:1459.2301\n",
            "Iteration:1755;Percent complete:43.9%; Average loss:1501.4728\n",
            "Iteration:1756;Percent complete:43.9%; Average loss:1146.0301\n",
            "Iteration:1757;Percent complete:43.9%; Average loss:744.9265\n",
            "Iteration:1758;Percent complete:44.0%; Average loss:1375.5515\n",
            "Iteration:1759;Percent complete:44.0%; Average loss:1473.8927\n",
            "Iteration:1760;Percent complete:44.0%; Average loss:1490.0175\n",
            "Iteration:1761;Percent complete:44.0%; Average loss:1544.5601\n",
            "Iteration:1762;Percent complete:44.0%; Average loss:1341.8089\n",
            "Iteration:1763;Percent complete:44.1%; Average loss:758.5113\n",
            "Iteration:1764;Percent complete:44.1%; Average loss:1424.5178\n",
            "Iteration:1765;Percent complete:44.1%; Average loss:1406.0079\n",
            "Iteration:1766;Percent complete:44.1%; Average loss:1459.9625\n",
            "Iteration:1767;Percent complete:44.2%; Average loss:542.0933\n",
            "Iteration:1768;Percent complete:44.2%; Average loss:1293.2838\n",
            "Iteration:1769;Percent complete:44.2%; Average loss:1411.5107\n",
            "Iteration:1770;Percent complete:44.2%; Average loss:1407.3177\n",
            "Iteration:1771;Percent complete:44.3%; Average loss:808.6039\n",
            "Iteration:1772;Percent complete:44.3%; Average loss:1422.5841\n",
            "Iteration:1773;Percent complete:44.3%; Average loss:1305.8568\n",
            "Iteration:1774;Percent complete:44.4%; Average loss:540.1494\n",
            "Iteration:1775;Percent complete:44.4%; Average loss:1324.5668\n",
            "Iteration:1776;Percent complete:44.4%; Average loss:1483.7425\n",
            "Iteration:1777;Percent complete:44.4%; Average loss:718.1235\n",
            "Iteration:1778;Percent complete:44.5%; Average loss:683.7808\n",
            "Iteration:1779;Percent complete:44.5%; Average loss:1329.3732\n",
            "Iteration:1780;Percent complete:44.5%; Average loss:1382.6421\n",
            "Iteration:1781;Percent complete:44.5%; Average loss:1386.9866\n",
            "Iteration:1782;Percent complete:44.5%; Average loss:445.4944\n",
            "Iteration:1783;Percent complete:44.6%; Average loss:1360.1417\n",
            "Iteration:1784;Percent complete:44.6%; Average loss:1106.7868\n",
            "Iteration:1785;Percent complete:44.6%; Average loss:1438.2895\n",
            "Iteration:1786;Percent complete:44.6%; Average loss:1385.9496\n",
            "Iteration:1787;Percent complete:44.7%; Average loss:1595.6968\n",
            "Iteration:1788;Percent complete:44.7%; Average loss:1387.4063\n",
            "Iteration:1789;Percent complete:44.7%; Average loss:1334.0296\n",
            "Iteration:1790;Percent complete:44.8%; Average loss:609.6634\n",
            "Iteration:1791;Percent complete:44.8%; Average loss:1658.7654\n",
            "Iteration:1792;Percent complete:44.8%; Average loss:1423.6079\n",
            "Iteration:1793;Percent complete:44.8%; Average loss:742.2941\n",
            "Iteration:1794;Percent complete:44.9%; Average loss:1378.6822\n",
            "Iteration:1795;Percent complete:44.9%; Average loss:1551.2108\n",
            "Iteration:1796;Percent complete:44.9%; Average loss:1416.8557\n",
            "Iteration:1797;Percent complete:44.9%; Average loss:1433.0348\n",
            "Iteration:1798;Percent complete:45.0%; Average loss:1335.9563\n",
            "Iteration:1799;Percent complete:45.0%; Average loss:699.6254\n",
            "Iteration:1800;Percent complete:45.0%; Average loss:1531.2352\n",
            "Iteration:1801;Percent complete:45.0%; Average loss:1378.7177\n",
            "Iteration:1802;Percent complete:45.1%; Average loss:1510.2288\n",
            "Iteration:1803;Percent complete:45.1%; Average loss:1284.5033\n",
            "Iteration:1804;Percent complete:45.1%; Average loss:1246.4239\n",
            "Iteration:1805;Percent complete:45.1%; Average loss:461.4413\n",
            "Iteration:1806;Percent complete:45.1%; Average loss:1420.0521\n",
            "Iteration:1807;Percent complete:45.2%; Average loss:1421.9772\n",
            "Iteration:1808;Percent complete:45.2%; Average loss:1451.5790\n",
            "Iteration:1809;Percent complete:45.2%; Average loss:782.9089\n",
            "Iteration:1810;Percent complete:45.2%; Average loss:1288.0585\n",
            "Iteration:1811;Percent complete:45.3%; Average loss:1397.3071\n",
            "Iteration:1812;Percent complete:45.3%; Average loss:1339.9923\n",
            "Iteration:1813;Percent complete:45.3%; Average loss:1385.2805\n",
            "Iteration:1814;Percent complete:45.4%; Average loss:1417.4074\n",
            "Iteration:1815;Percent complete:45.4%; Average loss:1534.2784\n",
            "Iteration:1816;Percent complete:45.4%; Average loss:683.9181\n",
            "Iteration:1817;Percent complete:45.4%; Average loss:1427.7724\n",
            "Iteration:1818;Percent complete:45.5%; Average loss:1487.7486\n",
            "Iteration:1819;Percent complete:45.5%; Average loss:1398.3484\n",
            "Iteration:1820;Percent complete:45.5%; Average loss:1282.8537\n",
            "Iteration:1821;Percent complete:45.5%; Average loss:1547.3801\n",
            "Iteration:1822;Percent complete:45.6%; Average loss:1377.5199\n",
            "Iteration:1823;Percent complete:45.6%; Average loss:432.0628\n",
            "Iteration:1824;Percent complete:45.6%; Average loss:1379.2150\n",
            "Iteration:1825;Percent complete:45.6%; Average loss:1405.9299\n",
            "Iteration:1826;Percent complete:45.6%; Average loss:686.5882\n",
            "Iteration:1827;Percent complete:45.7%; Average loss:1216.7278\n",
            "Iteration:1828;Percent complete:45.7%; Average loss:1493.0341\n",
            "Iteration:1829;Percent complete:45.7%; Average loss:1255.4613\n",
            "Iteration:1830;Percent complete:45.8%; Average loss:594.4549\n",
            "Iteration:1831;Percent complete:45.8%; Average loss:1248.1771\n",
            "Iteration:1832;Percent complete:45.8%; Average loss:1325.1122\n",
            "Iteration:1833;Percent complete:45.8%; Average loss:494.9301\n",
            "Iteration:1834;Percent complete:45.9%; Average loss:1388.4482\n",
            "Iteration:1835;Percent complete:45.9%; Average loss:1392.7186\n",
            "Iteration:1836;Percent complete:45.9%; Average loss:700.9675\n",
            "Iteration:1837;Percent complete:45.9%; Average loss:1214.8978\n",
            "Iteration:1838;Percent complete:46.0%; Average loss:1607.1534\n",
            "Iteration:1839;Percent complete:46.0%; Average loss:1105.8887\n",
            "Iteration:1840;Percent complete:46.0%; Average loss:1246.4283\n",
            "Iteration:1841;Percent complete:46.0%; Average loss:1431.4584\n",
            "Iteration:1842;Percent complete:46.1%; Average loss:1365.9328\n",
            "Iteration:1843;Percent complete:46.1%; Average loss:1578.7743\n",
            "Iteration:1844;Percent complete:46.1%; Average loss:789.2183\n",
            "Iteration:1845;Percent complete:46.1%; Average loss:1581.4208\n",
            "Iteration:1846;Percent complete:46.2%; Average loss:1375.5049\n",
            "Iteration:1847;Percent complete:46.2%; Average loss:1237.7250\n",
            "Iteration:1848;Percent complete:46.2%; Average loss:652.1387\n",
            "Iteration:1849;Percent complete:46.2%; Average loss:1415.1151\n",
            "Iteration:1850;Percent complete:46.2%; Average loss:714.6833\n",
            "Iteration:1851;Percent complete:46.3%; Average loss:1590.0561\n",
            "Iteration:1852;Percent complete:46.3%; Average loss:1491.3624\n",
            "Iteration:1853;Percent complete:46.3%; Average loss:1394.9329\n",
            "Iteration:1854;Percent complete:46.4%; Average loss:1352.2739\n",
            "Iteration:1855;Percent complete:46.4%; Average loss:1368.1307\n",
            "Iteration:1856;Percent complete:46.4%; Average loss:1340.3953\n",
            "Iteration:1857;Percent complete:46.4%; Average loss:1268.4016\n",
            "Iteration:1858;Percent complete:46.5%; Average loss:1411.8503\n",
            "Iteration:1859;Percent complete:46.5%; Average loss:1513.9438\n",
            "Iteration:1860;Percent complete:46.5%; Average loss:1281.4071\n",
            "Iteration:1861;Percent complete:46.5%; Average loss:1540.3069\n",
            "Iteration:1862;Percent complete:46.6%; Average loss:1430.9294\n",
            "Iteration:1863;Percent complete:46.6%; Average loss:1204.2228\n",
            "Iteration:1864;Percent complete:46.6%; Average loss:1498.2504\n",
            "Iteration:1865;Percent complete:46.6%; Average loss:1234.0102\n",
            "Iteration:1866;Percent complete:46.7%; Average loss:1298.3823\n",
            "Iteration:1867;Percent complete:46.7%; Average loss:1381.0638\n",
            "Iteration:1868;Percent complete:46.7%; Average loss:1402.3424\n",
            "Iteration:1869;Percent complete:46.7%; Average loss:1449.5541\n",
            "Iteration:1870;Percent complete:46.8%; Average loss:640.0156\n",
            "Iteration:1871;Percent complete:46.8%; Average loss:1363.2264\n",
            "Iteration:1872;Percent complete:46.8%; Average loss:1231.3236\n",
            "Iteration:1873;Percent complete:46.8%; Average loss:1118.9758\n",
            "Iteration:1874;Percent complete:46.9%; Average loss:718.4702\n",
            "Iteration:1875;Percent complete:46.9%; Average loss:1202.1468\n",
            "Iteration:1876;Percent complete:46.9%; Average loss:1430.5683\n",
            "Iteration:1877;Percent complete:46.9%; Average loss:1309.8786\n",
            "Iteration:1878;Percent complete:46.9%; Average loss:1278.7427\n",
            "Iteration:1879;Percent complete:47.0%; Average loss:1344.8377\n",
            "Iteration:1880;Percent complete:47.0%; Average loss:1321.5403\n",
            "Iteration:1881;Percent complete:47.0%; Average loss:1303.5524\n",
            "Iteration:1882;Percent complete:47.0%; Average loss:1453.2528\n",
            "Iteration:1883;Percent complete:47.1%; Average loss:1302.4036\n",
            "Iteration:1884;Percent complete:47.1%; Average loss:1367.1539\n",
            "Iteration:1885;Percent complete:47.1%; Average loss:1327.7227\n",
            "Iteration:1886;Percent complete:47.1%; Average loss:707.9961\n",
            "Iteration:1887;Percent complete:47.2%; Average loss:1311.9236\n",
            "Iteration:1888;Percent complete:47.2%; Average loss:800.6627\n",
            "Iteration:1889;Percent complete:47.2%; Average loss:1331.8517\n",
            "Iteration:1890;Percent complete:47.2%; Average loss:1435.8197\n",
            "Iteration:1891;Percent complete:47.3%; Average loss:667.9922\n",
            "Iteration:1892;Percent complete:47.3%; Average loss:1399.0959\n",
            "Iteration:1893;Percent complete:47.3%; Average loss:1345.3650\n",
            "Iteration:1894;Percent complete:47.3%; Average loss:1556.1675\n",
            "Iteration:1895;Percent complete:47.4%; Average loss:732.4916\n",
            "Iteration:1896;Percent complete:47.4%; Average loss:1427.3897\n",
            "Iteration:1897;Percent complete:47.4%; Average loss:1350.3617\n",
            "Iteration:1898;Percent complete:47.4%; Average loss:1305.4046\n",
            "Iteration:1899;Percent complete:47.5%; Average loss:785.2960\n",
            "Iteration:1900;Percent complete:47.5%; Average loss:1231.2642\n",
            "Iteration:1901;Percent complete:47.5%; Average loss:1460.4693\n",
            "Iteration:1902;Percent complete:47.5%; Average loss:1451.0431\n",
            "Iteration:1903;Percent complete:47.6%; Average loss:1360.3544\n",
            "Iteration:1904;Percent complete:47.6%; Average loss:1382.1465\n",
            "Iteration:1905;Percent complete:47.6%; Average loss:1240.2121\n",
            "Iteration:1906;Percent complete:47.6%; Average loss:1307.3062\n",
            "Iteration:1907;Percent complete:47.7%; Average loss:1512.5657\n",
            "Iteration:1908;Percent complete:47.7%; Average loss:1242.3645\n",
            "Iteration:1909;Percent complete:47.7%; Average loss:1329.2041\n",
            "Iteration:1910;Percent complete:47.8%; Average loss:1409.1923\n",
            "Iteration:1911;Percent complete:47.8%; Average loss:1552.1521\n",
            "Iteration:1912;Percent complete:47.8%; Average loss:1404.7729\n",
            "Iteration:1913;Percent complete:47.8%; Average loss:1334.5686\n",
            "Iteration:1914;Percent complete:47.9%; Average loss:1357.4335\n",
            "Iteration:1915;Percent complete:47.9%; Average loss:1561.3602\n",
            "Iteration:1916;Percent complete:47.9%; Average loss:1298.2193\n",
            "Iteration:1917;Percent complete:47.9%; Average loss:1392.2664\n",
            "Iteration:1918;Percent complete:47.9%; Average loss:1612.6206\n",
            "Iteration:1919;Percent complete:48.0%; Average loss:1164.9849\n",
            "Iteration:1920;Percent complete:48.0%; Average loss:1328.0682\n",
            "Iteration:1921;Percent complete:48.0%; Average loss:1309.0143\n",
            "Iteration:1922;Percent complete:48.0%; Average loss:1465.1536\n",
            "Iteration:1923;Percent complete:48.1%; Average loss:679.7642\n",
            "Iteration:1924;Percent complete:48.1%; Average loss:1366.0578\n",
            "Iteration:1925;Percent complete:48.1%; Average loss:1335.4066\n",
            "Iteration:1926;Percent complete:48.1%; Average loss:1298.8565\n",
            "Iteration:1927;Percent complete:48.2%; Average loss:659.5162\n",
            "Iteration:1928;Percent complete:48.2%; Average loss:1417.1255\n",
            "Iteration:1929;Percent complete:48.2%; Average loss:1209.2869\n",
            "Iteration:1930;Percent complete:48.2%; Average loss:1244.7577\n",
            "Iteration:1931;Percent complete:48.3%; Average loss:1375.5139\n",
            "Iteration:1932;Percent complete:48.3%; Average loss:1350.6041\n",
            "Iteration:1933;Percent complete:48.3%; Average loss:1399.9717\n",
            "Iteration:1934;Percent complete:48.4%; Average loss:1525.9072\n",
            "Iteration:1935;Percent complete:48.4%; Average loss:1432.9207\n",
            "Iteration:1936;Percent complete:48.4%; Average loss:1434.7847\n",
            "Iteration:1937;Percent complete:48.4%; Average loss:1263.4819\n",
            "Iteration:1938;Percent complete:48.4%; Average loss:1468.2114\n",
            "Iteration:1939;Percent complete:48.5%; Average loss:1232.9416\n",
            "Iteration:1940;Percent complete:48.5%; Average loss:1332.4885\n",
            "Iteration:1941;Percent complete:48.5%; Average loss:1456.5287\n",
            "Iteration:1942;Percent complete:48.5%; Average loss:1352.0457\n",
            "Iteration:1943;Percent complete:48.6%; Average loss:634.0299\n",
            "Iteration:1944;Percent complete:48.6%; Average loss:1220.0090\n",
            "Iteration:1945;Percent complete:48.6%; Average loss:468.8137\n",
            "Iteration:1946;Percent complete:48.6%; Average loss:1561.7322\n",
            "Iteration:1947;Percent complete:48.7%; Average loss:1335.8560\n",
            "Iteration:1948;Percent complete:48.7%; Average loss:1250.5127\n",
            "Iteration:1949;Percent complete:48.7%; Average loss:649.4860\n",
            "Iteration:1950;Percent complete:48.8%; Average loss:1512.8576\n",
            "Iteration:1951;Percent complete:48.8%; Average loss:790.9747\n",
            "Iteration:1952;Percent complete:48.8%; Average loss:860.0578\n",
            "Iteration:1953;Percent complete:48.8%; Average loss:1322.3293\n",
            "Iteration:1954;Percent complete:48.9%; Average loss:1411.5972\n",
            "Iteration:1955;Percent complete:48.9%; Average loss:1387.3918\n",
            "Iteration:1956;Percent complete:48.9%; Average loss:672.0673\n",
            "Iteration:1957;Percent complete:48.9%; Average loss:1447.9082\n",
            "Iteration:1958;Percent complete:48.9%; Average loss:1357.1620\n",
            "Iteration:1959;Percent complete:49.0%; Average loss:1500.2729\n",
            "Iteration:1960;Percent complete:49.0%; Average loss:1518.1925\n",
            "Iteration:1961;Percent complete:49.0%; Average loss:1317.7011\n",
            "Iteration:1962;Percent complete:49.0%; Average loss:1228.4789\n",
            "Iteration:1963;Percent complete:49.1%; Average loss:446.3863\n",
            "Iteration:1964;Percent complete:49.1%; Average loss:1444.4730\n",
            "Iteration:1965;Percent complete:49.1%; Average loss:1638.9875\n",
            "Iteration:1966;Percent complete:49.1%; Average loss:467.6095\n",
            "Iteration:1967;Percent complete:49.2%; Average loss:1431.5216\n",
            "Iteration:1968;Percent complete:49.2%; Average loss:705.5113\n",
            "Iteration:1969;Percent complete:49.2%; Average loss:1343.6223\n",
            "Iteration:1970;Percent complete:49.2%; Average loss:1572.9770\n",
            "Iteration:1971;Percent complete:49.3%; Average loss:690.3859\n",
            "Iteration:1972;Percent complete:49.3%; Average loss:1454.5566\n",
            "Iteration:1973;Percent complete:49.3%; Average loss:1262.0418\n",
            "Iteration:1974;Percent complete:49.4%; Average loss:1583.5933\n",
            "Iteration:1975;Percent complete:49.4%; Average loss:660.6533\n",
            "Iteration:1976;Percent complete:49.4%; Average loss:1575.0091\n",
            "Iteration:1977;Percent complete:49.4%; Average loss:1451.4999\n",
            "Iteration:1978;Percent complete:49.5%; Average loss:426.8112\n",
            "Iteration:1979;Percent complete:49.5%; Average loss:1275.3560\n",
            "Iteration:1980;Percent complete:49.5%; Average loss:1432.2325\n",
            "Iteration:1981;Percent complete:49.5%; Average loss:1305.4948\n",
            "Iteration:1982;Percent complete:49.5%; Average loss:1551.8211\n",
            "Iteration:1983;Percent complete:49.6%; Average loss:1542.7415\n",
            "Iteration:1984;Percent complete:49.6%; Average loss:1437.1583\n",
            "Iteration:1985;Percent complete:49.6%; Average loss:659.0940\n",
            "Iteration:1986;Percent complete:49.6%; Average loss:1608.3166\n",
            "Iteration:1987;Percent complete:49.7%; Average loss:1322.6441\n",
            "Iteration:1988;Percent complete:49.7%; Average loss:1481.0590\n",
            "Iteration:1989;Percent complete:49.7%; Average loss:689.9448\n",
            "Iteration:1990;Percent complete:49.8%; Average loss:1415.9990\n",
            "Iteration:1991;Percent complete:49.8%; Average loss:1431.5621\n",
            "Iteration:1992;Percent complete:49.8%; Average loss:1477.6475\n",
            "Iteration:1993;Percent complete:49.8%; Average loss:1378.2284\n",
            "Iteration:1994;Percent complete:49.9%; Average loss:1351.3082\n",
            "Iteration:1995;Percent complete:49.9%; Average loss:1274.2335\n",
            "Iteration:1996;Percent complete:49.9%; Average loss:1406.6928\n",
            "Iteration:1997;Percent complete:49.9%; Average loss:1485.7410\n",
            "Iteration:1998;Percent complete:50.0%; Average loss:1427.1440\n",
            "Iteration:1999;Percent complete:50.0%; Average loss:1343.9538\n",
            "Iteration:2000;Percent complete:50.0%; Average loss:1460.5852\n",
            "Iteration:2001;Percent complete:50.0%; Average loss:763.8966\n",
            "Iteration:2002;Percent complete:50.0%; Average loss:1354.2609\n",
            "Iteration:2003;Percent complete:50.1%; Average loss:1349.1912\n",
            "Iteration:2004;Percent complete:50.1%; Average loss:1158.0959\n",
            "Iteration:2005;Percent complete:50.1%; Average loss:1372.8219\n",
            "Iteration:2006;Percent complete:50.1%; Average loss:1236.6412\n",
            "Iteration:2007;Percent complete:50.2%; Average loss:1383.6656\n",
            "Iteration:2008;Percent complete:50.2%; Average loss:1284.9670\n",
            "Iteration:2009;Percent complete:50.2%; Average loss:1347.4615\n",
            "Iteration:2010;Percent complete:50.2%; Average loss:1530.7606\n",
            "Iteration:2011;Percent complete:50.3%; Average loss:1469.2626\n",
            "Iteration:2012;Percent complete:50.3%; Average loss:1091.1707\n",
            "Iteration:2013;Percent complete:50.3%; Average loss:1441.9450\n",
            "Iteration:2014;Percent complete:50.3%; Average loss:716.9839\n",
            "Iteration:2015;Percent complete:50.4%; Average loss:1455.0622\n",
            "Iteration:2016;Percent complete:50.4%; Average loss:1385.0744\n",
            "Iteration:2017;Percent complete:50.4%; Average loss:618.6357\n",
            "Iteration:2018;Percent complete:50.4%; Average loss:1333.2891\n",
            "Iteration:2019;Percent complete:50.5%; Average loss:1306.7430\n",
            "Iteration:2020;Percent complete:50.5%; Average loss:690.3254\n",
            "Iteration:2021;Percent complete:50.5%; Average loss:1324.3646\n",
            "Iteration:2022;Percent complete:50.5%; Average loss:1362.7209\n",
            "Iteration:2023;Percent complete:50.6%; Average loss:1405.2157\n",
            "Iteration:2024;Percent complete:50.6%; Average loss:1299.6458\n",
            "Iteration:2025;Percent complete:50.6%; Average loss:1486.9698\n",
            "Iteration:2026;Percent complete:50.6%; Average loss:1567.6276\n",
            "Iteration:2027;Percent complete:50.7%; Average loss:1195.4016\n",
            "Iteration:2028;Percent complete:50.7%; Average loss:1245.8270\n",
            "Iteration:2029;Percent complete:50.7%; Average loss:1303.3003\n",
            "Iteration:2030;Percent complete:50.7%; Average loss:737.8614\n",
            "Iteration:2031;Percent complete:50.8%; Average loss:739.3445\n",
            "Iteration:2032;Percent complete:50.8%; Average loss:1330.5992\n",
            "Iteration:2033;Percent complete:50.8%; Average loss:1391.8871\n",
            "Iteration:2034;Percent complete:50.8%; Average loss:1519.5063\n",
            "Iteration:2035;Percent complete:50.9%; Average loss:1372.2155\n",
            "Iteration:2036;Percent complete:50.9%; Average loss:1314.0063\n",
            "Iteration:2037;Percent complete:50.9%; Average loss:1281.3243\n",
            "Iteration:2038;Percent complete:50.9%; Average loss:738.2276\n",
            "Iteration:2039;Percent complete:51.0%; Average loss:1508.9141\n",
            "Iteration:2040;Percent complete:51.0%; Average loss:1458.4183\n",
            "Iteration:2041;Percent complete:51.0%; Average loss:727.4175\n",
            "Iteration:2042;Percent complete:51.0%; Average loss:463.6331\n",
            "Iteration:2043;Percent complete:51.1%; Average loss:1346.1182\n",
            "Iteration:2044;Percent complete:51.1%; Average loss:1491.7677\n",
            "Iteration:2045;Percent complete:51.1%; Average loss:724.9288\n",
            "Iteration:2046;Percent complete:51.1%; Average loss:1400.4022\n",
            "Iteration:2047;Percent complete:51.2%; Average loss:1487.7918\n",
            "Iteration:2048;Percent complete:51.2%; Average loss:401.3914\n",
            "Iteration:2049;Percent complete:51.2%; Average loss:1375.5678\n",
            "Iteration:2050;Percent complete:51.2%; Average loss:1385.5759\n",
            "Iteration:2051;Percent complete:51.3%; Average loss:600.6042\n",
            "Iteration:2052;Percent complete:51.3%; Average loss:1487.9884\n",
            "Iteration:2053;Percent complete:51.3%; Average loss:1370.7310\n",
            "Iteration:2054;Percent complete:51.3%; Average loss:515.5079\n",
            "Iteration:2055;Percent complete:51.4%; Average loss:1264.4324\n",
            "Iteration:2056;Percent complete:51.4%; Average loss:611.4599\n",
            "Iteration:2057;Percent complete:51.4%; Average loss:1333.9083\n",
            "Iteration:2058;Percent complete:51.4%; Average loss:1279.2380\n",
            "Iteration:2059;Percent complete:51.5%; Average loss:787.8436\n",
            "Iteration:2060;Percent complete:51.5%; Average loss:679.5307\n",
            "Iteration:2061;Percent complete:51.5%; Average loss:1275.3170\n",
            "Iteration:2062;Percent complete:51.5%; Average loss:1643.8569\n",
            "Iteration:2063;Percent complete:51.6%; Average loss:747.7651\n",
            "Iteration:2064;Percent complete:51.6%; Average loss:728.9111\n",
            "Iteration:2065;Percent complete:51.6%; Average loss:1193.6595\n",
            "Iteration:2066;Percent complete:51.6%; Average loss:728.3897\n",
            "Iteration:2067;Percent complete:51.7%; Average loss:466.6723\n",
            "Iteration:2068;Percent complete:51.7%; Average loss:1321.8859\n",
            "Iteration:2069;Percent complete:51.7%; Average loss:683.4019\n",
            "Iteration:2070;Percent complete:51.7%; Average loss:1462.4577\n",
            "Iteration:2071;Percent complete:51.8%; Average loss:687.9448\n",
            "Iteration:2072;Percent complete:51.8%; Average loss:1488.4751\n",
            "Iteration:2073;Percent complete:51.8%; Average loss:1127.5797\n",
            "Iteration:2074;Percent complete:51.8%; Average loss:1458.6412\n",
            "Iteration:2075;Percent complete:51.9%; Average loss:1418.8595\n",
            "Iteration:2076;Percent complete:51.9%; Average loss:765.0769\n",
            "Iteration:2077;Percent complete:51.9%; Average loss:428.5971\n",
            "Iteration:2078;Percent complete:51.9%; Average loss:1312.8974\n",
            "Iteration:2079;Percent complete:52.0%; Average loss:1295.8229\n",
            "Iteration:2080;Percent complete:52.0%; Average loss:1278.2456\n",
            "Iteration:2081;Percent complete:52.0%; Average loss:1322.4193\n",
            "Iteration:2082;Percent complete:52.0%; Average loss:1427.4564\n",
            "Iteration:2083;Percent complete:52.1%; Average loss:1250.3042\n",
            "Iteration:2084;Percent complete:52.1%; Average loss:1503.2783\n",
            "Iteration:2085;Percent complete:52.1%; Average loss:1501.8646\n",
            "Iteration:2086;Percent complete:52.1%; Average loss:1416.3541\n",
            "Iteration:2087;Percent complete:52.2%; Average loss:1454.4080\n",
            "Iteration:2088;Percent complete:52.2%; Average loss:1237.2324\n",
            "Iteration:2089;Percent complete:52.2%; Average loss:1209.1945\n",
            "Iteration:2090;Percent complete:52.2%; Average loss:1602.2971\n",
            "Iteration:2091;Percent complete:52.3%; Average loss:1508.8603\n",
            "Iteration:2092;Percent complete:52.3%; Average loss:1422.3583\n",
            "Iteration:2093;Percent complete:52.3%; Average loss:700.4141\n",
            "Iteration:2094;Percent complete:52.3%; Average loss:1273.5481\n",
            "Iteration:2095;Percent complete:52.4%; Average loss:1304.2196\n",
            "Iteration:2096;Percent complete:52.4%; Average loss:665.9557\n",
            "Iteration:2097;Percent complete:52.4%; Average loss:672.0888\n",
            "Iteration:2098;Percent complete:52.4%; Average loss:730.9877\n",
            "Iteration:2099;Percent complete:52.5%; Average loss:725.3823\n",
            "Iteration:2100;Percent complete:52.5%; Average loss:716.4506\n",
            "Iteration:2101;Percent complete:52.5%; Average loss:1479.2040\n",
            "Iteration:2102;Percent complete:52.5%; Average loss:1263.2892\n",
            "Iteration:2103;Percent complete:52.6%; Average loss:1273.6760\n",
            "Iteration:2104;Percent complete:52.6%; Average loss:1363.2076\n",
            "Iteration:2105;Percent complete:52.6%; Average loss:1186.9382\n",
            "Iteration:2106;Percent complete:52.6%; Average loss:1483.5250\n",
            "Iteration:2107;Percent complete:52.7%; Average loss:1217.6581\n",
            "Iteration:2108;Percent complete:52.7%; Average loss:1280.7220\n",
            "Iteration:2109;Percent complete:52.7%; Average loss:1227.5899\n",
            "Iteration:2110;Percent complete:52.8%; Average loss:1533.7070\n",
            "Iteration:2111;Percent complete:52.8%; Average loss:1487.1842\n",
            "Iteration:2112;Percent complete:52.8%; Average loss:1406.7988\n",
            "Iteration:2113;Percent complete:52.8%; Average loss:1210.4239\n",
            "Iteration:2114;Percent complete:52.8%; Average loss:1470.1656\n",
            "Iteration:2115;Percent complete:52.9%; Average loss:637.6747\n",
            "Iteration:2116;Percent complete:52.9%; Average loss:1341.4152\n",
            "Iteration:2117;Percent complete:52.9%; Average loss:659.0968\n",
            "Iteration:2118;Percent complete:52.9%; Average loss:1377.9917\n",
            "Iteration:2119;Percent complete:53.0%; Average loss:1502.4227\n",
            "Iteration:2120;Percent complete:53.0%; Average loss:1495.2006\n",
            "Iteration:2121;Percent complete:53.0%; Average loss:1331.8556\n",
            "Iteration:2122;Percent complete:53.0%; Average loss:1500.8421\n",
            "Iteration:2123;Percent complete:53.1%; Average loss:1143.0260\n",
            "Iteration:2124;Percent complete:53.1%; Average loss:727.1410\n",
            "Iteration:2125;Percent complete:53.1%; Average loss:1437.0940\n",
            "Iteration:2126;Percent complete:53.1%; Average loss:1486.7373\n",
            "Iteration:2127;Percent complete:53.2%; Average loss:1540.8080\n",
            "Iteration:2128;Percent complete:53.2%; Average loss:1351.3194\n",
            "Iteration:2129;Percent complete:53.2%; Average loss:718.8673\n",
            "Iteration:2130;Percent complete:53.2%; Average loss:1356.9957\n",
            "Iteration:2131;Percent complete:53.3%; Average loss:1351.4225\n",
            "Iteration:2132;Percent complete:53.3%; Average loss:1344.5891\n",
            "Iteration:2133;Percent complete:53.3%; Average loss:1434.2554\n",
            "Iteration:2134;Percent complete:53.3%; Average loss:1357.4749\n",
            "Iteration:2135;Percent complete:53.4%; Average loss:1182.3063\n",
            "Iteration:2136;Percent complete:53.4%; Average loss:1197.7003\n",
            "Iteration:2137;Percent complete:53.4%; Average loss:1407.9399\n",
            "Iteration:2138;Percent complete:53.4%; Average loss:1406.9184\n",
            "Iteration:2139;Percent complete:53.5%; Average loss:667.8155\n",
            "Iteration:2140;Percent complete:53.5%; Average loss:1535.2085\n",
            "Iteration:2141;Percent complete:53.5%; Average loss:1372.1678\n",
            "Iteration:2142;Percent complete:53.5%; Average loss:1345.1759\n",
            "Iteration:2143;Percent complete:53.6%; Average loss:1219.8556\n",
            "Iteration:2144;Percent complete:53.6%; Average loss:1225.6526\n",
            "Iteration:2145;Percent complete:53.6%; Average loss:1427.4418\n",
            "Iteration:2146;Percent complete:53.6%; Average loss:1364.2381\n",
            "Iteration:2147;Percent complete:53.7%; Average loss:1415.0505\n",
            "Iteration:2148;Percent complete:53.7%; Average loss:1295.6376\n",
            "Iteration:2149;Percent complete:53.7%; Average loss:711.9293\n",
            "Iteration:2150;Percent complete:53.8%; Average loss:1343.0019\n",
            "Iteration:2151;Percent complete:53.8%; Average loss:721.7847\n",
            "Iteration:2152;Percent complete:53.8%; Average loss:482.9119\n",
            "Iteration:2153;Percent complete:53.8%; Average loss:1442.9794\n",
            "Iteration:2154;Percent complete:53.8%; Average loss:1385.4728\n",
            "Iteration:2155;Percent complete:53.9%; Average loss:1295.9517\n",
            "Iteration:2156;Percent complete:53.9%; Average loss:282.3839\n",
            "Iteration:2157;Percent complete:53.9%; Average loss:1381.1749\n",
            "Iteration:2158;Percent complete:53.9%; Average loss:1480.5464\n",
            "Iteration:2159;Percent complete:54.0%; Average loss:1448.4776\n",
            "Iteration:2160;Percent complete:54.0%; Average loss:723.1806\n",
            "Iteration:2161;Percent complete:54.0%; Average loss:1449.3479\n",
            "Iteration:2162;Percent complete:54.0%; Average loss:703.4774\n",
            "Iteration:2163;Percent complete:54.1%; Average loss:1421.1630\n",
            "Iteration:2164;Percent complete:54.1%; Average loss:1394.3389\n",
            "Iteration:2165;Percent complete:54.1%; Average loss:1357.0105\n",
            "Iteration:2166;Percent complete:54.1%; Average loss:1513.4438\n",
            "Iteration:2167;Percent complete:54.2%; Average loss:1264.4105\n",
            "Iteration:2168;Percent complete:54.2%; Average loss:697.4509\n",
            "Iteration:2169;Percent complete:54.2%; Average loss:1350.3595\n",
            "Iteration:2170;Percent complete:54.2%; Average loss:1482.8065\n",
            "Iteration:2171;Percent complete:54.3%; Average loss:1105.4165\n",
            "Iteration:2172;Percent complete:54.3%; Average loss:1246.4960\n",
            "Iteration:2173;Percent complete:54.3%; Average loss:577.8982\n",
            "Iteration:2174;Percent complete:54.4%; Average loss:1351.0672\n",
            "Iteration:2175;Percent complete:54.4%; Average loss:1125.3259\n",
            "Iteration:2176;Percent complete:54.4%; Average loss:1327.2283\n",
            "Iteration:2177;Percent complete:54.4%; Average loss:1439.6334\n",
            "Iteration:2178;Percent complete:54.4%; Average loss:1633.9360\n",
            "Iteration:2179;Percent complete:54.5%; Average loss:1348.4191\n",
            "Iteration:2180;Percent complete:54.5%; Average loss:1163.7770\n",
            "Iteration:2181;Percent complete:54.5%; Average loss:1338.8750\n",
            "Iteration:2182;Percent complete:54.5%; Average loss:1355.2578\n",
            "Iteration:2183;Percent complete:54.6%; Average loss:1227.1233\n",
            "Iteration:2184;Percent complete:54.6%; Average loss:1391.3761\n",
            "Iteration:2185;Percent complete:54.6%; Average loss:1348.2065\n",
            "Iteration:2186;Percent complete:54.6%; Average loss:1395.1457\n",
            "Iteration:2187;Percent complete:54.7%; Average loss:1240.8825\n",
            "Iteration:2188;Percent complete:54.7%; Average loss:586.5008\n",
            "Iteration:2189;Percent complete:54.7%; Average loss:710.0633\n",
            "Iteration:2190;Percent complete:54.8%; Average loss:1501.7946\n",
            "Iteration:2191;Percent complete:54.8%; Average loss:724.5740\n",
            "Iteration:2192;Percent complete:54.8%; Average loss:1333.7068\n",
            "Iteration:2193;Percent complete:54.8%; Average loss:1335.6041\n",
            "Iteration:2194;Percent complete:54.9%; Average loss:710.5337\n",
            "Iteration:2195;Percent complete:54.9%; Average loss:1699.6330\n",
            "Iteration:2196;Percent complete:54.9%; Average loss:1434.3824\n",
            "Iteration:2197;Percent complete:54.9%; Average loss:680.5410\n",
            "Iteration:2198;Percent complete:54.9%; Average loss:1315.6351\n",
            "Iteration:2199;Percent complete:55.0%; Average loss:1384.2045\n",
            "Iteration:2200;Percent complete:55.0%; Average loss:716.7971\n",
            "Iteration:2201;Percent complete:55.0%; Average loss:672.6489\n",
            "Iteration:2202;Percent complete:55.0%; Average loss:1370.4316\n",
            "Iteration:2203;Percent complete:55.1%; Average loss:775.6471\n",
            "Iteration:2204;Percent complete:55.1%; Average loss:710.0581\n",
            "Iteration:2205;Percent complete:55.1%; Average loss:440.5440\n",
            "Iteration:2206;Percent complete:55.1%; Average loss:627.6985\n",
            "Iteration:2207;Percent complete:55.2%; Average loss:1239.9657\n",
            "Iteration:2208;Percent complete:55.2%; Average loss:1410.6634\n",
            "Iteration:2209;Percent complete:55.2%; Average loss:584.1529\n",
            "Iteration:2210;Percent complete:55.2%; Average loss:1423.2175\n",
            "Iteration:2211;Percent complete:55.3%; Average loss:664.6655\n",
            "Iteration:2212;Percent complete:55.3%; Average loss:1398.8701\n",
            "Iteration:2213;Percent complete:55.3%; Average loss:444.0402\n",
            "Iteration:2214;Percent complete:55.4%; Average loss:1696.1872\n",
            "Iteration:2215;Percent complete:55.4%; Average loss:1376.4688\n",
            "Iteration:2216;Percent complete:55.4%; Average loss:741.1144\n",
            "Iteration:2217;Percent complete:55.4%; Average loss:1498.7018\n",
            "Iteration:2218;Percent complete:55.5%; Average loss:467.9448\n",
            "Iteration:2219;Percent complete:55.5%; Average loss:1503.6702\n",
            "Iteration:2220;Percent complete:55.5%; Average loss:1217.7055\n",
            "Iteration:2221;Percent complete:55.5%; Average loss:1442.0096\n",
            "Iteration:2222;Percent complete:55.5%; Average loss:1465.5348\n",
            "Iteration:2223;Percent complete:55.6%; Average loss:1534.2677\n",
            "Iteration:2224;Percent complete:55.6%; Average loss:1542.7161\n",
            "Iteration:2225;Percent complete:55.6%; Average loss:775.3057\n",
            "Iteration:2226;Percent complete:55.6%; Average loss:741.2380\n",
            "Iteration:2227;Percent complete:55.7%; Average loss:1237.7483\n",
            "Iteration:2228;Percent complete:55.7%; Average loss:1480.7252\n",
            "Iteration:2229;Percent complete:55.7%; Average loss:1292.4630\n",
            "Iteration:2230;Percent complete:55.8%; Average loss:1583.8499\n",
            "Iteration:2231;Percent complete:55.8%; Average loss:1420.3129\n",
            "Iteration:2232;Percent complete:55.8%; Average loss:1110.9972\n",
            "Iteration:2233;Percent complete:55.8%; Average loss:761.5149\n",
            "Iteration:2234;Percent complete:55.9%; Average loss:1523.3690\n",
            "Iteration:2235;Percent complete:55.9%; Average loss:1243.4105\n",
            "Iteration:2236;Percent complete:55.9%; Average loss:1425.4957\n",
            "Iteration:2237;Percent complete:55.9%; Average loss:1386.7101\n",
            "Iteration:2238;Percent complete:56.0%; Average loss:1380.6319\n",
            "Iteration:2239;Percent complete:56.0%; Average loss:1251.9831\n",
            "Iteration:2240;Percent complete:56.0%; Average loss:1439.5964\n",
            "Iteration:2241;Percent complete:56.0%; Average loss:1515.0009\n",
            "Iteration:2242;Percent complete:56.0%; Average loss:1496.1071\n",
            "Iteration:2243;Percent complete:56.1%; Average loss:403.7805\n",
            "Iteration:2244;Percent complete:56.1%; Average loss:1535.5797\n",
            "Iteration:2245;Percent complete:56.1%; Average loss:1320.0780\n",
            "Iteration:2246;Percent complete:56.1%; Average loss:451.5646\n",
            "Iteration:2247;Percent complete:56.2%; Average loss:1281.2125\n",
            "Iteration:2248;Percent complete:56.2%; Average loss:1526.1587\n",
            "Iteration:2249;Percent complete:56.2%; Average loss:785.0646\n",
            "Iteration:2250;Percent complete:56.2%; Average loss:1397.1454\n",
            "Iteration:2251;Percent complete:56.3%; Average loss:461.7602\n",
            "Iteration:2252;Percent complete:56.3%; Average loss:1529.8278\n",
            "Iteration:2253;Percent complete:56.3%; Average loss:1455.6036\n",
            "Iteration:2254;Percent complete:56.4%; Average loss:1252.6445\n",
            "Iteration:2255;Percent complete:56.4%; Average loss:1274.8453\n",
            "Iteration:2256;Percent complete:56.4%; Average loss:759.9880\n",
            "Iteration:2257;Percent complete:56.4%; Average loss:1355.2431\n",
            "Iteration:2258;Percent complete:56.5%; Average loss:1297.2970\n",
            "Iteration:2259;Percent complete:56.5%; Average loss:1264.6882\n",
            "Iteration:2260;Percent complete:56.5%; Average loss:1500.3771\n",
            "Iteration:2261;Percent complete:56.5%; Average loss:1168.0079\n",
            "Iteration:2262;Percent complete:56.5%; Average loss:1153.4089\n",
            "Iteration:2263;Percent complete:56.6%; Average loss:1423.9570\n",
            "Iteration:2264;Percent complete:56.6%; Average loss:1173.8891\n",
            "Iteration:2265;Percent complete:56.6%; Average loss:1325.1394\n",
            "Iteration:2266;Percent complete:56.6%; Average loss:1326.6641\n",
            "Iteration:2267;Percent complete:56.7%; Average loss:474.2594\n",
            "Iteration:2268;Percent complete:56.7%; Average loss:1437.6191\n",
            "Iteration:2269;Percent complete:56.7%; Average loss:356.7879\n",
            "Iteration:2270;Percent complete:56.8%; Average loss:745.8117\n",
            "Iteration:2271;Percent complete:56.8%; Average loss:659.1927\n",
            "Iteration:2272;Percent complete:56.8%; Average loss:667.1018\n",
            "Iteration:2273;Percent complete:56.8%; Average loss:1451.1197\n",
            "Iteration:2274;Percent complete:56.9%; Average loss:781.3135\n",
            "Iteration:2275;Percent complete:56.9%; Average loss:1509.6068\n",
            "Iteration:2276;Percent complete:56.9%; Average loss:1131.6786\n",
            "Iteration:2277;Percent complete:56.9%; Average loss:1433.3510\n",
            "Iteration:2278;Percent complete:57.0%; Average loss:1514.9496\n",
            "Iteration:2279;Percent complete:57.0%; Average loss:1534.6534\n",
            "Iteration:2280;Percent complete:57.0%; Average loss:1306.9716\n",
            "Iteration:2281;Percent complete:57.0%; Average loss:699.6022\n",
            "Iteration:2282;Percent complete:57.0%; Average loss:1161.7290\n",
            "Iteration:2283;Percent complete:57.1%; Average loss:693.5921\n",
            "Iteration:2284;Percent complete:57.1%; Average loss:1332.7779\n",
            "Iteration:2285;Percent complete:57.1%; Average loss:808.6642\n",
            "Iteration:2286;Percent complete:57.1%; Average loss:765.0551\n",
            "Iteration:2287;Percent complete:57.2%; Average loss:1324.3345\n",
            "Iteration:2288;Percent complete:57.2%; Average loss:1260.5564\n",
            "Iteration:2289;Percent complete:57.2%; Average loss:1327.1911\n",
            "Iteration:2290;Percent complete:57.2%; Average loss:1389.6684\n",
            "Iteration:2291;Percent complete:57.3%; Average loss:1230.6037\n",
            "Iteration:2292;Percent complete:57.3%; Average loss:695.2214\n",
            "Iteration:2293;Percent complete:57.3%; Average loss:1369.6267\n",
            "Iteration:2294;Percent complete:57.4%; Average loss:618.5041\n",
            "Iteration:2295;Percent complete:57.4%; Average loss:1278.5790\n",
            "Iteration:2296;Percent complete:57.4%; Average loss:1394.2976\n",
            "Iteration:2297;Percent complete:57.4%; Average loss:1310.4100\n",
            "Iteration:2298;Percent complete:57.5%; Average loss:1589.7277\n",
            "Iteration:2299;Percent complete:57.5%; Average loss:467.0102\n",
            "Iteration:2300;Percent complete:57.5%; Average loss:1430.4194\n",
            "Iteration:2301;Percent complete:57.5%; Average loss:1745.3733\n",
            "Iteration:2302;Percent complete:57.6%; Average loss:1402.1570\n",
            "Iteration:2303;Percent complete:57.6%; Average loss:789.6373\n",
            "Iteration:2304;Percent complete:57.6%; Average loss:1469.1549\n",
            "Iteration:2305;Percent complete:57.6%; Average loss:1247.3889\n",
            "Iteration:2306;Percent complete:57.6%; Average loss:1336.7094\n",
            "Iteration:2307;Percent complete:57.7%; Average loss:1372.5880\n",
            "Iteration:2308;Percent complete:57.7%; Average loss:1431.0312\n",
            "Iteration:2309;Percent complete:57.7%; Average loss:686.7381\n",
            "Iteration:2310;Percent complete:57.8%; Average loss:1323.2254\n",
            "Iteration:2311;Percent complete:57.8%; Average loss:771.1171\n",
            "Iteration:2312;Percent complete:57.8%; Average loss:1450.6526\n",
            "Iteration:2313;Percent complete:57.8%; Average loss:1191.1442\n",
            "Iteration:2314;Percent complete:57.9%; Average loss:584.1442\n",
            "Iteration:2315;Percent complete:57.9%; Average loss:1465.4729\n",
            "Iteration:2316;Percent complete:57.9%; Average loss:1370.4658\n",
            "Iteration:2317;Percent complete:57.9%; Average loss:1348.0430\n",
            "Iteration:2318;Percent complete:58.0%; Average loss:790.5339\n",
            "Iteration:2319;Percent complete:58.0%; Average loss:1258.9168\n",
            "Iteration:2320;Percent complete:58.0%; Average loss:1300.7515\n",
            "Iteration:2321;Percent complete:58.0%; Average loss:1504.7732\n",
            "Iteration:2322;Percent complete:58.1%; Average loss:1670.6648\n",
            "Iteration:2323;Percent complete:58.1%; Average loss:1388.8374\n",
            "Iteration:2324;Percent complete:58.1%; Average loss:1378.8653\n",
            "Iteration:2325;Percent complete:58.1%; Average loss:1197.5430\n",
            "Iteration:2326;Percent complete:58.1%; Average loss:1450.2832\n",
            "Iteration:2327;Percent complete:58.2%; Average loss:1372.2313\n",
            "Iteration:2328;Percent complete:58.2%; Average loss:1358.2754\n",
            "Iteration:2329;Percent complete:58.2%; Average loss:1380.6000\n",
            "Iteration:2330;Percent complete:58.2%; Average loss:746.4734\n",
            "Iteration:2331;Percent complete:58.3%; Average loss:1458.7214\n",
            "Iteration:2332;Percent complete:58.3%; Average loss:593.1961\n",
            "Iteration:2333;Percent complete:58.3%; Average loss:1168.2367\n",
            "Iteration:2334;Percent complete:58.4%; Average loss:1372.3495\n",
            "Iteration:2335;Percent complete:58.4%; Average loss:1599.2405\n",
            "Iteration:2336;Percent complete:58.4%; Average loss:1446.2028\n",
            "Iteration:2337;Percent complete:58.4%; Average loss:1351.2965\n",
            "Iteration:2338;Percent complete:58.5%; Average loss:704.4293\n",
            "Iteration:2339;Percent complete:58.5%; Average loss:1219.9814\n",
            "Iteration:2340;Percent complete:58.5%; Average loss:1370.2934\n",
            "Iteration:2341;Percent complete:58.5%; Average loss:1480.7727\n",
            "Iteration:2342;Percent complete:58.6%; Average loss:326.8066\n",
            "Iteration:2343;Percent complete:58.6%; Average loss:1364.3743\n",
            "Iteration:2344;Percent complete:58.6%; Average loss:1236.6941\n",
            "Iteration:2345;Percent complete:58.6%; Average loss:690.3785\n",
            "Iteration:2346;Percent complete:58.7%; Average loss:465.2211\n",
            "Iteration:2347;Percent complete:58.7%; Average loss:1576.5865\n",
            "Iteration:2348;Percent complete:58.7%; Average loss:1190.6781\n",
            "Iteration:2349;Percent complete:58.7%; Average loss:699.8527\n",
            "Iteration:2350;Percent complete:58.8%; Average loss:1357.9303\n",
            "Iteration:2351;Percent complete:58.8%; Average loss:1442.6473\n",
            "Iteration:2352;Percent complete:58.8%; Average loss:1278.7821\n",
            "Iteration:2353;Percent complete:58.8%; Average loss:1454.5497\n",
            "Iteration:2354;Percent complete:58.9%; Average loss:1333.3486\n",
            "Iteration:2355;Percent complete:58.9%; Average loss:1299.9221\n",
            "Iteration:2356;Percent complete:58.9%; Average loss:629.2159\n",
            "Iteration:2357;Percent complete:58.9%; Average loss:1487.6547\n",
            "Iteration:2358;Percent complete:59.0%; Average loss:1632.4341\n",
            "Iteration:2359;Percent complete:59.0%; Average loss:655.2865\n",
            "Iteration:2360;Percent complete:59.0%; Average loss:711.6769\n",
            "Iteration:2361;Percent complete:59.0%; Average loss:1303.4985\n",
            "Iteration:2362;Percent complete:59.1%; Average loss:1304.4802\n",
            "Iteration:2363;Percent complete:59.1%; Average loss:633.1950\n",
            "Iteration:2364;Percent complete:59.1%; Average loss:1318.6605\n",
            "Iteration:2365;Percent complete:59.1%; Average loss:1589.7238\n",
            "Iteration:2366;Percent complete:59.2%; Average loss:791.2592\n",
            "Iteration:2367;Percent complete:59.2%; Average loss:1385.4368\n",
            "Iteration:2368;Percent complete:59.2%; Average loss:643.5004\n",
            "Iteration:2369;Percent complete:59.2%; Average loss:1503.6053\n",
            "Iteration:2370;Percent complete:59.2%; Average loss:1550.8668\n",
            "Iteration:2371;Percent complete:59.3%; Average loss:1479.1931\n",
            "Iteration:2372;Percent complete:59.3%; Average loss:1304.6222\n",
            "Iteration:2373;Percent complete:59.3%; Average loss:1493.0915\n",
            "Iteration:2374;Percent complete:59.4%; Average loss:1360.1503\n",
            "Iteration:2375;Percent complete:59.4%; Average loss:1361.5070\n",
            "Iteration:2376;Percent complete:59.4%; Average loss:1295.5164\n",
            "Iteration:2377;Percent complete:59.4%; Average loss:1369.7607\n",
            "Iteration:2378;Percent complete:59.5%; Average loss:1498.8502\n",
            "Iteration:2379;Percent complete:59.5%; Average loss:425.8826\n",
            "Iteration:2380;Percent complete:59.5%; Average loss:1190.2289\n",
            "Iteration:2381;Percent complete:59.5%; Average loss:1611.8384\n",
            "Iteration:2382;Percent complete:59.6%; Average loss:1348.9536\n",
            "Iteration:2383;Percent complete:59.6%; Average loss:1353.7551\n",
            "Iteration:2384;Percent complete:59.6%; Average loss:692.5880\n",
            "Iteration:2385;Percent complete:59.6%; Average loss:1249.5855\n",
            "Iteration:2386;Percent complete:59.7%; Average loss:1286.5345\n",
            "Iteration:2387;Percent complete:59.7%; Average loss:1378.5390\n",
            "Iteration:2388;Percent complete:59.7%; Average loss:1361.7856\n",
            "Iteration:2389;Percent complete:59.7%; Average loss:1356.9337\n",
            "Iteration:2390;Percent complete:59.8%; Average loss:1338.8574\n",
            "Iteration:2391;Percent complete:59.8%; Average loss:1217.0840\n",
            "Iteration:2392;Percent complete:59.8%; Average loss:659.4513\n",
            "Iteration:2393;Percent complete:59.8%; Average loss:1567.8320\n",
            "Iteration:2394;Percent complete:59.9%; Average loss:648.8246\n",
            "Iteration:2395;Percent complete:59.9%; Average loss:1449.2100\n",
            "Iteration:2396;Percent complete:59.9%; Average loss:655.5439\n",
            "Iteration:2397;Percent complete:59.9%; Average loss:1273.2573\n",
            "Iteration:2398;Percent complete:60.0%; Average loss:1469.8934\n",
            "Iteration:2399;Percent complete:60.0%; Average loss:523.4861\n",
            "Iteration:2400;Percent complete:60.0%; Average loss:1365.5593\n",
            "Iteration:2401;Percent complete:60.0%; Average loss:1268.5238\n",
            "Iteration:2402;Percent complete:60.1%; Average loss:620.7856\n",
            "Iteration:2403;Percent complete:60.1%; Average loss:686.5505\n",
            "Iteration:2404;Percent complete:60.1%; Average loss:1451.8574\n",
            "Iteration:2405;Percent complete:60.1%; Average loss:1307.8258\n",
            "Iteration:2406;Percent complete:60.2%; Average loss:1470.2443\n",
            "Iteration:2407;Percent complete:60.2%; Average loss:1308.6737\n",
            "Iteration:2408;Percent complete:60.2%; Average loss:1446.2636\n",
            "Iteration:2409;Percent complete:60.2%; Average loss:1390.2683\n",
            "Iteration:2410;Percent complete:60.2%; Average loss:1526.4603\n",
            "Iteration:2411;Percent complete:60.3%; Average loss:1348.3871\n",
            "Iteration:2412;Percent complete:60.3%; Average loss:1371.4032\n",
            "Iteration:2413;Percent complete:60.3%; Average loss:1326.2458\n",
            "Iteration:2414;Percent complete:60.4%; Average loss:1308.9525\n",
            "Iteration:2415;Percent complete:60.4%; Average loss:1399.0796\n",
            "Iteration:2416;Percent complete:60.4%; Average loss:1420.3124\n",
            "Iteration:2417;Percent complete:60.4%; Average loss:1327.3287\n",
            "Iteration:2418;Percent complete:60.5%; Average loss:1213.8464\n",
            "Iteration:2419;Percent complete:60.5%; Average loss:1635.5388\n",
            "Iteration:2420;Percent complete:60.5%; Average loss:1446.8081\n",
            "Iteration:2421;Percent complete:60.5%; Average loss:1326.7940\n",
            "Iteration:2422;Percent complete:60.6%; Average loss:614.1650\n",
            "Iteration:2423;Percent complete:60.6%; Average loss:1256.8074\n",
            "Iteration:2424;Percent complete:60.6%; Average loss:1342.2982\n",
            "Iteration:2425;Percent complete:60.6%; Average loss:1341.9220\n",
            "Iteration:2426;Percent complete:60.7%; Average loss:1391.5734\n",
            "Iteration:2427;Percent complete:60.7%; Average loss:1321.8980\n",
            "Iteration:2428;Percent complete:60.7%; Average loss:1427.2067\n",
            "Iteration:2429;Percent complete:60.7%; Average loss:684.0618\n",
            "Iteration:2430;Percent complete:60.8%; Average loss:1593.7505\n",
            "Iteration:2431;Percent complete:60.8%; Average loss:1428.2580\n",
            "Iteration:2432;Percent complete:60.8%; Average loss:1274.1790\n",
            "Iteration:2433;Percent complete:60.8%; Average loss:1501.7626\n",
            "Iteration:2434;Percent complete:60.9%; Average loss:1338.4810\n",
            "Iteration:2435;Percent complete:60.9%; Average loss:1387.8578\n",
            "Iteration:2436;Percent complete:60.9%; Average loss:1312.5208\n",
            "Iteration:2437;Percent complete:60.9%; Average loss:1518.8557\n",
            "Iteration:2438;Percent complete:61.0%; Average loss:1440.3862\n",
            "Iteration:2439;Percent complete:61.0%; Average loss:816.6793\n",
            "Iteration:2440;Percent complete:61.0%; Average loss:1442.7735\n",
            "Iteration:2441;Percent complete:61.0%; Average loss:1458.4040\n",
            "Iteration:2442;Percent complete:61.1%; Average loss:1220.2443\n",
            "Iteration:2443;Percent complete:61.1%; Average loss:657.7076\n",
            "Iteration:2444;Percent complete:61.1%; Average loss:1243.7923\n",
            "Iteration:2445;Percent complete:61.1%; Average loss:1415.9336\n",
            "Iteration:2446;Percent complete:61.2%; Average loss:1319.8586\n",
            "Iteration:2447;Percent complete:61.2%; Average loss:1498.5896\n",
            "Iteration:2448;Percent complete:61.2%; Average loss:1397.4892\n",
            "Iteration:2449;Percent complete:61.2%; Average loss:1408.3389\n",
            "Iteration:2450;Percent complete:61.3%; Average loss:1385.6196\n",
            "Iteration:2451;Percent complete:61.3%; Average loss:1265.5169\n",
            "Iteration:2452;Percent complete:61.3%; Average loss:1269.0505\n",
            "Iteration:2453;Percent complete:61.3%; Average loss:1359.2762\n",
            "Iteration:2454;Percent complete:61.4%; Average loss:1546.6174\n",
            "Iteration:2455;Percent complete:61.4%; Average loss:1444.3793\n",
            "Iteration:2456;Percent complete:61.4%; Average loss:1479.5287\n",
            "Iteration:2457;Percent complete:61.4%; Average loss:665.6242\n",
            "Iteration:2458;Percent complete:61.5%; Average loss:1332.7307\n",
            "Iteration:2459;Percent complete:61.5%; Average loss:707.2659\n",
            "Iteration:2460;Percent complete:61.5%; Average loss:1488.5582\n",
            "Iteration:2461;Percent complete:61.5%; Average loss:1476.3125\n",
            "Iteration:2462;Percent complete:61.6%; Average loss:709.5515\n",
            "Iteration:2463;Percent complete:61.6%; Average loss:1421.6242\n",
            "Iteration:2464;Percent complete:61.6%; Average loss:1230.0769\n",
            "Iteration:2465;Percent complete:61.6%; Average loss:777.2796\n",
            "Iteration:2466;Percent complete:61.7%; Average loss:1603.7236\n",
            "Iteration:2467;Percent complete:61.7%; Average loss:763.1656\n",
            "Iteration:2468;Percent complete:61.7%; Average loss:1416.4359\n",
            "Iteration:2469;Percent complete:61.7%; Average loss:1561.9605\n",
            "Iteration:2470;Percent complete:61.8%; Average loss:1337.5681\n",
            "Iteration:2471;Percent complete:61.8%; Average loss:1557.2410\n",
            "Iteration:2472;Percent complete:61.8%; Average loss:1354.3311\n",
            "Iteration:2473;Percent complete:61.8%; Average loss:1511.0258\n",
            "Iteration:2474;Percent complete:61.9%; Average loss:1364.7562\n",
            "Iteration:2475;Percent complete:61.9%; Average loss:1443.9552\n",
            "Iteration:2476;Percent complete:61.9%; Average loss:1294.9801\n",
            "Iteration:2477;Percent complete:61.9%; Average loss:595.7750\n",
            "Iteration:2478;Percent complete:62.0%; Average loss:1442.2565\n",
            "Iteration:2479;Percent complete:62.0%; Average loss:1301.2063\n",
            "Iteration:2480;Percent complete:62.0%; Average loss:1433.2581\n",
            "Iteration:2481;Percent complete:62.0%; Average loss:734.7551\n",
            "Iteration:2482;Percent complete:62.1%; Average loss:694.0270\n",
            "Iteration:2483;Percent complete:62.1%; Average loss:1278.3004\n",
            "Iteration:2484;Percent complete:62.1%; Average loss:1580.8481\n",
            "Iteration:2485;Percent complete:62.1%; Average loss:715.2609\n",
            "Iteration:2486;Percent complete:62.2%; Average loss:632.6711\n",
            "Iteration:2487;Percent complete:62.2%; Average loss:633.0520\n",
            "Iteration:2488;Percent complete:62.2%; Average loss:1424.7345\n",
            "Iteration:2489;Percent complete:62.2%; Average loss:1442.4547\n",
            "Iteration:2490;Percent complete:62.3%; Average loss:661.4796\n",
            "Iteration:2491;Percent complete:62.3%; Average loss:1335.8930\n",
            "Iteration:2492;Percent complete:62.3%; Average loss:653.9711\n",
            "Iteration:2493;Percent complete:62.3%; Average loss:1180.2307\n",
            "Iteration:2494;Percent complete:62.4%; Average loss:1386.5819\n",
            "Iteration:2495;Percent complete:62.4%; Average loss:1298.0985\n",
            "Iteration:2496;Percent complete:62.4%; Average loss:1388.9486\n",
            "Iteration:2497;Percent complete:62.4%; Average loss:677.4432\n",
            "Iteration:2498;Percent complete:62.5%; Average loss:1351.1711\n",
            "Iteration:2499;Percent complete:62.5%; Average loss:1490.1735\n",
            "Iteration:2500;Percent complete:62.5%; Average loss:1405.2615\n",
            "Iteration:2501;Percent complete:62.5%; Average loss:1507.7897\n",
            "Iteration:2502;Percent complete:62.5%; Average loss:1318.3365\n",
            "Iteration:2503;Percent complete:62.6%; Average loss:645.4462\n",
            "Iteration:2504;Percent complete:62.6%; Average loss:1275.8851\n",
            "Iteration:2505;Percent complete:62.6%; Average loss:1328.1102\n",
            "Iteration:2506;Percent complete:62.6%; Average loss:1476.0003\n",
            "Iteration:2507;Percent complete:62.7%; Average loss:1370.7191\n",
            "Iteration:2508;Percent complete:62.7%; Average loss:1406.6621\n",
            "Iteration:2509;Percent complete:62.7%; Average loss:693.8854\n",
            "Iteration:2510;Percent complete:62.7%; Average loss:753.8236\n",
            "Iteration:2511;Percent complete:62.8%; Average loss:662.4677\n",
            "Iteration:2512;Percent complete:62.8%; Average loss:691.3538\n",
            "Iteration:2513;Percent complete:62.8%; Average loss:1213.3255\n",
            "Iteration:2514;Percent complete:62.8%; Average loss:1288.6466\n",
            "Iteration:2515;Percent complete:62.9%; Average loss:1173.7276\n",
            "Iteration:2516;Percent complete:62.9%; Average loss:1524.0703\n",
            "Iteration:2517;Percent complete:62.9%; Average loss:1351.8524\n",
            "Iteration:2518;Percent complete:62.9%; Average loss:1438.7441\n",
            "Iteration:2519;Percent complete:63.0%; Average loss:681.8964\n",
            "Iteration:2520;Percent complete:63.0%; Average loss:1436.3021\n",
            "Iteration:2521;Percent complete:63.0%; Average loss:1443.2210\n",
            "Iteration:2522;Percent complete:63.0%; Average loss:1319.3298\n",
            "Iteration:2523;Percent complete:63.1%; Average loss:1534.8990\n",
            "Iteration:2524;Percent complete:63.1%; Average loss:1591.6932\n",
            "Iteration:2525;Percent complete:63.1%; Average loss:1438.2395\n",
            "Iteration:2526;Percent complete:63.1%; Average loss:759.0674\n",
            "Iteration:2527;Percent complete:63.2%; Average loss:1278.7762\n",
            "Iteration:2528;Percent complete:63.2%; Average loss:1236.6643\n",
            "Iteration:2529;Percent complete:63.2%; Average loss:671.1998\n",
            "Iteration:2530;Percent complete:63.2%; Average loss:781.6946\n",
            "Iteration:2531;Percent complete:63.3%; Average loss:1463.5800\n",
            "Iteration:2532;Percent complete:63.3%; Average loss:1286.3770\n",
            "Iteration:2533;Percent complete:63.3%; Average loss:1328.5803\n",
            "Iteration:2534;Percent complete:63.3%; Average loss:1434.5815\n",
            "Iteration:2535;Percent complete:63.4%; Average loss:1438.4214\n",
            "Iteration:2536;Percent complete:63.4%; Average loss:1518.3637\n",
            "Iteration:2537;Percent complete:63.4%; Average loss:1397.7005\n",
            "Iteration:2538;Percent complete:63.4%; Average loss:726.2549\n",
            "Iteration:2539;Percent complete:63.5%; Average loss:1479.3745\n",
            "Iteration:2540;Percent complete:63.5%; Average loss:1297.9477\n",
            "Iteration:2541;Percent complete:63.5%; Average loss:1348.5655\n",
            "Iteration:2542;Percent complete:63.5%; Average loss:1222.1223\n",
            "Iteration:2543;Percent complete:63.6%; Average loss:1364.6734\n",
            "Iteration:2544;Percent complete:63.6%; Average loss:1426.7340\n",
            "Iteration:2545;Percent complete:63.6%; Average loss:740.1352\n",
            "Iteration:2546;Percent complete:63.6%; Average loss:1151.5086\n",
            "Iteration:2547;Percent complete:63.7%; Average loss:1464.0348\n",
            "Iteration:2548;Percent complete:63.7%; Average loss:1338.9814\n",
            "Iteration:2549;Percent complete:63.7%; Average loss:1235.0405\n",
            "Iteration:2550;Percent complete:63.7%; Average loss:709.9895\n",
            "Iteration:2551;Percent complete:63.8%; Average loss:1559.8976\n",
            "Iteration:2552;Percent complete:63.8%; Average loss:643.4696\n",
            "Iteration:2553;Percent complete:63.8%; Average loss:1260.5714\n",
            "Iteration:2554;Percent complete:63.8%; Average loss:1699.0334\n",
            "Iteration:2555;Percent complete:63.9%; Average loss:1391.3688\n",
            "Iteration:2556;Percent complete:63.9%; Average loss:1271.7296\n",
            "Iteration:2557;Percent complete:63.9%; Average loss:1398.7298\n",
            "Iteration:2558;Percent complete:63.9%; Average loss:1410.5145\n",
            "Iteration:2559;Percent complete:64.0%; Average loss:632.2992\n",
            "Iteration:2560;Percent complete:64.0%; Average loss:1486.8261\n",
            "Iteration:2561;Percent complete:64.0%; Average loss:1231.7252\n",
            "Iteration:2562;Percent complete:64.0%; Average loss:1347.5245\n",
            "Iteration:2563;Percent complete:64.1%; Average loss:1337.6872\n",
            "Iteration:2564;Percent complete:64.1%; Average loss:1319.4514\n",
            "Iteration:2565;Percent complete:64.1%; Average loss:1325.5816\n",
            "Iteration:2566;Percent complete:64.1%; Average loss:1488.8476\n",
            "Iteration:2567;Percent complete:64.2%; Average loss:1395.3316\n",
            "Iteration:2568;Percent complete:64.2%; Average loss:665.6286\n",
            "Iteration:2569;Percent complete:64.2%; Average loss:1466.5113\n",
            "Iteration:2570;Percent complete:64.2%; Average loss:1299.0188\n",
            "Iteration:2571;Percent complete:64.3%; Average loss:1504.6832\n",
            "Iteration:2572;Percent complete:64.3%; Average loss:1141.5561\n",
            "Iteration:2573;Percent complete:64.3%; Average loss:1410.2019\n",
            "Iteration:2574;Percent complete:64.3%; Average loss:1504.4218\n",
            "Iteration:2575;Percent complete:64.4%; Average loss:1522.5824\n",
            "Iteration:2576;Percent complete:64.4%; Average loss:468.3470\n",
            "Iteration:2577;Percent complete:64.4%; Average loss:1533.6540\n",
            "Iteration:2578;Percent complete:64.5%; Average loss:1267.1844\n",
            "Iteration:2579;Percent complete:64.5%; Average loss:1375.0127\n",
            "Iteration:2580;Percent complete:64.5%; Average loss:1218.3293\n",
            "Iteration:2581;Percent complete:64.5%; Average loss:722.6770\n",
            "Iteration:2582;Percent complete:64.5%; Average loss:1152.0307\n",
            "Iteration:2583;Percent complete:64.6%; Average loss:1312.6804\n",
            "Iteration:2584;Percent complete:64.6%; Average loss:1300.0982\n",
            "Iteration:2585;Percent complete:64.6%; Average loss:1443.9972\n",
            "Iteration:2586;Percent complete:64.6%; Average loss:385.0489\n",
            "Iteration:2587;Percent complete:64.7%; Average loss:679.3989\n",
            "Iteration:2588;Percent complete:64.7%; Average loss:731.9183\n",
            "Iteration:2589;Percent complete:64.7%; Average loss:1079.1139\n",
            "Iteration:2590;Percent complete:64.8%; Average loss:1388.6909\n",
            "Iteration:2591;Percent complete:64.8%; Average loss:1417.2552\n",
            "Iteration:2592;Percent complete:64.8%; Average loss:661.3114\n",
            "Iteration:2593;Percent complete:64.8%; Average loss:709.7801\n",
            "Iteration:2594;Percent complete:64.8%; Average loss:1310.3197\n",
            "Iteration:2595;Percent complete:64.9%; Average loss:1303.2451\n",
            "Iteration:2596;Percent complete:64.9%; Average loss:781.7920\n",
            "Iteration:2597;Percent complete:64.9%; Average loss:641.0681\n",
            "Iteration:2598;Percent complete:65.0%; Average loss:1453.6996\n",
            "Iteration:2599;Percent complete:65.0%; Average loss:1482.8865\n",
            "Iteration:2600;Percent complete:65.0%; Average loss:1219.6819\n",
            "Iteration:2601;Percent complete:65.0%; Average loss:493.4935\n",
            "Iteration:2602;Percent complete:65.0%; Average loss:1433.0872\n",
            "Iteration:2603;Percent complete:65.1%; Average loss:1297.1156\n",
            "Iteration:2604;Percent complete:65.1%; Average loss:1139.6839\n",
            "Iteration:2605;Percent complete:65.1%; Average loss:1328.1154\n",
            "Iteration:2606;Percent complete:65.1%; Average loss:1403.6850\n",
            "Iteration:2607;Percent complete:65.2%; Average loss:733.3732\n",
            "Iteration:2608;Percent complete:65.2%; Average loss:1477.2969\n",
            "Iteration:2609;Percent complete:65.2%; Average loss:1455.9252\n",
            "Iteration:2610;Percent complete:65.2%; Average loss:1355.2388\n",
            "Iteration:2611;Percent complete:65.3%; Average loss:1389.5449\n",
            "Iteration:2612;Percent complete:65.3%; Average loss:1404.0655\n",
            "Iteration:2613;Percent complete:65.3%; Average loss:1337.1154\n",
            "Iteration:2614;Percent complete:65.3%; Average loss:1313.9681\n",
            "Iteration:2615;Percent complete:65.4%; Average loss:1406.2712\n",
            "Iteration:2616;Percent complete:65.4%; Average loss:1543.6641\n",
            "Iteration:2617;Percent complete:65.4%; Average loss:1638.4662\n",
            "Iteration:2618;Percent complete:65.5%; Average loss:687.1364\n",
            "Iteration:2619;Percent complete:65.5%; Average loss:1581.6664\n",
            "Iteration:2620;Percent complete:65.5%; Average loss:705.5136\n",
            "Iteration:2621;Percent complete:65.5%; Average loss:1369.6263\n",
            "Iteration:2622;Percent complete:65.5%; Average loss:1358.2090\n",
            "Iteration:2623;Percent complete:65.6%; Average loss:695.8777\n",
            "Iteration:2624;Percent complete:65.6%; Average loss:1359.5836\n",
            "Iteration:2625;Percent complete:65.6%; Average loss:1368.3809\n",
            "Iteration:2626;Percent complete:65.6%; Average loss:701.5011\n",
            "Iteration:2627;Percent complete:65.7%; Average loss:442.2262\n",
            "Iteration:2628;Percent complete:65.7%; Average loss:1359.6571\n",
            "Iteration:2629;Percent complete:65.7%; Average loss:655.2415\n",
            "Iteration:2630;Percent complete:65.8%; Average loss:767.2527\n",
            "Iteration:2631;Percent complete:65.8%; Average loss:679.8878\n",
            "Iteration:2632;Percent complete:65.8%; Average loss:1330.5531\n",
            "Iteration:2633;Percent complete:65.8%; Average loss:1338.7020\n",
            "Iteration:2634;Percent complete:65.8%; Average loss:1421.3624\n",
            "Iteration:2635;Percent complete:65.9%; Average loss:1393.3664\n",
            "Iteration:2636;Percent complete:65.9%; Average loss:1338.7418\n",
            "Iteration:2637;Percent complete:65.9%; Average loss:1549.4644\n",
            "Iteration:2638;Percent complete:66.0%; Average loss:745.8253\n",
            "Iteration:2639;Percent complete:66.0%; Average loss:350.8149\n",
            "Iteration:2640;Percent complete:66.0%; Average loss:1351.0043\n",
            "Iteration:2641;Percent complete:66.0%; Average loss:1380.8059\n",
            "Iteration:2642;Percent complete:66.0%; Average loss:1421.5507\n",
            "Iteration:2643;Percent complete:66.1%; Average loss:809.6550\n",
            "Iteration:2644;Percent complete:66.1%; Average loss:1334.5222\n",
            "Iteration:2645;Percent complete:66.1%; Average loss:1433.1851\n",
            "Iteration:2646;Percent complete:66.1%; Average loss:1541.8294\n",
            "Iteration:2647;Percent complete:66.2%; Average loss:1356.6302\n",
            "Iteration:2648;Percent complete:66.2%; Average loss:1343.2090\n",
            "Iteration:2649;Percent complete:66.2%; Average loss:1488.4858\n",
            "Iteration:2650;Percent complete:66.2%; Average loss:659.9991\n",
            "Iteration:2651;Percent complete:66.3%; Average loss:1510.6567\n",
            "Iteration:2652;Percent complete:66.3%; Average loss:1263.3656\n",
            "Iteration:2653;Percent complete:66.3%; Average loss:1413.4319\n",
            "Iteration:2654;Percent complete:66.3%; Average loss:1327.1447\n",
            "Iteration:2655;Percent complete:66.4%; Average loss:1260.9310\n",
            "Iteration:2656;Percent complete:66.4%; Average loss:1449.1478\n",
            "Iteration:2657;Percent complete:66.4%; Average loss:1615.0271\n",
            "Iteration:2658;Percent complete:66.5%; Average loss:1213.8133\n",
            "Iteration:2659;Percent complete:66.5%; Average loss:1258.4570\n",
            "Iteration:2660;Percent complete:66.5%; Average loss:1362.4587\n",
            "Iteration:2661;Percent complete:66.5%; Average loss:1115.0181\n",
            "Iteration:2662;Percent complete:66.5%; Average loss:1389.3596\n",
            "Iteration:2663;Percent complete:66.6%; Average loss:1211.0482\n",
            "Iteration:2664;Percent complete:66.6%; Average loss:688.8364\n",
            "Iteration:2665;Percent complete:66.6%; Average loss:1365.6202\n",
            "Iteration:2666;Percent complete:66.6%; Average loss:1252.9050\n",
            "Iteration:2667;Percent complete:66.7%; Average loss:1608.2501\n",
            "Iteration:2668;Percent complete:66.7%; Average loss:1473.5993\n",
            "Iteration:2669;Percent complete:66.7%; Average loss:1274.0573\n",
            "Iteration:2670;Percent complete:66.8%; Average loss:1113.7188\n",
            "Iteration:2671;Percent complete:66.8%; Average loss:1251.2883\n",
            "Iteration:2672;Percent complete:66.8%; Average loss:1354.7678\n",
            "Iteration:2673;Percent complete:66.8%; Average loss:1356.9138\n",
            "Iteration:2674;Percent complete:66.8%; Average loss:1441.7572\n",
            "Iteration:2675;Percent complete:66.9%; Average loss:731.6931\n",
            "Iteration:2676;Percent complete:66.9%; Average loss:1199.1811\n",
            "Iteration:2677;Percent complete:66.9%; Average loss:812.8683\n",
            "Iteration:2678;Percent complete:67.0%; Average loss:884.4417\n",
            "Iteration:2679;Percent complete:67.0%; Average loss:1380.1142\n",
            "Iteration:2680;Percent complete:67.0%; Average loss:1523.7097\n",
            "Iteration:2681;Percent complete:67.0%; Average loss:717.4770\n",
            "Iteration:2682;Percent complete:67.0%; Average loss:1496.3245\n",
            "Iteration:2683;Percent complete:67.1%; Average loss:1383.6281\n",
            "Iteration:2684;Percent complete:67.1%; Average loss:1503.4071\n",
            "Iteration:2685;Percent complete:67.1%; Average loss:669.0792\n",
            "Iteration:2686;Percent complete:67.2%; Average loss:1308.2746\n",
            "Iteration:2687;Percent complete:67.2%; Average loss:748.9069\n",
            "Iteration:2688;Percent complete:67.2%; Average loss:1269.6923\n",
            "Iteration:2689;Percent complete:67.2%; Average loss:1293.6264\n",
            "Iteration:2690;Percent complete:67.2%; Average loss:1378.3379\n",
            "Iteration:2691;Percent complete:67.3%; Average loss:1267.8147\n",
            "Iteration:2692;Percent complete:67.3%; Average loss:1380.6139\n",
            "Iteration:2693;Percent complete:67.3%; Average loss:642.4077\n",
            "Iteration:2694;Percent complete:67.3%; Average loss:1352.6354\n",
            "Iteration:2695;Percent complete:67.4%; Average loss:427.4198\n",
            "Iteration:2696;Percent complete:67.4%; Average loss:1394.7695\n",
            "Iteration:2697;Percent complete:67.4%; Average loss:747.5515\n",
            "Iteration:2698;Percent complete:67.5%; Average loss:1453.6256\n",
            "Iteration:2699;Percent complete:67.5%; Average loss:831.8575\n",
            "Iteration:2700;Percent complete:67.5%; Average loss:1485.0360\n",
            "Iteration:2701;Percent complete:67.5%; Average loss:1318.9477\n",
            "Iteration:2702;Percent complete:67.5%; Average loss:1405.3070\n",
            "Iteration:2703;Percent complete:67.6%; Average loss:1460.9159\n",
            "Iteration:2704;Percent complete:67.6%; Average loss:1318.1438\n",
            "Iteration:2705;Percent complete:67.6%; Average loss:652.9241\n",
            "Iteration:2706;Percent complete:67.7%; Average loss:1356.3834\n",
            "Iteration:2707;Percent complete:67.7%; Average loss:1402.5186\n",
            "Iteration:2708;Percent complete:67.7%; Average loss:1388.7948\n",
            "Iteration:2709;Percent complete:67.7%; Average loss:1511.7903\n",
            "Iteration:2710;Percent complete:67.8%; Average loss:1517.2842\n",
            "Iteration:2711;Percent complete:67.8%; Average loss:690.0154\n",
            "Iteration:2712;Percent complete:67.8%; Average loss:1179.7391\n",
            "Iteration:2713;Percent complete:67.8%; Average loss:1575.4542\n",
            "Iteration:2714;Percent complete:67.8%; Average loss:771.4610\n",
            "Iteration:2715;Percent complete:67.9%; Average loss:1367.0231\n",
            "Iteration:2716;Percent complete:67.9%; Average loss:1491.0193\n",
            "Iteration:2717;Percent complete:67.9%; Average loss:1430.9873\n",
            "Iteration:2718;Percent complete:68.0%; Average loss:1215.9580\n",
            "Iteration:2719;Percent complete:68.0%; Average loss:1284.1128\n",
            "Iteration:2720;Percent complete:68.0%; Average loss:1208.5485\n",
            "Iteration:2721;Percent complete:68.0%; Average loss:1281.2168\n",
            "Iteration:2722;Percent complete:68.0%; Average loss:1289.6358\n",
            "Iteration:2723;Percent complete:68.1%; Average loss:1327.0903\n",
            "Iteration:2724;Percent complete:68.1%; Average loss:1484.9468\n",
            "Iteration:2725;Percent complete:68.1%; Average loss:1356.8901\n",
            "Iteration:2726;Percent complete:68.2%; Average loss:1434.0050\n",
            "Iteration:2727;Percent complete:68.2%; Average loss:1357.2297\n",
            "Iteration:2728;Percent complete:68.2%; Average loss:1322.7437\n",
            "Iteration:2729;Percent complete:68.2%; Average loss:1226.7952\n",
            "Iteration:2730;Percent complete:68.2%; Average loss:1387.1313\n",
            "Iteration:2731;Percent complete:68.3%; Average loss:680.6475\n",
            "Iteration:2732;Percent complete:68.3%; Average loss:785.6770\n",
            "Iteration:2733;Percent complete:68.3%; Average loss:1248.1778\n",
            "Iteration:2734;Percent complete:68.3%; Average loss:1453.9217\n",
            "Iteration:2735;Percent complete:68.4%; Average loss:698.8663\n",
            "Iteration:2736;Percent complete:68.4%; Average loss:770.2228\n",
            "Iteration:2737;Percent complete:68.4%; Average loss:1535.5619\n",
            "Iteration:2738;Percent complete:68.5%; Average loss:1351.1460\n",
            "Iteration:2739;Percent complete:68.5%; Average loss:1383.1907\n",
            "Iteration:2740;Percent complete:68.5%; Average loss:750.5443\n",
            "Iteration:2741;Percent complete:68.5%; Average loss:1492.2538\n",
            "Iteration:2742;Percent complete:68.5%; Average loss:485.1583\n",
            "Iteration:2743;Percent complete:68.6%; Average loss:1365.1786\n",
            "Iteration:2744;Percent complete:68.6%; Average loss:802.0385\n",
            "Iteration:2745;Percent complete:68.6%; Average loss:777.3709\n",
            "Iteration:2746;Percent complete:68.7%; Average loss:1329.1503\n",
            "Iteration:2747;Percent complete:68.7%; Average loss:1370.4922\n",
            "Iteration:2748;Percent complete:68.7%; Average loss:1467.6888\n",
            "Iteration:2749;Percent complete:68.7%; Average loss:1271.9080\n",
            "Iteration:2750;Percent complete:68.8%; Average loss:480.3111\n",
            "Iteration:2751;Percent complete:68.8%; Average loss:1411.7948\n",
            "Iteration:2752;Percent complete:68.8%; Average loss:652.8538\n",
            "Iteration:2753;Percent complete:68.8%; Average loss:1549.8107\n",
            "Iteration:2754;Percent complete:68.8%; Average loss:1246.8475\n",
            "Iteration:2755;Percent complete:68.9%; Average loss:1406.8600\n",
            "Iteration:2756;Percent complete:68.9%; Average loss:1398.9209\n",
            "Iteration:2757;Percent complete:68.9%; Average loss:1290.1726\n",
            "Iteration:2758;Percent complete:69.0%; Average loss:1378.4961\n",
            "Iteration:2759;Percent complete:69.0%; Average loss:770.4780\n",
            "Iteration:2760;Percent complete:69.0%; Average loss:1271.4357\n",
            "Iteration:2761;Percent complete:69.0%; Average loss:1384.7571\n",
            "Iteration:2762;Percent complete:69.0%; Average loss:720.7551\n",
            "Iteration:2763;Percent complete:69.1%; Average loss:1379.0887\n",
            "Iteration:2764;Percent complete:69.1%; Average loss:1425.6785\n",
            "Iteration:2765;Percent complete:69.1%; Average loss:1558.5699\n",
            "Iteration:2766;Percent complete:69.2%; Average loss:753.2983\n",
            "Iteration:2767;Percent complete:69.2%; Average loss:1277.7242\n",
            "Iteration:2768;Percent complete:69.2%; Average loss:1455.9102\n",
            "Iteration:2769;Percent complete:69.2%; Average loss:1363.2061\n",
            "Iteration:2770;Percent complete:69.2%; Average loss:1571.4938\n",
            "Iteration:2771;Percent complete:69.3%; Average loss:1543.7744\n",
            "Iteration:2772;Percent complete:69.3%; Average loss:698.0611\n",
            "Iteration:2773;Percent complete:69.3%; Average loss:1339.1587\n",
            "Iteration:2774;Percent complete:69.3%; Average loss:590.1006\n",
            "Iteration:2775;Percent complete:69.4%; Average loss:1419.8476\n",
            "Iteration:2776;Percent complete:69.4%; Average loss:1380.2633\n",
            "Iteration:2777;Percent complete:69.4%; Average loss:1264.9863\n",
            "Iteration:2778;Percent complete:69.5%; Average loss:653.4897\n",
            "Iteration:2779;Percent complete:69.5%; Average loss:1355.5719\n",
            "Iteration:2780;Percent complete:69.5%; Average loss:1420.9612\n",
            "Iteration:2781;Percent complete:69.5%; Average loss:1257.5647\n",
            "Iteration:2782;Percent complete:69.5%; Average loss:1322.3099\n",
            "Iteration:2783;Percent complete:69.6%; Average loss:714.2827\n",
            "Iteration:2784;Percent complete:69.6%; Average loss:1467.8917\n",
            "Iteration:2785;Percent complete:69.6%; Average loss:608.5653\n",
            "Iteration:2786;Percent complete:69.7%; Average loss:1112.0217\n",
            "Iteration:2787;Percent complete:69.7%; Average loss:1404.3631\n",
            "Iteration:2788;Percent complete:69.7%; Average loss:1329.5978\n",
            "Iteration:2789;Percent complete:69.7%; Average loss:627.9171\n",
            "Iteration:2790;Percent complete:69.8%; Average loss:1536.2459\n",
            "Iteration:2791;Percent complete:69.8%; Average loss:1538.4700\n",
            "Iteration:2792;Percent complete:69.8%; Average loss:1426.4843\n",
            "Iteration:2793;Percent complete:69.8%; Average loss:1302.1181\n",
            "Iteration:2794;Percent complete:69.8%; Average loss:1370.3186\n",
            "Iteration:2795;Percent complete:69.9%; Average loss:1532.1011\n",
            "Iteration:2796;Percent complete:69.9%; Average loss:785.8399\n",
            "Iteration:2797;Percent complete:69.9%; Average loss:1454.1403\n",
            "Iteration:2798;Percent complete:70.0%; Average loss:1157.5340\n",
            "Iteration:2799;Percent complete:70.0%; Average loss:1299.8404\n",
            "Iteration:2800;Percent complete:70.0%; Average loss:1274.6185\n",
            "Iteration:2801;Percent complete:70.0%; Average loss:1249.9330\n",
            "Iteration:2802;Percent complete:70.0%; Average loss:737.5519\n",
            "Iteration:2803;Percent complete:70.1%; Average loss:1249.8751\n",
            "Iteration:2804;Percent complete:70.1%; Average loss:1418.8279\n",
            "Iteration:2805;Percent complete:70.1%; Average loss:728.2776\n",
            "Iteration:2806;Percent complete:70.2%; Average loss:704.0131\n",
            "Iteration:2807;Percent complete:70.2%; Average loss:1348.5466\n",
            "Iteration:2808;Percent complete:70.2%; Average loss:746.5030\n",
            "Iteration:2809;Percent complete:70.2%; Average loss:481.3005\n",
            "Iteration:2810;Percent complete:70.2%; Average loss:1269.0829\n",
            "Iteration:2811;Percent complete:70.3%; Average loss:703.7898\n",
            "Iteration:2812;Percent complete:70.3%; Average loss:1218.5908\n",
            "Iteration:2813;Percent complete:70.3%; Average loss:1232.4904\n",
            "Iteration:2814;Percent complete:70.3%; Average loss:1292.7279\n",
            "Iteration:2815;Percent complete:70.4%; Average loss:1221.3594\n",
            "Iteration:2816;Percent complete:70.4%; Average loss:675.6607\n",
            "Iteration:2817;Percent complete:70.4%; Average loss:1228.8303\n",
            "Iteration:2818;Percent complete:70.5%; Average loss:1300.4948\n",
            "Iteration:2819;Percent complete:70.5%; Average loss:1260.6884\n",
            "Iteration:2820;Percent complete:70.5%; Average loss:1315.3921\n",
            "Iteration:2821;Percent complete:70.5%; Average loss:1410.0672\n",
            "Iteration:2822;Percent complete:70.5%; Average loss:1511.0310\n",
            "Iteration:2823;Percent complete:70.6%; Average loss:466.4928\n",
            "Iteration:2824;Percent complete:70.6%; Average loss:1334.9537\n",
            "Iteration:2825;Percent complete:70.6%; Average loss:1404.1602\n",
            "Iteration:2826;Percent complete:70.7%; Average loss:1171.3322\n",
            "Iteration:2827;Percent complete:70.7%; Average loss:1440.2154\n",
            "Iteration:2828;Percent complete:70.7%; Average loss:1357.2789\n",
            "Iteration:2829;Percent complete:70.7%; Average loss:1319.0944\n",
            "Iteration:2830;Percent complete:70.8%; Average loss:1560.9920\n",
            "Iteration:2831;Percent complete:70.8%; Average loss:1297.1845\n",
            "Iteration:2832;Percent complete:70.8%; Average loss:1332.1542\n",
            "Iteration:2833;Percent complete:70.8%; Average loss:1505.8162\n",
            "Iteration:2834;Percent complete:70.9%; Average loss:1202.4661\n",
            "Iteration:2835;Percent complete:70.9%; Average loss:1349.5783\n",
            "Iteration:2836;Percent complete:70.9%; Average loss:1562.7204\n",
            "Iteration:2837;Percent complete:70.9%; Average loss:1423.7246\n",
            "Iteration:2838;Percent complete:71.0%; Average loss:1276.7767\n",
            "Iteration:2839;Percent complete:71.0%; Average loss:1368.9172\n",
            "Iteration:2840;Percent complete:71.0%; Average loss:1409.6356\n",
            "Iteration:2841;Percent complete:71.0%; Average loss:1388.9850\n",
            "Iteration:2842;Percent complete:71.0%; Average loss:626.2875\n",
            "Iteration:2843;Percent complete:71.1%; Average loss:1459.2953\n",
            "Iteration:2844;Percent complete:71.1%; Average loss:1390.8749\n",
            "Iteration:2845;Percent complete:71.1%; Average loss:469.0645\n",
            "Iteration:2846;Percent complete:71.2%; Average loss:1329.7345\n",
            "Iteration:2847;Percent complete:71.2%; Average loss:1560.2562\n",
            "Iteration:2848;Percent complete:71.2%; Average loss:1393.2362\n",
            "Iteration:2849;Percent complete:71.2%; Average loss:1437.5138\n",
            "Iteration:2850;Percent complete:71.2%; Average loss:1461.9661\n",
            "Iteration:2851;Percent complete:71.3%; Average loss:1484.8904\n",
            "Iteration:2852;Percent complete:71.3%; Average loss:644.8575\n",
            "Iteration:2853;Percent complete:71.3%; Average loss:1265.3579\n",
            "Iteration:2854;Percent complete:71.4%; Average loss:1318.7956\n",
            "Iteration:2855;Percent complete:71.4%; Average loss:1178.7391\n",
            "Iteration:2856;Percent complete:71.4%; Average loss:486.7807\n",
            "Iteration:2857;Percent complete:71.4%; Average loss:1469.2166\n",
            "Iteration:2858;Percent complete:71.5%; Average loss:1649.3610\n",
            "Iteration:2859;Percent complete:71.5%; Average loss:736.4501\n",
            "Iteration:2860;Percent complete:71.5%; Average loss:671.6076\n",
            "Iteration:2861;Percent complete:71.5%; Average loss:1412.5486\n",
            "Iteration:2862;Percent complete:71.5%; Average loss:1523.8543\n",
            "Iteration:2863;Percent complete:71.6%; Average loss:1467.1559\n",
            "Iteration:2864;Percent complete:71.6%; Average loss:1531.0960\n",
            "Iteration:2865;Percent complete:71.6%; Average loss:1409.7388\n",
            "Iteration:2866;Percent complete:71.7%; Average loss:1295.5811\n",
            "Iteration:2867;Percent complete:71.7%; Average loss:1496.9772\n",
            "Iteration:2868;Percent complete:71.7%; Average loss:1295.2102\n",
            "Iteration:2869;Percent complete:71.7%; Average loss:1394.4965\n",
            "Iteration:2870;Percent complete:71.8%; Average loss:1514.5711\n",
            "Iteration:2871;Percent complete:71.8%; Average loss:1207.3134\n",
            "Iteration:2872;Percent complete:71.8%; Average loss:1365.5691\n",
            "Iteration:2873;Percent complete:71.8%; Average loss:1313.6502\n",
            "Iteration:2874;Percent complete:71.9%; Average loss:1305.1872\n",
            "Iteration:2875;Percent complete:71.9%; Average loss:1412.1107\n",
            "Iteration:2876;Percent complete:71.9%; Average loss:1358.7696\n",
            "Iteration:2877;Percent complete:71.9%; Average loss:1311.4233\n",
            "Iteration:2878;Percent complete:72.0%; Average loss:550.1328\n",
            "Iteration:2879;Percent complete:72.0%; Average loss:716.0547\n",
            "Iteration:2880;Percent complete:72.0%; Average loss:1337.9694\n",
            "Iteration:2881;Percent complete:72.0%; Average loss:1516.5059\n",
            "Iteration:2882;Percent complete:72.0%; Average loss:1236.2421\n",
            "Iteration:2883;Percent complete:72.1%; Average loss:1332.4330\n",
            "Iteration:2884;Percent complete:72.1%; Average loss:1373.8958\n",
            "Iteration:2885;Percent complete:72.1%; Average loss:1178.3622\n",
            "Iteration:2886;Percent complete:72.2%; Average loss:1743.2633\n",
            "Iteration:2887;Percent complete:72.2%; Average loss:1314.1477\n",
            "Iteration:2888;Percent complete:72.2%; Average loss:1297.7710\n",
            "Iteration:2889;Percent complete:72.2%; Average loss:1325.1837\n",
            "Iteration:2890;Percent complete:72.2%; Average loss:1261.8387\n",
            "Iteration:2891;Percent complete:72.3%; Average loss:395.0157\n",
            "Iteration:2892;Percent complete:72.3%; Average loss:1492.5148\n",
            "Iteration:2893;Percent complete:72.3%; Average loss:1306.7987\n",
            "Iteration:2894;Percent complete:72.4%; Average loss:1296.0174\n",
            "Iteration:2895;Percent complete:72.4%; Average loss:435.7435\n",
            "Iteration:2896;Percent complete:72.4%; Average loss:717.3163\n",
            "Iteration:2897;Percent complete:72.4%; Average loss:728.8326\n",
            "Iteration:2898;Percent complete:72.5%; Average loss:1527.7930\n",
            "Iteration:2899;Percent complete:72.5%; Average loss:1341.7986\n",
            "Iteration:2900;Percent complete:72.5%; Average loss:687.1842\n",
            "Iteration:2901;Percent complete:72.5%; Average loss:1149.5724\n",
            "Iteration:2902;Percent complete:72.5%; Average loss:1488.5180\n",
            "Iteration:2903;Percent complete:72.6%; Average loss:719.1580\n",
            "Iteration:2904;Percent complete:72.6%; Average loss:1334.3436\n",
            "Iteration:2905;Percent complete:72.6%; Average loss:1368.6471\n",
            "Iteration:2906;Percent complete:72.7%; Average loss:1379.7705\n",
            "Iteration:2907;Percent complete:72.7%; Average loss:1323.2358\n",
            "Iteration:2908;Percent complete:72.7%; Average loss:1280.9653\n",
            "Iteration:2909;Percent complete:72.7%; Average loss:1460.7038\n",
            "Iteration:2910;Percent complete:72.8%; Average loss:1404.0314\n",
            "Iteration:2911;Percent complete:72.8%; Average loss:1298.0171\n",
            "Iteration:2912;Percent complete:72.8%; Average loss:1516.8940\n",
            "Iteration:2913;Percent complete:72.8%; Average loss:1351.9787\n",
            "Iteration:2914;Percent complete:72.9%; Average loss:1274.3291\n",
            "Iteration:2915;Percent complete:72.9%; Average loss:1305.2849\n",
            "Iteration:2916;Percent complete:72.9%; Average loss:1424.9855\n",
            "Iteration:2917;Percent complete:72.9%; Average loss:1344.0377\n",
            "Iteration:2918;Percent complete:73.0%; Average loss:1285.6966\n",
            "Iteration:2919;Percent complete:73.0%; Average loss:1222.2580\n",
            "Iteration:2920;Percent complete:73.0%; Average loss:791.5639\n",
            "Iteration:2921;Percent complete:73.0%; Average loss:1405.1646\n",
            "Iteration:2922;Percent complete:73.0%; Average loss:1106.4395\n",
            "Iteration:2923;Percent complete:73.1%; Average loss:1386.6180\n",
            "Iteration:2924;Percent complete:73.1%; Average loss:1276.3983\n",
            "Iteration:2925;Percent complete:73.1%; Average loss:1370.4487\n",
            "Iteration:2926;Percent complete:73.2%; Average loss:1278.4683\n",
            "Iteration:2927;Percent complete:73.2%; Average loss:1319.4841\n",
            "Iteration:2928;Percent complete:73.2%; Average loss:1325.2021\n",
            "Iteration:2929;Percent complete:73.2%; Average loss:1295.1300\n",
            "Iteration:2930;Percent complete:73.2%; Average loss:362.5228\n",
            "Iteration:2931;Percent complete:73.3%; Average loss:1483.2059\n",
            "Iteration:2932;Percent complete:73.3%; Average loss:1523.8282\n",
            "Iteration:2933;Percent complete:73.3%; Average loss:1431.3548\n",
            "Iteration:2934;Percent complete:73.4%; Average loss:1489.0086\n",
            "Iteration:2935;Percent complete:73.4%; Average loss:1437.1967\n",
            "Iteration:2936;Percent complete:73.4%; Average loss:1309.9022\n",
            "Iteration:2937;Percent complete:73.4%; Average loss:1468.2876\n",
            "Iteration:2938;Percent complete:73.5%; Average loss:1332.3754\n",
            "Iteration:2939;Percent complete:73.5%; Average loss:1269.9206\n",
            "Iteration:2940;Percent complete:73.5%; Average loss:1318.7823\n",
            "Iteration:2941;Percent complete:73.5%; Average loss:1359.6230\n",
            "Iteration:2942;Percent complete:73.6%; Average loss:1297.0793\n",
            "Iteration:2943;Percent complete:73.6%; Average loss:1401.4242\n",
            "Iteration:2944;Percent complete:73.6%; Average loss:1267.4234\n",
            "Iteration:2945;Percent complete:73.6%; Average loss:1307.0134\n",
            "Iteration:2946;Percent complete:73.7%; Average loss:1450.6882\n",
            "Iteration:2947;Percent complete:73.7%; Average loss:1424.5897\n",
            "Iteration:2948;Percent complete:73.7%; Average loss:1567.2943\n",
            "Iteration:2949;Percent complete:73.7%; Average loss:780.6226\n",
            "Iteration:2950;Percent complete:73.8%; Average loss:1335.3071\n",
            "Iteration:2951;Percent complete:73.8%; Average loss:644.7681\n",
            "Iteration:2952;Percent complete:73.8%; Average loss:1503.8979\n",
            "Iteration:2953;Percent complete:73.8%; Average loss:774.6243\n",
            "Iteration:2954;Percent complete:73.9%; Average loss:1350.1156\n",
            "Iteration:2955;Percent complete:73.9%; Average loss:1433.3880\n",
            "Iteration:2956;Percent complete:73.9%; Average loss:1646.8297\n",
            "Iteration:2957;Percent complete:73.9%; Average loss:1259.5159\n",
            "Iteration:2958;Percent complete:74.0%; Average loss:1519.2495\n",
            "Iteration:2959;Percent complete:74.0%; Average loss:1443.2387\n",
            "Iteration:2960;Percent complete:74.0%; Average loss:1526.5026\n",
            "Iteration:2961;Percent complete:74.0%; Average loss:1193.0859\n",
            "Iteration:2962;Percent complete:74.1%; Average loss:1435.8103\n",
            "Iteration:2963;Percent complete:74.1%; Average loss:1615.4313\n",
            "Iteration:2964;Percent complete:74.1%; Average loss:1304.5655\n",
            "Iteration:2965;Percent complete:74.1%; Average loss:732.8248\n",
            "Iteration:2966;Percent complete:74.2%; Average loss:1421.4649\n",
            "Iteration:2967;Percent complete:74.2%; Average loss:1554.6505\n",
            "Iteration:2968;Percent complete:74.2%; Average loss:1300.7628\n",
            "Iteration:2969;Percent complete:74.2%; Average loss:771.9148\n",
            "Iteration:2970;Percent complete:74.2%; Average loss:1320.0000\n",
            "Iteration:2971;Percent complete:74.3%; Average loss:1301.0135\n",
            "Iteration:2972;Percent complete:74.3%; Average loss:733.3941\n",
            "Iteration:2973;Percent complete:74.3%; Average loss:1439.9633\n",
            "Iteration:2974;Percent complete:74.4%; Average loss:1500.4451\n",
            "Iteration:2975;Percent complete:74.4%; Average loss:1675.7236\n",
            "Iteration:2976;Percent complete:74.4%; Average loss:1477.0556\n",
            "Iteration:2977;Percent complete:74.4%; Average loss:1543.6593\n",
            "Iteration:2978;Percent complete:74.5%; Average loss:1606.5972\n",
            "Iteration:2979;Percent complete:74.5%; Average loss:701.1366\n",
            "Iteration:2980;Percent complete:74.5%; Average loss:1399.0876\n",
            "Iteration:2981;Percent complete:74.5%; Average loss:1276.1730\n",
            "Iteration:2982;Percent complete:74.6%; Average loss:1458.0668\n",
            "Iteration:2983;Percent complete:74.6%; Average loss:626.3623\n",
            "Iteration:2984;Percent complete:74.6%; Average loss:1287.7549\n",
            "Iteration:2985;Percent complete:74.6%; Average loss:1340.9735\n",
            "Iteration:2986;Percent complete:74.7%; Average loss:735.7152\n",
            "Iteration:2987;Percent complete:74.7%; Average loss:1502.0938\n",
            "Iteration:2988;Percent complete:74.7%; Average loss:1493.6029\n",
            "Iteration:2989;Percent complete:74.7%; Average loss:1471.1540\n",
            "Iteration:2990;Percent complete:74.8%; Average loss:754.3096\n",
            "Iteration:2991;Percent complete:74.8%; Average loss:1366.6098\n",
            "Iteration:2992;Percent complete:74.8%; Average loss:1334.0719\n",
            "Iteration:2993;Percent complete:74.8%; Average loss:1376.8811\n",
            "Iteration:2994;Percent complete:74.9%; Average loss:1245.7168\n",
            "Iteration:2995;Percent complete:74.9%; Average loss:1583.9533\n",
            "Iteration:2996;Percent complete:74.9%; Average loss:1342.6140\n",
            "Iteration:2997;Percent complete:74.9%; Average loss:738.6481\n",
            "Iteration:2998;Percent complete:75.0%; Average loss:1448.9410\n",
            "Iteration:2999;Percent complete:75.0%; Average loss:1297.7212\n",
            "Iteration:3000;Percent complete:75.0%; Average loss:1242.9132\n",
            "Iteration:3001;Percent complete:75.0%; Average loss:1256.1328\n",
            "Iteration:3002;Percent complete:75.0%; Average loss:1473.8176\n",
            "Iteration:3003;Percent complete:75.1%; Average loss:1315.0835\n",
            "Iteration:3004;Percent complete:75.1%; Average loss:1315.6335\n",
            "Iteration:3005;Percent complete:75.1%; Average loss:1299.4158\n",
            "Iteration:3006;Percent complete:75.1%; Average loss:1317.7412\n",
            "Iteration:3007;Percent complete:75.2%; Average loss:1416.3459\n",
            "Iteration:3008;Percent complete:75.2%; Average loss:1414.9535\n",
            "Iteration:3009;Percent complete:75.2%; Average loss:1608.7586\n",
            "Iteration:3010;Percent complete:75.2%; Average loss:1401.1601\n",
            "Iteration:3011;Percent complete:75.3%; Average loss:1503.5044\n",
            "Iteration:3012;Percent complete:75.3%; Average loss:1306.5914\n",
            "Iteration:3013;Percent complete:75.3%; Average loss:1514.6340\n",
            "Iteration:3014;Percent complete:75.3%; Average loss:1302.8613\n",
            "Iteration:3015;Percent complete:75.4%; Average loss:1330.3110\n",
            "Iteration:3016;Percent complete:75.4%; Average loss:1358.3791\n",
            "Iteration:3017;Percent complete:75.4%; Average loss:1551.8409\n",
            "Iteration:3018;Percent complete:75.4%; Average loss:1523.3148\n",
            "Iteration:3019;Percent complete:75.5%; Average loss:1424.0863\n",
            "Iteration:3020;Percent complete:75.5%; Average loss:686.5091\n",
            "Iteration:3021;Percent complete:75.5%; Average loss:1232.8289\n",
            "Iteration:3022;Percent complete:75.5%; Average loss:1202.6575\n",
            "Iteration:3023;Percent complete:75.6%; Average loss:1446.0229\n",
            "Iteration:3024;Percent complete:75.6%; Average loss:1447.3872\n",
            "Iteration:3025;Percent complete:75.6%; Average loss:1746.9641\n",
            "Iteration:3026;Percent complete:75.6%; Average loss:683.3364\n",
            "Iteration:3027;Percent complete:75.7%; Average loss:1471.1094\n",
            "Iteration:3028;Percent complete:75.7%; Average loss:1543.1914\n",
            "Iteration:3029;Percent complete:75.7%; Average loss:1216.2597\n",
            "Iteration:3030;Percent complete:75.8%; Average loss:1335.1781\n",
            "Iteration:3031;Percent complete:75.8%; Average loss:1356.0925\n",
            "Iteration:3032;Percent complete:75.8%; Average loss:410.5234\n",
            "Iteration:3033;Percent complete:75.8%; Average loss:1340.8927\n",
            "Iteration:3034;Percent complete:75.8%; Average loss:688.4981\n",
            "Iteration:3035;Percent complete:75.9%; Average loss:652.2583\n",
            "Iteration:3036;Percent complete:75.9%; Average loss:769.7758\n",
            "Iteration:3037;Percent complete:75.9%; Average loss:698.0493\n",
            "Iteration:3038;Percent complete:75.9%; Average loss:1210.7923\n",
            "Iteration:3039;Percent complete:76.0%; Average loss:1467.7808\n",
            "Iteration:3040;Percent complete:76.0%; Average loss:730.6964\n",
            "Iteration:3041;Percent complete:76.0%; Average loss:690.5707\n",
            "Iteration:3042;Percent complete:76.0%; Average loss:1333.2572\n",
            "Iteration:3043;Percent complete:76.1%; Average loss:658.9066\n",
            "Iteration:3044;Percent complete:76.1%; Average loss:1345.4912\n",
            "Iteration:3045;Percent complete:76.1%; Average loss:1506.9870\n",
            "Iteration:3046;Percent complete:76.1%; Average loss:759.1985\n",
            "Iteration:3047;Percent complete:76.2%; Average loss:1265.9861\n",
            "Iteration:3048;Percent complete:76.2%; Average loss:1513.8207\n",
            "Iteration:3049;Percent complete:76.2%; Average loss:1448.2941\n",
            "Iteration:3050;Percent complete:76.2%; Average loss:1330.1334\n",
            "Iteration:3051;Percent complete:76.3%; Average loss:1367.2213\n",
            "Iteration:3052;Percent complete:76.3%; Average loss:1308.2138\n",
            "Iteration:3053;Percent complete:76.3%; Average loss:1283.8880\n",
            "Iteration:3054;Percent complete:76.3%; Average loss:1482.3654\n",
            "Iteration:3055;Percent complete:76.4%; Average loss:1335.4288\n",
            "Iteration:3056;Percent complete:76.4%; Average loss:1384.0154\n",
            "Iteration:3057;Percent complete:76.4%; Average loss:1342.1897\n",
            "Iteration:3058;Percent complete:76.4%; Average loss:1461.1557\n",
            "Iteration:3059;Percent complete:76.5%; Average loss:1357.1352\n",
            "Iteration:3060;Percent complete:76.5%; Average loss:477.9091\n",
            "Iteration:3061;Percent complete:76.5%; Average loss:1482.7158\n",
            "Iteration:3062;Percent complete:76.5%; Average loss:1482.4859\n",
            "Iteration:3063;Percent complete:76.6%; Average loss:834.2085\n",
            "Iteration:3064;Percent complete:76.6%; Average loss:1147.6396\n",
            "Iteration:3065;Percent complete:76.6%; Average loss:414.0239\n",
            "Iteration:3066;Percent complete:76.6%; Average loss:669.5669\n",
            "Iteration:3067;Percent complete:76.7%; Average loss:1359.4446\n",
            "Iteration:3068;Percent complete:76.7%; Average loss:1358.1644\n",
            "Iteration:3069;Percent complete:76.7%; Average loss:1388.1448\n",
            "Iteration:3070;Percent complete:76.8%; Average loss:1384.1114\n",
            "Iteration:3071;Percent complete:76.8%; Average loss:476.7904\n",
            "Iteration:3072;Percent complete:76.8%; Average loss:1469.3774\n",
            "Iteration:3073;Percent complete:76.8%; Average loss:1265.0256\n",
            "Iteration:3074;Percent complete:76.8%; Average loss:1352.9050\n",
            "Iteration:3075;Percent complete:76.9%; Average loss:1397.6616\n",
            "Iteration:3076;Percent complete:76.9%; Average loss:1174.2071\n",
            "Iteration:3077;Percent complete:76.9%; Average loss:1524.9793\n",
            "Iteration:3078;Percent complete:77.0%; Average loss:1292.4495\n",
            "Iteration:3079;Percent complete:77.0%; Average loss:629.9583\n",
            "Iteration:3080;Percent complete:77.0%; Average loss:1430.3879\n",
            "Iteration:3081;Percent complete:77.0%; Average loss:704.8735\n",
            "Iteration:3082;Percent complete:77.0%; Average loss:1351.1697\n",
            "Iteration:3083;Percent complete:77.1%; Average loss:1356.1801\n",
            "Iteration:3084;Percent complete:77.1%; Average loss:1524.6897\n",
            "Iteration:3085;Percent complete:77.1%; Average loss:1263.6875\n",
            "Iteration:3086;Percent complete:77.1%; Average loss:1446.2849\n",
            "Iteration:3087;Percent complete:77.2%; Average loss:668.8529\n",
            "Iteration:3088;Percent complete:77.2%; Average loss:1479.7790\n",
            "Iteration:3089;Percent complete:77.2%; Average loss:1489.7980\n",
            "Iteration:3090;Percent complete:77.2%; Average loss:1686.7546\n",
            "Iteration:3091;Percent complete:77.3%; Average loss:1182.0504\n",
            "Iteration:3092;Percent complete:77.3%; Average loss:1298.5978\n",
            "Iteration:3093;Percent complete:77.3%; Average loss:700.2090\n",
            "Iteration:3094;Percent complete:77.3%; Average loss:1592.4104\n",
            "Iteration:3095;Percent complete:77.4%; Average loss:690.7558\n",
            "Iteration:3096;Percent complete:77.4%; Average loss:762.1126\n",
            "Iteration:3097;Percent complete:77.4%; Average loss:1278.1300\n",
            "Iteration:3098;Percent complete:77.5%; Average loss:1504.2946\n",
            "Iteration:3099;Percent complete:77.5%; Average loss:1491.9454\n",
            "Iteration:3100;Percent complete:77.5%; Average loss:716.2084\n",
            "Iteration:3101;Percent complete:77.5%; Average loss:1423.0179\n",
            "Iteration:3102;Percent complete:77.5%; Average loss:1429.3775\n",
            "Iteration:3103;Percent complete:77.6%; Average loss:409.2464\n",
            "Iteration:3104;Percent complete:77.6%; Average loss:1344.5358\n",
            "Iteration:3105;Percent complete:77.6%; Average loss:1317.6539\n",
            "Iteration:3106;Percent complete:77.6%; Average loss:1479.1429\n",
            "Iteration:3107;Percent complete:77.7%; Average loss:1370.6549\n",
            "Iteration:3108;Percent complete:77.7%; Average loss:1485.5057\n",
            "Iteration:3109;Percent complete:77.7%; Average loss:1381.6951\n",
            "Iteration:3110;Percent complete:77.8%; Average loss:672.1360\n",
            "Iteration:3111;Percent complete:77.8%; Average loss:1299.8738\n",
            "Iteration:3112;Percent complete:77.8%; Average loss:782.3173\n",
            "Iteration:3113;Percent complete:77.8%; Average loss:1298.5109\n",
            "Iteration:3114;Percent complete:77.8%; Average loss:1316.1104\n",
            "Iteration:3115;Percent complete:77.9%; Average loss:640.5702\n",
            "Iteration:3116;Percent complete:77.9%; Average loss:1547.2339\n",
            "Iteration:3117;Percent complete:77.9%; Average loss:1421.2418\n",
            "Iteration:3118;Percent complete:78.0%; Average loss:1455.7967\n",
            "Iteration:3119;Percent complete:78.0%; Average loss:1309.7129\n",
            "Iteration:3120;Percent complete:78.0%; Average loss:1220.8110\n",
            "Iteration:3121;Percent complete:78.0%; Average loss:1343.5127\n",
            "Iteration:3122;Percent complete:78.0%; Average loss:657.3628\n",
            "Iteration:3123;Percent complete:78.1%; Average loss:1291.9328\n",
            "Iteration:3124;Percent complete:78.1%; Average loss:431.0304\n",
            "Iteration:3125;Percent complete:78.1%; Average loss:697.0767\n",
            "Iteration:3126;Percent complete:78.1%; Average loss:1298.8967\n",
            "Iteration:3127;Percent complete:78.2%; Average loss:1370.2570\n",
            "Iteration:3128;Percent complete:78.2%; Average loss:650.5869\n",
            "Iteration:3129;Percent complete:78.2%; Average loss:742.6199\n",
            "Iteration:3130;Percent complete:78.2%; Average loss:1407.4806\n",
            "Iteration:3131;Percent complete:78.3%; Average loss:1367.3093\n",
            "Iteration:3132;Percent complete:78.3%; Average loss:1351.5213\n",
            "Iteration:3133;Percent complete:78.3%; Average loss:1336.3837\n",
            "Iteration:3134;Percent complete:78.3%; Average loss:1322.7006\n",
            "Iteration:3135;Percent complete:78.4%; Average loss:1276.4829\n",
            "Iteration:3136;Percent complete:78.4%; Average loss:700.6286\n",
            "Iteration:3137;Percent complete:78.4%; Average loss:1451.1479\n",
            "Iteration:3138;Percent complete:78.5%; Average loss:1399.6779\n",
            "Iteration:3139;Percent complete:78.5%; Average loss:1394.4913\n",
            "Iteration:3140;Percent complete:78.5%; Average loss:431.3807\n",
            "Iteration:3141;Percent complete:78.5%; Average loss:450.1259\n",
            "Iteration:3142;Percent complete:78.5%; Average loss:1311.2049\n",
            "Iteration:3143;Percent complete:78.6%; Average loss:645.6844\n",
            "Iteration:3144;Percent complete:78.6%; Average loss:1303.7303\n",
            "Iteration:3145;Percent complete:78.6%; Average loss:1242.1955\n",
            "Iteration:3146;Percent complete:78.6%; Average loss:1303.8850\n",
            "Iteration:3147;Percent complete:78.7%; Average loss:1475.0174\n",
            "Iteration:3148;Percent complete:78.7%; Average loss:1446.9302\n",
            "Iteration:3149;Percent complete:78.7%; Average loss:1427.0414\n",
            "Iteration:3150;Percent complete:78.8%; Average loss:1488.6134\n",
            "Iteration:3151;Percent complete:78.8%; Average loss:1602.5503\n",
            "Iteration:3152;Percent complete:78.8%; Average loss:755.1696\n",
            "Iteration:3153;Percent complete:78.8%; Average loss:1272.8283\n",
            "Iteration:3154;Percent complete:78.8%; Average loss:1377.2537\n",
            "Iteration:3155;Percent complete:78.9%; Average loss:1245.2148\n",
            "Iteration:3156;Percent complete:78.9%; Average loss:1416.7593\n",
            "Iteration:3157;Percent complete:78.9%; Average loss:672.3836\n",
            "Iteration:3158;Percent complete:79.0%; Average loss:1572.2863\n",
            "Iteration:3159;Percent complete:79.0%; Average loss:705.4729\n",
            "Iteration:3160;Percent complete:79.0%; Average loss:706.0213\n",
            "Iteration:3161;Percent complete:79.0%; Average loss:1330.6566\n",
            "Iteration:3162;Percent complete:79.0%; Average loss:1048.7614\n",
            "Iteration:3163;Percent complete:79.1%; Average loss:1390.5804\n",
            "Iteration:3164;Percent complete:79.1%; Average loss:1466.0321\n",
            "Iteration:3165;Percent complete:79.1%; Average loss:1540.6684\n",
            "Iteration:3166;Percent complete:79.1%; Average loss:1278.1757\n",
            "Iteration:3167;Percent complete:79.2%; Average loss:1509.0289\n",
            "Iteration:3168;Percent complete:79.2%; Average loss:1369.0038\n",
            "Iteration:3169;Percent complete:79.2%; Average loss:1359.8445\n",
            "Iteration:3170;Percent complete:79.2%; Average loss:1233.7596\n",
            "Iteration:3171;Percent complete:79.3%; Average loss:1297.6285\n",
            "Iteration:3172;Percent complete:79.3%; Average loss:1357.9124\n",
            "Iteration:3173;Percent complete:79.3%; Average loss:1706.0658\n",
            "Iteration:3174;Percent complete:79.3%; Average loss:1088.1825\n",
            "Iteration:3175;Percent complete:79.4%; Average loss:1431.2221\n",
            "Iteration:3176;Percent complete:79.4%; Average loss:1426.8768\n",
            "Iteration:3177;Percent complete:79.4%; Average loss:1412.5230\n",
            "Iteration:3178;Percent complete:79.5%; Average loss:1270.9676\n",
            "Iteration:3179;Percent complete:79.5%; Average loss:329.4852\n",
            "Iteration:3180;Percent complete:79.5%; Average loss:703.5334\n",
            "Iteration:3181;Percent complete:79.5%; Average loss:1421.4223\n",
            "Iteration:3182;Percent complete:79.5%; Average loss:1129.3987\n",
            "Iteration:3183;Percent complete:79.6%; Average loss:1460.4977\n",
            "Iteration:3184;Percent complete:79.6%; Average loss:1386.9061\n",
            "Iteration:3185;Percent complete:79.6%; Average loss:1303.6369\n",
            "Iteration:3186;Percent complete:79.7%; Average loss:1374.5511\n",
            "Iteration:3187;Percent complete:79.7%; Average loss:1313.1074\n",
            "Iteration:3188;Percent complete:79.7%; Average loss:1193.1858\n",
            "Iteration:3189;Percent complete:79.7%; Average loss:476.6293\n",
            "Iteration:3190;Percent complete:79.8%; Average loss:1425.0235\n",
            "Iteration:3191;Percent complete:79.8%; Average loss:486.3496\n",
            "Iteration:3192;Percent complete:79.8%; Average loss:1474.4850\n",
            "Iteration:3193;Percent complete:79.8%; Average loss:1345.7952\n",
            "Iteration:3194;Percent complete:79.8%; Average loss:1435.5533\n",
            "Iteration:3195;Percent complete:79.9%; Average loss:1483.9536\n",
            "Iteration:3196;Percent complete:79.9%; Average loss:1298.9315\n",
            "Iteration:3197;Percent complete:79.9%; Average loss:1347.0670\n",
            "Iteration:3198;Percent complete:80.0%; Average loss:1497.4684\n",
            "Iteration:3199;Percent complete:80.0%; Average loss:1289.8697\n",
            "Iteration:3200;Percent complete:80.0%; Average loss:1459.6117\n",
            "Iteration:3201;Percent complete:80.0%; Average loss:1373.3605\n",
            "Iteration:3202;Percent complete:80.0%; Average loss:1295.0184\n",
            "Iteration:3203;Percent complete:80.1%; Average loss:1310.1137\n",
            "Iteration:3204;Percent complete:80.1%; Average loss:1277.8696\n",
            "Iteration:3205;Percent complete:80.1%; Average loss:1397.0635\n",
            "Iteration:3206;Percent complete:80.2%; Average loss:1342.3153\n",
            "Iteration:3207;Percent complete:80.2%; Average loss:1285.1011\n",
            "Iteration:3208;Percent complete:80.2%; Average loss:1377.9581\n",
            "Iteration:3209;Percent complete:80.2%; Average loss:743.8282\n",
            "Iteration:3210;Percent complete:80.2%; Average loss:1451.3906\n",
            "Iteration:3211;Percent complete:80.3%; Average loss:1543.1617\n",
            "Iteration:3212;Percent complete:80.3%; Average loss:1431.6280\n",
            "Iteration:3213;Percent complete:80.3%; Average loss:772.4390\n",
            "Iteration:3214;Percent complete:80.3%; Average loss:1480.2242\n",
            "Iteration:3215;Percent complete:80.4%; Average loss:1314.6256\n",
            "Iteration:3216;Percent complete:80.4%; Average loss:1248.0465\n",
            "Iteration:3217;Percent complete:80.4%; Average loss:1541.7227\n",
            "Iteration:3218;Percent complete:80.5%; Average loss:716.1203\n",
            "Iteration:3219;Percent complete:80.5%; Average loss:1578.8430\n",
            "Iteration:3220;Percent complete:80.5%; Average loss:1388.5660\n",
            "Iteration:3221;Percent complete:80.5%; Average loss:1450.9433\n",
            "Iteration:3222;Percent complete:80.5%; Average loss:1412.3943\n",
            "Iteration:3223;Percent complete:80.6%; Average loss:1508.6012\n",
            "Iteration:3224;Percent complete:80.6%; Average loss:1468.0519\n",
            "Iteration:3225;Percent complete:80.6%; Average loss:1582.0387\n",
            "Iteration:3226;Percent complete:80.7%; Average loss:1232.6493\n",
            "Iteration:3227;Percent complete:80.7%; Average loss:1465.8513\n",
            "Iteration:3228;Percent complete:80.7%; Average loss:470.2627\n",
            "Iteration:3229;Percent complete:80.7%; Average loss:662.9483\n",
            "Iteration:3230;Percent complete:80.8%; Average loss:1362.7194\n",
            "Iteration:3231;Percent complete:80.8%; Average loss:1281.6123\n",
            "Iteration:3232;Percent complete:80.8%; Average loss:1378.4351\n",
            "Iteration:3233;Percent complete:80.8%; Average loss:1369.6409\n",
            "Iteration:3234;Percent complete:80.8%; Average loss:654.5075\n",
            "Iteration:3235;Percent complete:80.9%; Average loss:422.8755\n",
            "Iteration:3236;Percent complete:80.9%; Average loss:1488.3420\n",
            "Iteration:3237;Percent complete:80.9%; Average loss:683.9696\n",
            "Iteration:3238;Percent complete:81.0%; Average loss:1523.2326\n",
            "Iteration:3239;Percent complete:81.0%; Average loss:1155.6518\n",
            "Iteration:3240;Percent complete:81.0%; Average loss:1282.0130\n",
            "Iteration:3241;Percent complete:81.0%; Average loss:1290.5514\n",
            "Iteration:3242;Percent complete:81.0%; Average loss:1234.5658\n",
            "Iteration:3243;Percent complete:81.1%; Average loss:1178.3969\n",
            "Iteration:3244;Percent complete:81.1%; Average loss:1292.0209\n",
            "Iteration:3245;Percent complete:81.1%; Average loss:1433.3911\n",
            "Iteration:3246;Percent complete:81.2%; Average loss:1443.8013\n",
            "Iteration:3247;Percent complete:81.2%; Average loss:1572.5346\n",
            "Iteration:3248;Percent complete:81.2%; Average loss:713.2679\n",
            "Iteration:3249;Percent complete:81.2%; Average loss:1385.5479\n",
            "Iteration:3250;Percent complete:81.2%; Average loss:1403.8878\n",
            "Iteration:3251;Percent complete:81.3%; Average loss:1330.1662\n",
            "Iteration:3252;Percent complete:81.3%; Average loss:1539.3673\n",
            "Iteration:3253;Percent complete:81.3%; Average loss:687.7148\n",
            "Iteration:3254;Percent complete:81.3%; Average loss:1186.9365\n",
            "Iteration:3255;Percent complete:81.4%; Average loss:833.2728\n",
            "Iteration:3256;Percent complete:81.4%; Average loss:663.2242\n",
            "Iteration:3257;Percent complete:81.4%; Average loss:1503.7383\n",
            "Iteration:3258;Percent complete:81.5%; Average loss:1398.9135\n",
            "Iteration:3259;Percent complete:81.5%; Average loss:1324.9117\n",
            "Iteration:3260;Percent complete:81.5%; Average loss:1337.3030\n",
            "Iteration:3261;Percent complete:81.5%; Average loss:1326.3562\n",
            "Iteration:3262;Percent complete:81.5%; Average loss:1306.3158\n",
            "Iteration:3263;Percent complete:81.6%; Average loss:524.5275\n",
            "Iteration:3264;Percent complete:81.6%; Average loss:1313.5729\n",
            "Iteration:3265;Percent complete:81.6%; Average loss:1649.4479\n",
            "Iteration:3266;Percent complete:81.7%; Average loss:1386.6577\n",
            "Iteration:3267;Percent complete:81.7%; Average loss:1473.1434\n",
            "Iteration:3268;Percent complete:81.7%; Average loss:1397.9159\n",
            "Iteration:3269;Percent complete:81.7%; Average loss:1426.0284\n",
            "Iteration:3270;Percent complete:81.8%; Average loss:1417.6914\n",
            "Iteration:3271;Percent complete:81.8%; Average loss:1445.6774\n",
            "Iteration:3272;Percent complete:81.8%; Average loss:1170.9528\n",
            "Iteration:3273;Percent complete:81.8%; Average loss:701.6450\n",
            "Iteration:3274;Percent complete:81.8%; Average loss:1202.4361\n",
            "Iteration:3275;Percent complete:81.9%; Average loss:1401.8884\n",
            "Iteration:3276;Percent complete:81.9%; Average loss:1423.5342\n",
            "Iteration:3277;Percent complete:81.9%; Average loss:1302.2983\n",
            "Iteration:3278;Percent complete:82.0%; Average loss:1268.4989\n",
            "Iteration:3279;Percent complete:82.0%; Average loss:1512.1519\n",
            "Iteration:3280;Percent complete:82.0%; Average loss:1240.5873\n",
            "Iteration:3281;Percent complete:82.0%; Average loss:1255.5875\n",
            "Iteration:3282;Percent complete:82.0%; Average loss:1385.2361\n",
            "Iteration:3283;Percent complete:82.1%; Average loss:1339.7680\n",
            "Iteration:3284;Percent complete:82.1%; Average loss:1338.1584\n",
            "Iteration:3285;Percent complete:82.1%; Average loss:1092.7634\n",
            "Iteration:3286;Percent complete:82.2%; Average loss:1364.4750\n",
            "Iteration:3287;Percent complete:82.2%; Average loss:694.4876\n",
            "Iteration:3288;Percent complete:82.2%; Average loss:1199.3942\n",
            "Iteration:3289;Percent complete:82.2%; Average loss:1441.7141\n",
            "Iteration:3290;Percent complete:82.2%; Average loss:1368.9640\n",
            "Iteration:3291;Percent complete:82.3%; Average loss:1331.2473\n",
            "Iteration:3292;Percent complete:82.3%; Average loss:1262.2017\n",
            "Iteration:3293;Percent complete:82.3%; Average loss:1478.1798\n",
            "Iteration:3294;Percent complete:82.3%; Average loss:1379.4381\n",
            "Iteration:3295;Percent complete:82.4%; Average loss:1319.8746\n",
            "Iteration:3296;Percent complete:82.4%; Average loss:1273.5368\n",
            "Iteration:3297;Percent complete:82.4%; Average loss:1450.1433\n",
            "Iteration:3298;Percent complete:82.5%; Average loss:678.4024\n",
            "Iteration:3299;Percent complete:82.5%; Average loss:1422.4054\n",
            "Iteration:3300;Percent complete:82.5%; Average loss:1377.7732\n",
            "Iteration:3301;Percent complete:82.5%; Average loss:1394.5937\n",
            "Iteration:3302;Percent complete:82.5%; Average loss:736.6342\n",
            "Iteration:3303;Percent complete:82.6%; Average loss:1347.6043\n",
            "Iteration:3304;Percent complete:82.6%; Average loss:1278.4069\n",
            "Iteration:3305;Percent complete:82.6%; Average loss:728.0929\n",
            "Iteration:3306;Percent complete:82.7%; Average loss:1353.4754\n",
            "Iteration:3307;Percent complete:82.7%; Average loss:1392.1441\n",
            "Iteration:3308;Percent complete:82.7%; Average loss:1372.6102\n",
            "Iteration:3309;Percent complete:82.7%; Average loss:1570.5661\n",
            "Iteration:3310;Percent complete:82.8%; Average loss:1361.3917\n",
            "Iteration:3311;Percent complete:82.8%; Average loss:632.0264\n",
            "Iteration:3312;Percent complete:82.8%; Average loss:1395.7633\n",
            "Iteration:3313;Percent complete:82.8%; Average loss:1521.2439\n",
            "Iteration:3314;Percent complete:82.8%; Average loss:717.7667\n",
            "Iteration:3315;Percent complete:82.9%; Average loss:434.3523\n",
            "Iteration:3316;Percent complete:82.9%; Average loss:1399.1370\n",
            "Iteration:3317;Percent complete:82.9%; Average loss:1175.7280\n",
            "Iteration:3318;Percent complete:83.0%; Average loss:670.5477\n",
            "Iteration:3319;Percent complete:83.0%; Average loss:1161.2183\n",
            "Iteration:3320;Percent complete:83.0%; Average loss:1360.4545\n",
            "Iteration:3321;Percent complete:83.0%; Average loss:1258.9460\n",
            "Iteration:3322;Percent complete:83.0%; Average loss:1290.2964\n",
            "Iteration:3323;Percent complete:83.1%; Average loss:1245.0139\n",
            "Iteration:3324;Percent complete:83.1%; Average loss:572.8591\n",
            "Iteration:3325;Percent complete:83.1%; Average loss:1572.6413\n",
            "Iteration:3326;Percent complete:83.2%; Average loss:1416.1890\n",
            "Iteration:3327;Percent complete:83.2%; Average loss:400.5511\n",
            "Iteration:3328;Percent complete:83.2%; Average loss:593.8502\n",
            "Iteration:3329;Percent complete:83.2%; Average loss:1298.2636\n",
            "Iteration:3330;Percent complete:83.2%; Average loss:717.3711\n",
            "Iteration:3331;Percent complete:83.3%; Average loss:468.5638\n",
            "Iteration:3332;Percent complete:83.3%; Average loss:1366.6910\n",
            "Iteration:3333;Percent complete:83.3%; Average loss:1269.2720\n",
            "Iteration:3334;Percent complete:83.4%; Average loss:1346.8009\n",
            "Iteration:3335;Percent complete:83.4%; Average loss:1214.3168\n",
            "Iteration:3336;Percent complete:83.4%; Average loss:766.4262\n",
            "Iteration:3337;Percent complete:83.4%; Average loss:1294.6351\n",
            "Iteration:3338;Percent complete:83.5%; Average loss:1407.4374\n",
            "Iteration:3339;Percent complete:83.5%; Average loss:1334.2907\n",
            "Iteration:3340;Percent complete:83.5%; Average loss:1460.1963\n",
            "Iteration:3341;Percent complete:83.5%; Average loss:1359.1000\n",
            "Iteration:3342;Percent complete:83.5%; Average loss:1335.3903\n",
            "Iteration:3343;Percent complete:83.6%; Average loss:1582.1083\n",
            "Iteration:3344;Percent complete:83.6%; Average loss:376.6998\n",
            "Iteration:3345;Percent complete:83.6%; Average loss:1297.9612\n",
            "Iteration:3346;Percent complete:83.7%; Average loss:1256.5503\n",
            "Iteration:3347;Percent complete:83.7%; Average loss:1457.5622\n",
            "Iteration:3348;Percent complete:83.7%; Average loss:1275.3585\n",
            "Iteration:3349;Percent complete:83.7%; Average loss:713.0846\n",
            "Iteration:3350;Percent complete:83.8%; Average loss:1403.8847\n",
            "Iteration:3351;Percent complete:83.8%; Average loss:1581.2967\n",
            "Iteration:3352;Percent complete:83.8%; Average loss:1334.7869\n",
            "Iteration:3353;Percent complete:83.8%; Average loss:1419.4442\n",
            "Iteration:3354;Percent complete:83.9%; Average loss:1341.6245\n",
            "Iteration:3355;Percent complete:83.9%; Average loss:739.7789\n",
            "Iteration:3356;Percent complete:83.9%; Average loss:1537.0857\n",
            "Iteration:3357;Percent complete:83.9%; Average loss:731.0257\n",
            "Iteration:3358;Percent complete:84.0%; Average loss:1465.7779\n",
            "Iteration:3359;Percent complete:84.0%; Average loss:1571.3062\n",
            "Iteration:3360;Percent complete:84.0%; Average loss:1296.3027\n",
            "Iteration:3361;Percent complete:84.0%; Average loss:700.6378\n",
            "Iteration:3362;Percent complete:84.0%; Average loss:1421.5932\n",
            "Iteration:3363;Percent complete:84.1%; Average loss:1386.8157\n",
            "Iteration:3364;Percent complete:84.1%; Average loss:636.7563\n",
            "Iteration:3365;Percent complete:84.1%; Average loss:741.9107\n",
            "Iteration:3366;Percent complete:84.2%; Average loss:1461.6579\n",
            "Iteration:3367;Percent complete:84.2%; Average loss:1302.8661\n",
            "Iteration:3368;Percent complete:84.2%; Average loss:1329.7445\n",
            "Iteration:3369;Percent complete:84.2%; Average loss:1323.5496\n",
            "Iteration:3370;Percent complete:84.2%; Average loss:1454.8142\n",
            "Iteration:3371;Percent complete:84.3%; Average loss:1431.8075\n",
            "Iteration:3372;Percent complete:84.3%; Average loss:1342.0770\n",
            "Iteration:3373;Percent complete:84.3%; Average loss:1598.2256\n",
            "Iteration:3374;Percent complete:84.4%; Average loss:1371.4780\n",
            "Iteration:3375;Percent complete:84.4%; Average loss:1432.2546\n",
            "Iteration:3376;Percent complete:84.4%; Average loss:438.8365\n",
            "Iteration:3377;Percent complete:84.4%; Average loss:1346.3350\n",
            "Iteration:3378;Percent complete:84.5%; Average loss:677.0345\n",
            "Iteration:3379;Percent complete:84.5%; Average loss:1314.4085\n",
            "Iteration:3380;Percent complete:84.5%; Average loss:1166.0408\n",
            "Iteration:3381;Percent complete:84.5%; Average loss:1143.4057\n",
            "Iteration:3382;Percent complete:84.5%; Average loss:1356.6518\n",
            "Iteration:3383;Percent complete:84.6%; Average loss:670.8857\n",
            "Iteration:3384;Percent complete:84.6%; Average loss:1496.5002\n",
            "Iteration:3385;Percent complete:84.6%; Average loss:1237.3172\n",
            "Iteration:3386;Percent complete:84.7%; Average loss:1213.5543\n",
            "Iteration:3387;Percent complete:84.7%; Average loss:1305.3255\n",
            "Iteration:3388;Percent complete:84.7%; Average loss:1362.0730\n",
            "Iteration:3389;Percent complete:84.7%; Average loss:1396.0291\n",
            "Iteration:3390;Percent complete:84.8%; Average loss:657.9939\n",
            "Iteration:3391;Percent complete:84.8%; Average loss:1539.9197\n",
            "Iteration:3392;Percent complete:84.8%; Average loss:1467.0902\n",
            "Iteration:3393;Percent complete:84.8%; Average loss:1541.1790\n",
            "Iteration:3394;Percent complete:84.9%; Average loss:1404.4963\n",
            "Iteration:3395;Percent complete:84.9%; Average loss:679.6577\n",
            "Iteration:3396;Percent complete:84.9%; Average loss:1239.2986\n",
            "Iteration:3397;Percent complete:84.9%; Average loss:1436.3994\n",
            "Iteration:3398;Percent complete:85.0%; Average loss:1368.8033\n",
            "Iteration:3399;Percent complete:85.0%; Average loss:1268.9731\n",
            "Iteration:3400;Percent complete:85.0%; Average loss:762.7435\n",
            "Iteration:3401;Percent complete:85.0%; Average loss:510.5264\n",
            "Iteration:3402;Percent complete:85.0%; Average loss:1209.9173\n",
            "Iteration:3403;Percent complete:85.1%; Average loss:1380.3085\n",
            "Iteration:3404;Percent complete:85.1%; Average loss:676.9352\n",
            "Iteration:3405;Percent complete:85.1%; Average loss:1418.3646\n",
            "Iteration:3406;Percent complete:85.2%; Average loss:1196.2708\n",
            "Iteration:3407;Percent complete:85.2%; Average loss:1263.9385\n",
            "Iteration:3408;Percent complete:85.2%; Average loss:1238.7936\n",
            "Iteration:3409;Percent complete:85.2%; Average loss:684.5606\n",
            "Iteration:3410;Percent complete:85.2%; Average loss:1482.6762\n",
            "Iteration:3411;Percent complete:85.3%; Average loss:1477.1609\n",
            "Iteration:3412;Percent complete:85.3%; Average loss:1376.1928\n",
            "Iteration:3413;Percent complete:85.3%; Average loss:1487.1303\n",
            "Iteration:3414;Percent complete:85.4%; Average loss:686.9904\n",
            "Iteration:3415;Percent complete:85.4%; Average loss:1453.4403\n",
            "Iteration:3416;Percent complete:85.4%; Average loss:1223.2386\n",
            "Iteration:3417;Percent complete:85.4%; Average loss:1302.5347\n",
            "Iteration:3418;Percent complete:85.5%; Average loss:491.9515\n",
            "Iteration:3419;Percent complete:85.5%; Average loss:1243.5291\n",
            "Iteration:3420;Percent complete:85.5%; Average loss:1303.8792\n",
            "Iteration:3421;Percent complete:85.5%; Average loss:1350.1489\n",
            "Iteration:3422;Percent complete:85.5%; Average loss:1344.4780\n",
            "Iteration:3423;Percent complete:85.6%; Average loss:744.6261\n",
            "Iteration:3424;Percent complete:85.6%; Average loss:1300.9296\n",
            "Iteration:3425;Percent complete:85.6%; Average loss:729.6159\n",
            "Iteration:3426;Percent complete:85.7%; Average loss:1402.7976\n",
            "Iteration:3427;Percent complete:85.7%; Average loss:1318.3641\n",
            "Iteration:3428;Percent complete:85.7%; Average loss:703.1121\n",
            "Iteration:3429;Percent complete:85.7%; Average loss:1299.1851\n",
            "Iteration:3430;Percent complete:85.8%; Average loss:1348.9090\n",
            "Iteration:3431;Percent complete:85.8%; Average loss:1364.8219\n",
            "Iteration:3432;Percent complete:85.8%; Average loss:1294.7300\n",
            "Iteration:3433;Percent complete:85.8%; Average loss:1239.5125\n",
            "Iteration:3434;Percent complete:85.9%; Average loss:1353.0603\n",
            "Iteration:3435;Percent complete:85.9%; Average loss:724.0041\n",
            "Iteration:3436;Percent complete:85.9%; Average loss:637.8504\n",
            "Iteration:3437;Percent complete:85.9%; Average loss:1304.8269\n",
            "Iteration:3438;Percent complete:86.0%; Average loss:1258.7564\n",
            "Iteration:3439;Percent complete:86.0%; Average loss:1274.3297\n",
            "Iteration:3440;Percent complete:86.0%; Average loss:1499.0187\n",
            "Iteration:3441;Percent complete:86.0%; Average loss:1253.5127\n",
            "Iteration:3442;Percent complete:86.1%; Average loss:1350.7683\n",
            "Iteration:3443;Percent complete:86.1%; Average loss:1529.2963\n",
            "Iteration:3444;Percent complete:86.1%; Average loss:1567.3427\n",
            "Iteration:3445;Percent complete:86.1%; Average loss:705.4377\n",
            "Iteration:3446;Percent complete:86.2%; Average loss:1231.4662\n",
            "Iteration:3447;Percent complete:86.2%; Average loss:1544.3298\n",
            "Iteration:3448;Percent complete:86.2%; Average loss:322.6503\n",
            "Iteration:3449;Percent complete:86.2%; Average loss:1221.3385\n",
            "Iteration:3450;Percent complete:86.2%; Average loss:1094.5577\n",
            "Iteration:3451;Percent complete:86.3%; Average loss:1394.4351\n",
            "Iteration:3452;Percent complete:86.3%; Average loss:1401.6261\n",
            "Iteration:3453;Percent complete:86.3%; Average loss:1628.5205\n",
            "Iteration:3454;Percent complete:86.4%; Average loss:1550.1999\n",
            "Iteration:3455;Percent complete:86.4%; Average loss:1308.7921\n",
            "Iteration:3456;Percent complete:86.4%; Average loss:1434.1369\n",
            "Iteration:3457;Percent complete:86.4%; Average loss:475.5884\n",
            "Iteration:3458;Percent complete:86.5%; Average loss:1473.9110\n",
            "Iteration:3459;Percent complete:86.5%; Average loss:1528.7405\n",
            "Iteration:3460;Percent complete:86.5%; Average loss:1441.5369\n",
            "Iteration:3461;Percent complete:86.5%; Average loss:1418.9056\n",
            "Iteration:3462;Percent complete:86.6%; Average loss:1232.8643\n",
            "Iteration:3463;Percent complete:86.6%; Average loss:1431.4224\n",
            "Iteration:3464;Percent complete:86.6%; Average loss:1534.8852\n",
            "Iteration:3465;Percent complete:86.6%; Average loss:1411.7208\n",
            "Iteration:3466;Percent complete:86.7%; Average loss:1653.2515\n",
            "Iteration:3467;Percent complete:86.7%; Average loss:1396.8459\n",
            "Iteration:3468;Percent complete:86.7%; Average loss:1586.2180\n",
            "Iteration:3469;Percent complete:86.7%; Average loss:674.0672\n",
            "Iteration:3470;Percent complete:86.8%; Average loss:1303.9756\n",
            "Iteration:3471;Percent complete:86.8%; Average loss:472.4158\n",
            "Iteration:3472;Percent complete:86.8%; Average loss:1520.6050\n",
            "Iteration:3473;Percent complete:86.8%; Average loss:1326.7110\n",
            "Iteration:3474;Percent complete:86.9%; Average loss:646.4970\n",
            "Iteration:3475;Percent complete:86.9%; Average loss:1575.2252\n",
            "Iteration:3476;Percent complete:86.9%; Average loss:1453.6693\n",
            "Iteration:3477;Percent complete:86.9%; Average loss:1427.7498\n",
            "Iteration:3478;Percent complete:87.0%; Average loss:1175.2515\n",
            "Iteration:3479;Percent complete:87.0%; Average loss:1387.7631\n",
            "Iteration:3480;Percent complete:87.0%; Average loss:731.2440\n",
            "Iteration:3481;Percent complete:87.0%; Average loss:1250.7258\n",
            "Iteration:3482;Percent complete:87.1%; Average loss:1506.9840\n",
            "Iteration:3483;Percent complete:87.1%; Average loss:1579.5429\n",
            "Iteration:3484;Percent complete:87.1%; Average loss:1382.4523\n",
            "Iteration:3485;Percent complete:87.1%; Average loss:1288.9777\n",
            "Iteration:3486;Percent complete:87.2%; Average loss:1265.8614\n",
            "Iteration:3487;Percent complete:87.2%; Average loss:1354.6618\n",
            "Iteration:3488;Percent complete:87.2%; Average loss:657.1228\n",
            "Iteration:3489;Percent complete:87.2%; Average loss:1512.6531\n",
            "Iteration:3490;Percent complete:87.2%; Average loss:1486.4950\n",
            "Iteration:3491;Percent complete:87.3%; Average loss:653.1088\n",
            "Iteration:3492;Percent complete:87.3%; Average loss:1395.4170\n",
            "Iteration:3493;Percent complete:87.3%; Average loss:1293.6551\n",
            "Iteration:3494;Percent complete:87.4%; Average loss:1325.2493\n",
            "Iteration:3495;Percent complete:87.4%; Average loss:719.8099\n",
            "Iteration:3496;Percent complete:87.4%; Average loss:1349.7335\n",
            "Iteration:3497;Percent complete:87.4%; Average loss:1363.1913\n",
            "Iteration:3498;Percent complete:87.5%; Average loss:1382.0193\n",
            "Iteration:3499;Percent complete:87.5%; Average loss:610.0183\n",
            "Iteration:3500;Percent complete:87.5%; Average loss:1277.0503\n",
            "Iteration:3501;Percent complete:87.5%; Average loss:1296.1830\n",
            "Iteration:3502;Percent complete:87.5%; Average loss:1475.2867\n",
            "Iteration:3503;Percent complete:87.6%; Average loss:1406.8332\n",
            "Iteration:3504;Percent complete:87.6%; Average loss:758.9324\n",
            "Iteration:3505;Percent complete:87.6%; Average loss:1446.6792\n",
            "Iteration:3506;Percent complete:87.6%; Average loss:1256.3836\n",
            "Iteration:3507;Percent complete:87.7%; Average loss:697.5353\n",
            "Iteration:3508;Percent complete:87.7%; Average loss:1338.2111\n",
            "Iteration:3509;Percent complete:87.7%; Average loss:1418.3179\n",
            "Iteration:3510;Percent complete:87.8%; Average loss:1492.7044\n",
            "Iteration:3511;Percent complete:87.8%; Average loss:734.1888\n",
            "Iteration:3512;Percent complete:87.8%; Average loss:633.5406\n",
            "Iteration:3513;Percent complete:87.8%; Average loss:1320.5014\n",
            "Iteration:3514;Percent complete:87.8%; Average loss:740.7356\n",
            "Iteration:3515;Percent complete:87.9%; Average loss:419.0258\n",
            "Iteration:3516;Percent complete:87.9%; Average loss:1307.1721\n",
            "Iteration:3517;Percent complete:87.9%; Average loss:1560.9449\n",
            "Iteration:3518;Percent complete:87.9%; Average loss:1404.2844\n",
            "Iteration:3519;Percent complete:88.0%; Average loss:1346.2926\n",
            "Iteration:3520;Percent complete:88.0%; Average loss:698.9814\n",
            "Iteration:3521;Percent complete:88.0%; Average loss:1357.1924\n",
            "Iteration:3522;Percent complete:88.0%; Average loss:485.9533\n",
            "Iteration:3523;Percent complete:88.1%; Average loss:1147.6071\n",
            "Iteration:3524;Percent complete:88.1%; Average loss:1303.1553\n",
            "Iteration:3525;Percent complete:88.1%; Average loss:1243.9516\n",
            "Iteration:3526;Percent complete:88.1%; Average loss:1309.8236\n",
            "Iteration:3527;Percent complete:88.2%; Average loss:1228.2526\n",
            "Iteration:3528;Percent complete:88.2%; Average loss:1222.0526\n",
            "Iteration:3529;Percent complete:88.2%; Average loss:643.0830\n",
            "Iteration:3530;Percent complete:88.2%; Average loss:751.9655\n",
            "Iteration:3531;Percent complete:88.3%; Average loss:1321.7667\n",
            "Iteration:3532;Percent complete:88.3%; Average loss:1418.7450\n",
            "Iteration:3533;Percent complete:88.3%; Average loss:1350.6163\n",
            "Iteration:3534;Percent complete:88.3%; Average loss:1135.4947\n",
            "Iteration:3535;Percent complete:88.4%; Average loss:719.4078\n",
            "Iteration:3536;Percent complete:88.4%; Average loss:542.1018\n",
            "Iteration:3537;Percent complete:88.4%; Average loss:1391.8971\n",
            "Iteration:3538;Percent complete:88.4%; Average loss:1310.9537\n",
            "Iteration:3539;Percent complete:88.5%; Average loss:1469.7225\n",
            "Iteration:3540;Percent complete:88.5%; Average loss:1390.7757\n",
            "Iteration:3541;Percent complete:88.5%; Average loss:1379.0481\n",
            "Iteration:3542;Percent complete:88.5%; Average loss:1343.1905\n",
            "Iteration:3543;Percent complete:88.6%; Average loss:1129.8020\n",
            "Iteration:3544;Percent complete:88.6%; Average loss:1250.9359\n",
            "Iteration:3545;Percent complete:88.6%; Average loss:1442.8418\n",
            "Iteration:3546;Percent complete:88.6%; Average loss:1209.5734\n",
            "Iteration:3547;Percent complete:88.7%; Average loss:1477.8604\n",
            "Iteration:3548;Percent complete:88.7%; Average loss:455.7695\n",
            "Iteration:3549;Percent complete:88.7%; Average loss:636.1539\n",
            "Iteration:3550;Percent complete:88.8%; Average loss:1364.6800\n",
            "Iteration:3551;Percent complete:88.8%; Average loss:1311.7078\n",
            "Iteration:3552;Percent complete:88.8%; Average loss:1383.6470\n",
            "Iteration:3553;Percent complete:88.8%; Average loss:749.0969\n",
            "Iteration:3554;Percent complete:88.8%; Average loss:1411.4849\n",
            "Iteration:3555;Percent complete:88.9%; Average loss:1400.2466\n",
            "Iteration:3556;Percent complete:88.9%; Average loss:1516.4365\n",
            "Iteration:3557;Percent complete:88.9%; Average loss:1241.9131\n",
            "Iteration:3558;Percent complete:88.9%; Average loss:695.3411\n",
            "Iteration:3559;Percent complete:89.0%; Average loss:1413.2208\n",
            "Iteration:3560;Percent complete:89.0%; Average loss:1414.4102\n",
            "Iteration:3561;Percent complete:89.0%; Average loss:1434.7086\n",
            "Iteration:3562;Percent complete:89.0%; Average loss:1473.5162\n",
            "Iteration:3563;Percent complete:89.1%; Average loss:1522.4430\n",
            "Iteration:3564;Percent complete:89.1%; Average loss:1259.3792\n",
            "Iteration:3565;Percent complete:89.1%; Average loss:1346.9927\n",
            "Iteration:3566;Percent complete:89.1%; Average loss:1401.0805\n",
            "Iteration:3567;Percent complete:89.2%; Average loss:1584.0807\n",
            "Iteration:3568;Percent complete:89.2%; Average loss:1479.0762\n",
            "Iteration:3569;Percent complete:89.2%; Average loss:1363.7781\n",
            "Iteration:3570;Percent complete:89.2%; Average loss:1268.3637\n",
            "Iteration:3571;Percent complete:89.3%; Average loss:1509.6660\n",
            "Iteration:3572;Percent complete:89.3%; Average loss:1248.0795\n",
            "Iteration:3573;Percent complete:89.3%; Average loss:1255.6654\n",
            "Iteration:3574;Percent complete:89.3%; Average loss:1253.8397\n",
            "Iteration:3575;Percent complete:89.4%; Average loss:1151.7545\n",
            "Iteration:3576;Percent complete:89.4%; Average loss:1693.8317\n",
            "Iteration:3577;Percent complete:89.4%; Average loss:1412.7348\n",
            "Iteration:3578;Percent complete:89.5%; Average loss:1548.3777\n",
            "Iteration:3579;Percent complete:89.5%; Average loss:1350.6783\n",
            "Iteration:3580;Percent complete:89.5%; Average loss:1604.8027\n",
            "Iteration:3581;Percent complete:89.5%; Average loss:1330.5534\n",
            "Iteration:3582;Percent complete:89.5%; Average loss:683.8786\n",
            "Iteration:3583;Percent complete:89.6%; Average loss:1141.0909\n",
            "Iteration:3584;Percent complete:89.6%; Average loss:1329.3627\n",
            "Iteration:3585;Percent complete:89.6%; Average loss:1454.0864\n",
            "Iteration:3586;Percent complete:89.6%; Average loss:1524.5381\n",
            "Iteration:3587;Percent complete:89.7%; Average loss:1303.9441\n",
            "Iteration:3588;Percent complete:89.7%; Average loss:714.5799\n",
            "Iteration:3589;Percent complete:89.7%; Average loss:1250.3583\n",
            "Iteration:3590;Percent complete:89.8%; Average loss:1227.8628\n",
            "Iteration:3591;Percent complete:89.8%; Average loss:1370.1469\n",
            "Iteration:3592;Percent complete:89.8%; Average loss:742.5294\n",
            "Iteration:3593;Percent complete:89.8%; Average loss:652.2665\n",
            "Iteration:3594;Percent complete:89.8%; Average loss:1463.0632\n",
            "Iteration:3595;Percent complete:89.9%; Average loss:1401.9277\n",
            "Iteration:3596;Percent complete:89.9%; Average loss:1415.0211\n",
            "Iteration:3597;Percent complete:89.9%; Average loss:1526.5209\n",
            "Iteration:3598;Percent complete:90.0%; Average loss:821.7843\n",
            "Iteration:3599;Percent complete:90.0%; Average loss:1232.5089\n",
            "Iteration:3600;Percent complete:90.0%; Average loss:1398.6065\n",
            "Iteration:3601;Percent complete:90.0%; Average loss:601.8749\n",
            "Iteration:3602;Percent complete:90.0%; Average loss:1271.9610\n",
            "Iteration:3603;Percent complete:90.1%; Average loss:1282.2724\n",
            "Iteration:3604;Percent complete:90.1%; Average loss:1351.9889\n",
            "Iteration:3605;Percent complete:90.1%; Average loss:1695.6801\n",
            "Iteration:3606;Percent complete:90.1%; Average loss:619.6831\n",
            "Iteration:3607;Percent complete:90.2%; Average loss:1536.0164\n",
            "Iteration:3608;Percent complete:90.2%; Average loss:798.0582\n",
            "Iteration:3609;Percent complete:90.2%; Average loss:1301.4793\n",
            "Iteration:3610;Percent complete:90.2%; Average loss:726.1425\n",
            "Iteration:3611;Percent complete:90.3%; Average loss:1234.6751\n",
            "Iteration:3612;Percent complete:90.3%; Average loss:656.4429\n",
            "Iteration:3613;Percent complete:90.3%; Average loss:711.6785\n",
            "Iteration:3614;Percent complete:90.3%; Average loss:1400.9684\n",
            "Iteration:3615;Percent complete:90.4%; Average loss:1242.8914\n",
            "Iteration:3616;Percent complete:90.4%; Average loss:642.1473\n",
            "Iteration:3617;Percent complete:90.4%; Average loss:1486.7894\n",
            "Iteration:3618;Percent complete:90.5%; Average loss:674.8107\n",
            "Iteration:3619;Percent complete:90.5%; Average loss:652.4055\n",
            "Iteration:3620;Percent complete:90.5%; Average loss:1473.0148\n",
            "Iteration:3621;Percent complete:90.5%; Average loss:1585.4052\n",
            "Iteration:3622;Percent complete:90.5%; Average loss:1445.0401\n",
            "Iteration:3623;Percent complete:90.6%; Average loss:709.2566\n",
            "Iteration:3624;Percent complete:90.6%; Average loss:721.8513\n",
            "Iteration:3625;Percent complete:90.6%; Average loss:1274.2384\n",
            "Iteration:3626;Percent complete:90.6%; Average loss:593.0303\n",
            "Iteration:3627;Percent complete:90.7%; Average loss:1277.7301\n",
            "Iteration:3628;Percent complete:90.7%; Average loss:1485.8381\n",
            "Iteration:3629;Percent complete:90.7%; Average loss:1392.1911\n",
            "Iteration:3630;Percent complete:90.8%; Average loss:1309.2720\n",
            "Iteration:3631;Percent complete:90.8%; Average loss:699.5707\n",
            "Iteration:3632;Percent complete:90.8%; Average loss:1507.5384\n",
            "Iteration:3633;Percent complete:90.8%; Average loss:1494.6270\n",
            "Iteration:3634;Percent complete:90.8%; Average loss:594.6334\n",
            "Iteration:3635;Percent complete:90.9%; Average loss:1279.4808\n",
            "Iteration:3636;Percent complete:90.9%; Average loss:712.7713\n",
            "Iteration:3637;Percent complete:90.9%; Average loss:1351.5125\n",
            "Iteration:3638;Percent complete:91.0%; Average loss:1190.4460\n",
            "Iteration:3639;Percent complete:91.0%; Average loss:1265.6390\n",
            "Iteration:3640;Percent complete:91.0%; Average loss:1430.6246\n",
            "Iteration:3641;Percent complete:91.0%; Average loss:1297.9368\n",
            "Iteration:3642;Percent complete:91.0%; Average loss:1351.4822\n",
            "Iteration:3643;Percent complete:91.1%; Average loss:1170.3753\n",
            "Iteration:3644;Percent complete:91.1%; Average loss:1265.9725\n",
            "Iteration:3645;Percent complete:91.1%; Average loss:1388.1578\n",
            "Iteration:3646;Percent complete:91.1%; Average loss:1292.0164\n",
            "Iteration:3647;Percent complete:91.2%; Average loss:1161.6969\n",
            "Iteration:3648;Percent complete:91.2%; Average loss:1232.3124\n",
            "Iteration:3649;Percent complete:91.2%; Average loss:1265.8886\n",
            "Iteration:3650;Percent complete:91.2%; Average loss:709.2884\n",
            "Iteration:3651;Percent complete:91.3%; Average loss:1650.0364\n",
            "Iteration:3652;Percent complete:91.3%; Average loss:1421.7784\n",
            "Iteration:3653;Percent complete:91.3%; Average loss:800.8600\n",
            "Iteration:3654;Percent complete:91.3%; Average loss:1140.9343\n",
            "Iteration:3655;Percent complete:91.4%; Average loss:1222.5079\n",
            "Iteration:3656;Percent complete:91.4%; Average loss:1474.5834\n",
            "Iteration:3657;Percent complete:91.4%; Average loss:1450.5062\n",
            "Iteration:3658;Percent complete:91.5%; Average loss:682.6032\n",
            "Iteration:3659;Percent complete:91.5%; Average loss:1494.8225\n",
            "Iteration:3660;Percent complete:91.5%; Average loss:1322.7679\n",
            "Iteration:3661;Percent complete:91.5%; Average loss:1377.9772\n",
            "Iteration:3662;Percent complete:91.5%; Average loss:627.9724\n",
            "Iteration:3663;Percent complete:91.6%; Average loss:1662.1082\n",
            "Iteration:3664;Percent complete:91.6%; Average loss:500.9869\n",
            "Iteration:3665;Percent complete:91.6%; Average loss:1444.6937\n",
            "Iteration:3666;Percent complete:91.6%; Average loss:1225.8104\n",
            "Iteration:3667;Percent complete:91.7%; Average loss:1528.4198\n",
            "Iteration:3668;Percent complete:91.7%; Average loss:754.6242\n",
            "Iteration:3669;Percent complete:91.7%; Average loss:1315.5354\n",
            "Iteration:3670;Percent complete:91.8%; Average loss:1410.8318\n",
            "Iteration:3671;Percent complete:91.8%; Average loss:1507.5523\n",
            "Iteration:3672;Percent complete:91.8%; Average loss:1373.8794\n",
            "Iteration:3673;Percent complete:91.8%; Average loss:443.8454\n",
            "Iteration:3674;Percent complete:91.8%; Average loss:1286.6585\n",
            "Iteration:3675;Percent complete:91.9%; Average loss:1658.2048\n",
            "Iteration:3676;Percent complete:91.9%; Average loss:1199.4815\n",
            "Iteration:3677;Percent complete:91.9%; Average loss:1616.3440\n",
            "Iteration:3678;Percent complete:92.0%; Average loss:777.7008\n",
            "Iteration:3679;Percent complete:92.0%; Average loss:1554.1396\n",
            "Iteration:3680;Percent complete:92.0%; Average loss:1327.7832\n",
            "Iteration:3681;Percent complete:92.0%; Average loss:699.3418\n",
            "Iteration:3682;Percent complete:92.0%; Average loss:1133.0865\n",
            "Iteration:3683;Percent complete:92.1%; Average loss:643.8800\n",
            "Iteration:3684;Percent complete:92.1%; Average loss:1569.1531\n",
            "Iteration:3685;Percent complete:92.1%; Average loss:1244.1190\n",
            "Iteration:3686;Percent complete:92.2%; Average loss:1299.2008\n",
            "Iteration:3687;Percent complete:92.2%; Average loss:620.1001\n",
            "Iteration:3688;Percent complete:92.2%; Average loss:1225.1765\n",
            "Iteration:3689;Percent complete:92.2%; Average loss:1340.4723\n",
            "Iteration:3690;Percent complete:92.2%; Average loss:1240.3643\n",
            "Iteration:3691;Percent complete:92.3%; Average loss:1386.7356\n",
            "Iteration:3692;Percent complete:92.3%; Average loss:649.0735\n",
            "Iteration:3693;Percent complete:92.3%; Average loss:1328.3681\n",
            "Iteration:3694;Percent complete:92.3%; Average loss:1347.0923\n",
            "Iteration:3695;Percent complete:92.4%; Average loss:1215.3556\n",
            "Iteration:3696;Percent complete:92.4%; Average loss:1362.2020\n",
            "Iteration:3697;Percent complete:92.4%; Average loss:1345.8147\n",
            "Iteration:3698;Percent complete:92.5%; Average loss:1370.7015\n",
            "Iteration:3699;Percent complete:92.5%; Average loss:640.8043\n",
            "Iteration:3700;Percent complete:92.5%; Average loss:1330.6442\n",
            "Iteration:3701;Percent complete:92.5%; Average loss:1438.9367\n",
            "Iteration:3702;Percent complete:92.5%; Average loss:1224.5772\n",
            "Iteration:3703;Percent complete:92.6%; Average loss:1374.9611\n",
            "Iteration:3704;Percent complete:92.6%; Average loss:1309.5185\n",
            "Iteration:3705;Percent complete:92.6%; Average loss:1380.2156\n",
            "Iteration:3706;Percent complete:92.7%; Average loss:1498.5778\n",
            "Iteration:3707;Percent complete:92.7%; Average loss:1363.9463\n",
            "Iteration:3708;Percent complete:92.7%; Average loss:726.0614\n",
            "Iteration:3709;Percent complete:92.7%; Average loss:1608.5313\n",
            "Iteration:3710;Percent complete:92.8%; Average loss:696.2555\n",
            "Iteration:3711;Percent complete:92.8%; Average loss:1357.4632\n",
            "Iteration:3712;Percent complete:92.8%; Average loss:1268.8807\n",
            "Iteration:3713;Percent complete:92.8%; Average loss:1153.6092\n",
            "Iteration:3714;Percent complete:92.8%; Average loss:1454.4206\n",
            "Iteration:3715;Percent complete:92.9%; Average loss:662.5443\n",
            "Iteration:3716;Percent complete:92.9%; Average loss:1571.5603\n",
            "Iteration:3717;Percent complete:92.9%; Average loss:1418.7943\n",
            "Iteration:3718;Percent complete:93.0%; Average loss:1425.9308\n",
            "Iteration:3719;Percent complete:93.0%; Average loss:1349.7924\n",
            "Iteration:3720;Percent complete:93.0%; Average loss:1482.3332\n",
            "Iteration:3721;Percent complete:93.0%; Average loss:1569.9335\n",
            "Iteration:3722;Percent complete:93.0%; Average loss:1498.1531\n",
            "Iteration:3723;Percent complete:93.1%; Average loss:1224.1743\n",
            "Iteration:3724;Percent complete:93.1%; Average loss:1461.0035\n",
            "Iteration:3725;Percent complete:93.1%; Average loss:1195.8291\n",
            "Iteration:3726;Percent complete:93.2%; Average loss:1299.3126\n",
            "Iteration:3727;Percent complete:93.2%; Average loss:1245.0671\n",
            "Iteration:3728;Percent complete:93.2%; Average loss:480.8029\n",
            "Iteration:3729;Percent complete:93.2%; Average loss:1531.2621\n",
            "Iteration:3730;Percent complete:93.2%; Average loss:640.9034\n",
            "Iteration:3731;Percent complete:93.3%; Average loss:1263.0730\n",
            "Iteration:3732;Percent complete:93.3%; Average loss:662.1455\n",
            "Iteration:3733;Percent complete:93.3%; Average loss:1465.5208\n",
            "Iteration:3734;Percent complete:93.3%; Average loss:1243.1300\n",
            "Iteration:3735;Percent complete:93.4%; Average loss:1259.8285\n",
            "Iteration:3736;Percent complete:93.4%; Average loss:1367.8068\n",
            "Iteration:3737;Percent complete:93.4%; Average loss:1218.6396\n",
            "Iteration:3738;Percent complete:93.5%; Average loss:1249.6856\n",
            "Iteration:3739;Percent complete:93.5%; Average loss:1275.9582\n",
            "Iteration:3740;Percent complete:93.5%; Average loss:1486.2776\n",
            "Iteration:3741;Percent complete:93.5%; Average loss:1350.3125\n",
            "Iteration:3742;Percent complete:93.5%; Average loss:1266.2521\n",
            "Iteration:3743;Percent complete:93.6%; Average loss:487.3244\n",
            "Iteration:3744;Percent complete:93.6%; Average loss:1514.7891\n",
            "Iteration:3745;Percent complete:93.6%; Average loss:1324.7342\n",
            "Iteration:3746;Percent complete:93.7%; Average loss:1190.1891\n",
            "Iteration:3747;Percent complete:93.7%; Average loss:1470.6089\n",
            "Iteration:3748;Percent complete:93.7%; Average loss:1564.0047\n",
            "Iteration:3749;Percent complete:93.7%; Average loss:1371.5835\n",
            "Iteration:3750;Percent complete:93.8%; Average loss:1573.9417\n",
            "Iteration:3751;Percent complete:93.8%; Average loss:1543.3137\n",
            "Iteration:3752;Percent complete:93.8%; Average loss:1313.5314\n",
            "Iteration:3753;Percent complete:93.8%; Average loss:1264.2219\n",
            "Iteration:3754;Percent complete:93.8%; Average loss:1400.4389\n",
            "Iteration:3755;Percent complete:93.9%; Average loss:1278.4230\n",
            "Iteration:3756;Percent complete:93.9%; Average loss:1226.2037\n",
            "Iteration:3757;Percent complete:93.9%; Average loss:1176.3274\n",
            "Iteration:3758;Percent complete:94.0%; Average loss:1344.2903\n",
            "Iteration:3759;Percent complete:94.0%; Average loss:586.9134\n",
            "Iteration:3760;Percent complete:94.0%; Average loss:1232.8855\n",
            "Iteration:3761;Percent complete:94.0%; Average loss:1411.3054\n",
            "Iteration:3762;Percent complete:94.0%; Average loss:1343.8367\n",
            "Iteration:3763;Percent complete:94.1%; Average loss:432.9194\n",
            "Iteration:3764;Percent complete:94.1%; Average loss:446.9475\n",
            "Iteration:3765;Percent complete:94.1%; Average loss:1381.6686\n",
            "Iteration:3766;Percent complete:94.2%; Average loss:1362.1651\n",
            "Iteration:3767;Percent complete:94.2%; Average loss:1370.1302\n",
            "Iteration:3768;Percent complete:94.2%; Average loss:1397.9772\n",
            "Iteration:3769;Percent complete:94.2%; Average loss:1433.9680\n",
            "Iteration:3770;Percent complete:94.2%; Average loss:1163.0155\n",
            "Iteration:3771;Percent complete:94.3%; Average loss:1400.2607\n",
            "Iteration:3772;Percent complete:94.3%; Average loss:739.6613\n",
            "Iteration:3773;Percent complete:94.3%; Average loss:1348.8088\n",
            "Iteration:3774;Percent complete:94.3%; Average loss:1371.1219\n",
            "Iteration:3775;Percent complete:94.4%; Average loss:1470.6907\n",
            "Iteration:3776;Percent complete:94.4%; Average loss:1305.9763\n",
            "Iteration:3777;Percent complete:94.4%; Average loss:631.7765\n",
            "Iteration:3778;Percent complete:94.5%; Average loss:1397.1061\n",
            "Iteration:3779;Percent complete:94.5%; Average loss:1112.5747\n",
            "Iteration:3780;Percent complete:94.5%; Average loss:786.2769\n",
            "Iteration:3781;Percent complete:94.5%; Average loss:1365.7500\n",
            "Iteration:3782;Percent complete:94.5%; Average loss:1167.4950\n",
            "Iteration:3783;Percent complete:94.6%; Average loss:1310.2919\n",
            "Iteration:3784;Percent complete:94.6%; Average loss:475.5182\n",
            "Iteration:3785;Percent complete:94.6%; Average loss:1637.8473\n",
            "Iteration:3786;Percent complete:94.7%; Average loss:1484.6735\n",
            "Iteration:3787;Percent complete:94.7%; Average loss:1464.8321\n",
            "Iteration:3788;Percent complete:94.7%; Average loss:1378.6545\n",
            "Iteration:3789;Percent complete:94.7%; Average loss:705.0053\n",
            "Iteration:3790;Percent complete:94.8%; Average loss:1664.7396\n",
            "Iteration:3791;Percent complete:94.8%; Average loss:1277.5202\n",
            "Iteration:3792;Percent complete:94.8%; Average loss:640.3072\n",
            "Iteration:3793;Percent complete:94.8%; Average loss:1267.6766\n",
            "Iteration:3794;Percent complete:94.8%; Average loss:657.7463\n",
            "Iteration:3795;Percent complete:94.9%; Average loss:1316.9968\n",
            "Iteration:3796;Percent complete:94.9%; Average loss:1112.1205\n",
            "Iteration:3797;Percent complete:94.9%; Average loss:1516.7751\n",
            "Iteration:3798;Percent complete:95.0%; Average loss:1318.9717\n",
            "Iteration:3799;Percent complete:95.0%; Average loss:1235.3451\n",
            "Iteration:3800;Percent complete:95.0%; Average loss:1324.5021\n",
            "Iteration:3801;Percent complete:95.0%; Average loss:699.1634\n",
            "Iteration:3802;Percent complete:95.0%; Average loss:1462.0131\n",
            "Iteration:3803;Percent complete:95.1%; Average loss:509.2922\n",
            "Iteration:3804;Percent complete:95.1%; Average loss:1506.6906\n",
            "Iteration:3805;Percent complete:95.1%; Average loss:680.5014\n",
            "Iteration:3806;Percent complete:95.2%; Average loss:1350.1474\n",
            "Iteration:3807;Percent complete:95.2%; Average loss:483.3384\n",
            "Iteration:3808;Percent complete:95.2%; Average loss:1261.7119\n",
            "Iteration:3809;Percent complete:95.2%; Average loss:1219.9575\n",
            "Iteration:3810;Percent complete:95.2%; Average loss:1485.5167\n",
            "Iteration:3811;Percent complete:95.3%; Average loss:1408.4306\n",
            "Iteration:3812;Percent complete:95.3%; Average loss:1355.0935\n",
            "Iteration:3813;Percent complete:95.3%; Average loss:1510.6103\n",
            "Iteration:3814;Percent complete:95.3%; Average loss:1345.8391\n",
            "Iteration:3815;Percent complete:95.4%; Average loss:1453.5457\n",
            "Iteration:3816;Percent complete:95.4%; Average loss:1337.4743\n",
            "Iteration:3817;Percent complete:95.4%; Average loss:1403.2246\n",
            "Iteration:3818;Percent complete:95.5%; Average loss:1232.8431\n",
            "Iteration:3819;Percent complete:95.5%; Average loss:774.7686\n",
            "Iteration:3820;Percent complete:95.5%; Average loss:1217.6462\n",
            "Iteration:3821;Percent complete:95.5%; Average loss:1521.2634\n",
            "Iteration:3822;Percent complete:95.5%; Average loss:1289.9300\n",
            "Iteration:3823;Percent complete:95.6%; Average loss:1331.8910\n",
            "Iteration:3824;Percent complete:95.6%; Average loss:1346.2956\n",
            "Iteration:3825;Percent complete:95.6%; Average loss:1296.4296\n",
            "Iteration:3826;Percent complete:95.7%; Average loss:675.6619\n",
            "Iteration:3827;Percent complete:95.7%; Average loss:1318.1562\n",
            "Iteration:3828;Percent complete:95.7%; Average loss:1262.4817\n",
            "Iteration:3829;Percent complete:95.7%; Average loss:1326.8157\n",
            "Iteration:3830;Percent complete:95.8%; Average loss:1182.8241\n",
            "Iteration:3831;Percent complete:95.8%; Average loss:1349.1423\n",
            "Iteration:3832;Percent complete:95.8%; Average loss:688.4047\n",
            "Iteration:3833;Percent complete:95.8%; Average loss:1231.2911\n",
            "Iteration:3834;Percent complete:95.9%; Average loss:1260.5465\n",
            "Iteration:3835;Percent complete:95.9%; Average loss:1384.7823\n",
            "Iteration:3836;Percent complete:95.9%; Average loss:1290.8375\n",
            "Iteration:3837;Percent complete:95.9%; Average loss:636.5909\n",
            "Iteration:3838;Percent complete:96.0%; Average loss:653.4460\n",
            "Iteration:3839;Percent complete:96.0%; Average loss:1376.8418\n",
            "Iteration:3840;Percent complete:96.0%; Average loss:1360.5232\n",
            "Iteration:3841;Percent complete:96.0%; Average loss:439.5323\n",
            "Iteration:3842;Percent complete:96.0%; Average loss:1319.5850\n",
            "Iteration:3843;Percent complete:96.1%; Average loss:1310.8275\n",
            "Iteration:3844;Percent complete:96.1%; Average loss:1345.0911\n",
            "Iteration:3845;Percent complete:96.1%; Average loss:1234.0899\n",
            "Iteration:3846;Percent complete:96.2%; Average loss:1317.8268\n",
            "Iteration:3847;Percent complete:96.2%; Average loss:1278.1801\n",
            "Iteration:3848;Percent complete:96.2%; Average loss:437.2281\n",
            "Iteration:3849;Percent complete:96.2%; Average loss:1401.5213\n",
            "Iteration:3850;Percent complete:96.2%; Average loss:1283.7942\n",
            "Iteration:3851;Percent complete:96.3%; Average loss:1292.6370\n",
            "Iteration:3852;Percent complete:96.3%; Average loss:1273.3444\n",
            "Iteration:3853;Percent complete:96.3%; Average loss:1218.3223\n",
            "Iteration:3854;Percent complete:96.4%; Average loss:672.4905\n",
            "Iteration:3855;Percent complete:96.4%; Average loss:1254.9877\n",
            "Iteration:3856;Percent complete:96.4%; Average loss:1477.6873\n",
            "Iteration:3857;Percent complete:96.4%; Average loss:1228.1738\n",
            "Iteration:3858;Percent complete:96.5%; Average loss:1413.3583\n",
            "Iteration:3859;Percent complete:96.5%; Average loss:1306.9961\n",
            "Iteration:3860;Percent complete:96.5%; Average loss:1336.9944\n",
            "Iteration:3861;Percent complete:96.5%; Average loss:1278.9615\n",
            "Iteration:3862;Percent complete:96.5%; Average loss:1275.1419\n",
            "Iteration:3863;Percent complete:96.6%; Average loss:1234.1266\n",
            "Iteration:3864;Percent complete:96.6%; Average loss:1321.3840\n",
            "Iteration:3865;Percent complete:96.6%; Average loss:647.1474\n",
            "Iteration:3866;Percent complete:96.7%; Average loss:806.6548\n",
            "Iteration:3867;Percent complete:96.7%; Average loss:1253.1229\n",
            "Iteration:3868;Percent complete:96.7%; Average loss:642.4499\n",
            "Iteration:3869;Percent complete:96.7%; Average loss:1411.7569\n",
            "Iteration:3870;Percent complete:96.8%; Average loss:381.1322\n",
            "Iteration:3871;Percent complete:96.8%; Average loss:1290.9777\n",
            "Iteration:3872;Percent complete:96.8%; Average loss:1224.7278\n",
            "Iteration:3873;Percent complete:96.8%; Average loss:1592.8933\n",
            "Iteration:3874;Percent complete:96.9%; Average loss:1382.3756\n",
            "Iteration:3875;Percent complete:96.9%; Average loss:1209.7649\n",
            "Iteration:3876;Percent complete:96.9%; Average loss:603.4783\n",
            "Iteration:3877;Percent complete:96.9%; Average loss:1282.0648\n",
            "Iteration:3878;Percent complete:97.0%; Average loss:673.0479\n",
            "Iteration:3879;Percent complete:97.0%; Average loss:551.0356\n",
            "Iteration:3880;Percent complete:97.0%; Average loss:1298.7838\n",
            "Iteration:3881;Percent complete:97.0%; Average loss:1416.0687\n",
            "Iteration:3882;Percent complete:97.0%; Average loss:1388.8059\n",
            "Iteration:3883;Percent complete:97.1%; Average loss:1193.0663\n",
            "Iteration:3884;Percent complete:97.1%; Average loss:1194.1025\n",
            "Iteration:3885;Percent complete:97.1%; Average loss:1246.4796\n",
            "Iteration:3886;Percent complete:97.2%; Average loss:1428.8609\n",
            "Iteration:3887;Percent complete:97.2%; Average loss:1365.2370\n",
            "Iteration:3888;Percent complete:97.2%; Average loss:1567.2558\n",
            "Iteration:3889;Percent complete:97.2%; Average loss:1235.0435\n",
            "Iteration:3890;Percent complete:97.2%; Average loss:757.1733\n",
            "Iteration:3891;Percent complete:97.3%; Average loss:1239.9983\n",
            "Iteration:3892;Percent complete:97.3%; Average loss:1623.7255\n",
            "Iteration:3893;Percent complete:97.3%; Average loss:326.5415\n",
            "Iteration:3894;Percent complete:97.4%; Average loss:876.5429\n",
            "Iteration:3895;Percent complete:97.4%; Average loss:1563.7395\n",
            "Iteration:3896;Percent complete:97.4%; Average loss:636.3986\n",
            "Iteration:3897;Percent complete:97.4%; Average loss:1199.8464\n",
            "Iteration:3898;Percent complete:97.5%; Average loss:1335.6642\n",
            "Iteration:3899;Percent complete:97.5%; Average loss:1521.3274\n",
            "Iteration:3900;Percent complete:97.5%; Average loss:1257.5179\n",
            "Iteration:3901;Percent complete:97.5%; Average loss:1252.6155\n",
            "Iteration:3902;Percent complete:97.5%; Average loss:1364.0638\n",
            "Iteration:3903;Percent complete:97.6%; Average loss:1466.8449\n",
            "Iteration:3904;Percent complete:97.6%; Average loss:1104.3326\n",
            "Iteration:3905;Percent complete:97.6%; Average loss:1474.8986\n",
            "Iteration:3906;Percent complete:97.7%; Average loss:1517.3490\n",
            "Iteration:3907;Percent complete:97.7%; Average loss:1286.9365\n",
            "Iteration:3908;Percent complete:97.7%; Average loss:1479.5946\n",
            "Iteration:3909;Percent complete:97.7%; Average loss:1481.0967\n",
            "Iteration:3910;Percent complete:97.8%; Average loss:1239.3116\n",
            "Iteration:3911;Percent complete:97.8%; Average loss:1342.9544\n",
            "Iteration:3912;Percent complete:97.8%; Average loss:1473.9182\n",
            "Iteration:3913;Percent complete:97.8%; Average loss:765.8096\n",
            "Iteration:3914;Percent complete:97.9%; Average loss:1352.1253\n",
            "Iteration:3915;Percent complete:97.9%; Average loss:1216.8473\n",
            "Iteration:3916;Percent complete:97.9%; Average loss:1386.0848\n",
            "Iteration:3917;Percent complete:97.9%; Average loss:1241.0348\n",
            "Iteration:3918;Percent complete:98.0%; Average loss:1236.9528\n",
            "Iteration:3919;Percent complete:98.0%; Average loss:1489.5473\n",
            "Iteration:3920;Percent complete:98.0%; Average loss:1371.3222\n",
            "Iteration:3921;Percent complete:98.0%; Average loss:1408.2044\n",
            "Iteration:3922;Percent complete:98.0%; Average loss:1305.7586\n",
            "Iteration:3923;Percent complete:98.1%; Average loss:1357.5417\n",
            "Iteration:3924;Percent complete:98.1%; Average loss:1195.3679\n",
            "Iteration:3925;Percent complete:98.1%; Average loss:1186.9496\n",
            "Iteration:3926;Percent complete:98.2%; Average loss:1530.3930\n",
            "Iteration:3927;Percent complete:98.2%; Average loss:1439.2904\n",
            "Iteration:3928;Percent complete:98.2%; Average loss:1534.9179\n",
            "Iteration:3929;Percent complete:98.2%; Average loss:1395.6636\n",
            "Iteration:3930;Percent complete:98.2%; Average loss:1362.5257\n",
            "Iteration:3931;Percent complete:98.3%; Average loss:1422.6028\n",
            "Iteration:3932;Percent complete:98.3%; Average loss:1421.3478\n",
            "Iteration:3933;Percent complete:98.3%; Average loss:1486.0608\n",
            "Iteration:3934;Percent complete:98.4%; Average loss:1423.5188\n",
            "Iteration:3935;Percent complete:98.4%; Average loss:788.0636\n",
            "Iteration:3936;Percent complete:98.4%; Average loss:1358.8292\n",
            "Iteration:3937;Percent complete:98.4%; Average loss:735.8977\n",
            "Iteration:3938;Percent complete:98.5%; Average loss:728.4385\n",
            "Iteration:3939;Percent complete:98.5%; Average loss:715.2478\n",
            "Iteration:3940;Percent complete:98.5%; Average loss:1183.0036\n",
            "Iteration:3941;Percent complete:98.5%; Average loss:1176.7091\n",
            "Iteration:3942;Percent complete:98.6%; Average loss:1297.4246\n",
            "Iteration:3943;Percent complete:98.6%; Average loss:1277.1925\n",
            "Iteration:3944;Percent complete:98.6%; Average loss:1315.9639\n",
            "Iteration:3945;Percent complete:98.6%; Average loss:1215.0518\n",
            "Iteration:3946;Percent complete:98.7%; Average loss:1256.9512\n",
            "Iteration:3947;Percent complete:98.7%; Average loss:1498.0769\n",
            "Iteration:3948;Percent complete:98.7%; Average loss:1271.5077\n",
            "Iteration:3949;Percent complete:98.7%; Average loss:1280.6344\n",
            "Iteration:3950;Percent complete:98.8%; Average loss:707.2720\n",
            "Iteration:3951;Percent complete:98.8%; Average loss:605.3535\n",
            "Iteration:3952;Percent complete:98.8%; Average loss:1333.3043\n",
            "Iteration:3953;Percent complete:98.8%; Average loss:503.9030\n",
            "Iteration:3954;Percent complete:98.9%; Average loss:1366.7121\n",
            "Iteration:3955;Percent complete:98.9%; Average loss:1300.1720\n",
            "Iteration:3956;Percent complete:98.9%; Average loss:1295.6775\n",
            "Iteration:3957;Percent complete:98.9%; Average loss:1288.0972\n",
            "Iteration:3958;Percent complete:99.0%; Average loss:1366.2682\n",
            "Iteration:3959;Percent complete:99.0%; Average loss:716.1877\n",
            "Iteration:3960;Percent complete:99.0%; Average loss:1360.1486\n",
            "Iteration:3961;Percent complete:99.0%; Average loss:1238.9358\n",
            "Iteration:3962;Percent complete:99.1%; Average loss:1308.0045\n",
            "Iteration:3963;Percent complete:99.1%; Average loss:1368.0813\n",
            "Iteration:3964;Percent complete:99.1%; Average loss:1116.3453\n",
            "Iteration:3965;Percent complete:99.1%; Average loss:1306.4770\n",
            "Iteration:3966;Percent complete:99.2%; Average loss:1305.3581\n",
            "Iteration:3967;Percent complete:99.2%; Average loss:1430.8762\n",
            "Iteration:3968;Percent complete:99.2%; Average loss:1470.7824\n",
            "Iteration:3969;Percent complete:99.2%; Average loss:1224.4096\n",
            "Iteration:3970;Percent complete:99.2%; Average loss:653.6519\n",
            "Iteration:3971;Percent complete:99.3%; Average loss:627.2666\n",
            "Iteration:3972;Percent complete:99.3%; Average loss:1524.9346\n",
            "Iteration:3973;Percent complete:99.3%; Average loss:1543.3676\n",
            "Iteration:3974;Percent complete:99.4%; Average loss:1433.4030\n",
            "Iteration:3975;Percent complete:99.4%; Average loss:1528.8583\n",
            "Iteration:3976;Percent complete:99.4%; Average loss:1414.1224\n",
            "Iteration:3977;Percent complete:99.4%; Average loss:440.3891\n",
            "Iteration:3978;Percent complete:99.5%; Average loss:1508.4525\n",
            "Iteration:3979;Percent complete:99.5%; Average loss:1475.5025\n",
            "Iteration:3980;Percent complete:99.5%; Average loss:1314.1432\n",
            "Iteration:3981;Percent complete:99.5%; Average loss:1311.7532\n",
            "Iteration:3982;Percent complete:99.6%; Average loss:1420.3539\n",
            "Iteration:3983;Percent complete:99.6%; Average loss:1361.4832\n",
            "Iteration:3984;Percent complete:99.6%; Average loss:1370.4649\n",
            "Iteration:3985;Percent complete:99.6%; Average loss:1368.7982\n",
            "Iteration:3986;Percent complete:99.7%; Average loss:788.6164\n",
            "Iteration:3987;Percent complete:99.7%; Average loss:1397.6822\n",
            "Iteration:3988;Percent complete:99.7%; Average loss:1385.6440\n",
            "Iteration:3989;Percent complete:99.7%; Average loss:701.9667\n",
            "Iteration:3990;Percent complete:99.8%; Average loss:1287.2880\n",
            "Iteration:3991;Percent complete:99.8%; Average loss:1091.9269\n",
            "Iteration:3992;Percent complete:99.8%; Average loss:1467.6558\n",
            "Iteration:3993;Percent complete:99.8%; Average loss:688.5910\n",
            "Iteration:3994;Percent complete:99.9%; Average loss:1210.3856\n",
            "Iteration:3995;Percent complete:99.9%; Average loss:1510.3673\n",
            "Iteration:3996;Percent complete:99.9%; Average loss:438.4102\n",
            "Iteration:3997;Percent complete:99.9%; Average loss:1217.6266\n",
            "Iteration:3998;Percent complete:100.0%; Average loss:1411.0428\n",
            "Iteration:3999;Percent complete:100.0%; Average loss:1509.3667\n",
            "Iteration:4000;Percent complete:100.0%; Average loss:685.6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy0Dfo981-zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layer]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * sos_tok\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRX0owL52GbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=20):\n",
        "    ### Format input sentence as a batch\n",
        "    # woindexfromsentencerds -> indexes\n",
        "    indexes_batch = [indexfromsentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = 'how are you?'\n",
        "    # Check if it is quit case \n",
        "    # Evaluate sentence\n",
        "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "    # Format and print response sentence\n",
        "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "    print('Bot:', ' '.join(output_words))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpUgzU815NUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5703597f-a153-49f1-ddcf-0fb14cfa006a"
      },
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "searcher = GreedySearchDecoder(encoder,decoder)\n",
        "evaluateInput(encoder,decoder,searcher,voc)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bot: He\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}